[
  {
    "content": {
      "TLDR": "Lyric interpretations can help people understand songs and their lyrics quickly, and can also make it easier to manage, retrieve and discover songs efficiently from the growing mass of music archives. In this paper we propose BART-fusion, a novel model for generating lyrics interpretations from lyrics and music audio that combines a large-scale pre-trained language model with an audio encoder. We employ a cross-modal attention module to incorporate the audio representation into the lyrics representation to help the pre-trained language model understand the song from an audio perspective, while preserving the language model's original generative performance. We also release the Song Interpretation Dataset, a new large-scale dataset for training and evaluating our model. Experimental results show that the additional audio information helps our model to understand words and music better, and to generate precise and fluent interpretations. An additional experiment on cross-modal music retrieval shows that interpretations generated by BART-fusion can also help people retrieve music more accurately than with the original BART.",
      "abstract": "Lyric interpretations can help people understand songs and their lyrics quickly, and can also make it easier to manage, retrieve and discover songs efficiently from the growing mass of music archives. In this paper we propose BART-fusion, a novel model for generating lyrics interpretations from lyrics and music audio that combines a large-scale pre-trained language model with an audio encoder. We employ a cross-modal attention module to incorporate the audio representation into the lyrics representation to help the pre-trained language model understand the song from an audio perspective, while preserving the language model's original generative performance. We also release the Song Interpretation Dataset, a new large-scale dataset for training and evaluating our model. Experimental results show that the additional audio information helps our model to understand words and music better, and to generate precise and fluent interpretations. An additional experiment on cross-modal music retrieval shows that interpretations generated by BART-fusion can also help people retrieve music more accurately than with the original BART.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1jjVL9ryCaUaHxdKU6uA51OSK5KuHajsk)</b>",
      "authors": [
        "Zhang, Yixiao*",
        " Jiang, Junyan",
        " Xia, Gus",
        " Dixon, Simon"
      ],
      "authors_and_affil": [
        "Yixiao Zhang (Centre for Digital Music, Queen Mary University of London)*",
        " Junyan Jiang (Music X Lab, NYU Shanghai, MBZUAI)",
        " Gus Xia (Music X Lab, NYU Shanghai, MBZUAI)",
        " Simon Dixon (Centre for Digital Music, Queen Mary University of London)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CY0MCR4H",
      "day": "1",
      "keywords": [
        "Domain knowledge -> machine learning/artificial intelligence for music",
        " Evaluation, datasets, and reproducibility -> novel datasets and use cases",
        " MIR fundamentals and methodology -> web mining, and natural language processing",
        "MIR fundamentals and methodology -> lyrics and other textual data",
        " MIR fundamentals and methodology -> multimodality"
      ],
      "long_presentation": "TRUE",
      "paper_presentation": "Virtually",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000001.pdf",
      "poster_pdf": "https://drive.google.com/open?id=12pEH1F8X97iyUyzkvMtSxGBxPmvp5u89",
      "session": [
        "1"
      ],
      "slack_channel": "p1-01-zhang",
      "title": "Interpreting Song Lyrics with an Audio-Informed Pre-trained Language Model",
      "video": "https://drive.google.com/uc?export=preview&id=1jjVL9ryCaUaHxdKU6uA51OSK5KuHajsk"
    },
    "forum": "103",
    "id": "103",
    "pic_id": "https://drive.google.com/open?id=1RX-gNH83IsTWPWhmrm8MbkJG1UMlMNTO",
    "position": "01",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "Recent deep learning-based models for estimating beats and downbeats are mainly composed of three successive stages---feature extraction, sequence modeling, and post processing. While such a framework is prevalent in the scenario of sequence labeling tasks and yields promising results in beat and downbeat estimations, it also indicates a shortage of the employed neural networks, given that the post-processing usually provides a notable performance gain over the previous stage. Moreover, the assumption often made for the post-processing is not suitable for many musical pieces. In this work, we attempt to improve the performance of joint beat and downbeat estimation without incorporating the post-processing stage. By inspecting a state-of-the-art approach, we propose reformulations regarding the network architecture and the loss function. We evaluate our model on various music data and show that the proposed methods are capable of improving the baseline approach without the aid of a post-processing stage.",
      "abstract": "Recent deep learning-based models for estimating beats and downbeats are mainly composed of three successive stages---feature extraction, sequence modeling, and post processing. While such a framework is prevalent in the scenario of sequence labeling tasks and yields promising results in beat and downbeat estimations, it also indicates a shortage of the employed neural networks, given that the post-processing usually provides a notable performance gain over the previous stage. Moreover, the assumption often made for the post-processing is not suitable for many musical pieces. In this work, we attempt to improve the performance of joint beat and downbeat estimation without incorporating the post-processing stage. By inspecting a state-of-the-art approach, we propose reformulations regarding the network architecture and the loss function. We evaluate our model on various music data and show that the proposed methods are capable of improving the baseline approach without the aid of a post-processing stage.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1NUkdMAyYMortu8ssU8WwasTMUPt7zEby)</b>",
      "authors": [
        "Chen, Tsung-Ping*",
        " Su, Li"
      ],
      "authors_and_affil": [
        "Tsung-Ping Chen (Institute of Information Science, Academia Sinica, Taiwan)*",
        " Li Su (Institute of Information Science, Academia Sinica, Taiwan)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CMQU7CPN",
      "day": "1",
      "keywords": [
        "Musical features and properties -> rhythm, beat, tempo",
        "Musical features and properties"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Virtually",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000002.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1z_cJqeicGRJRs3ET12i1LK9mgcliKCKH",
      "session": [
        "1"
      ],
      "slack_channel": "p1-02-chen",
      "title": "Toward postprocessing-free neural networks for joint beat and downbeat estimation",
      "video": "https://drive.google.com/uc?export=preview&id=1NUkdMAyYMortu8ssU8WwasTMUPt7zEby"
    },
    "forum": "46",
    "id": "46",
    "pic_id": "https://drive.google.com/open?id=1KPYQUt8mo5JhaiX287vyitUPdF_xb7TG",
    "position": "02",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "We present a novel task of \"playing level conversion\": generating a music arrangement in a target difficulty level, given another arrangement of the same musical piece in a different level. For this task, we create a parallel dataset of piano arrangements in two strictly well-defined playing levels, annotated at individual phrase resolution, taken from the song catalog of a piano learning app.\n\nIn a series of experiments, we train models that successfully modify the playing level while preserving the musical 'essence'. We further show, via an ablation study, the contributions of specific data representation and augmentation techniques to the model's performance.\n\nIn order to evaluate the performance of our models, we conduct a human evaluation study with expert musicians. The evaluation shows that our best model creates arrangements that are almost as good as ground truth examples. Additionally, we propose MuTE, an automated evaluation metric for music translation tasks, and show that it correlates with human ratings.",
      "abstract": "We present a novel task of \"playing level conversion\": generating a music arrangement in a target difficulty level, given another arrangement of the same musical piece in a different level. For this task, we create a parallel dataset of piano arrangements in two strictly well-defined playing levels, annotated at individual phrase resolution, taken from the song catalog of a piano learning app.\n\nIn a series of experiments, we train models that successfully modify the playing level while preserving the musical 'essence'. We further show, via an ablation study, the contributions of specific data representation and augmentation techniques to the model's performance.\n\nIn order to evaluate the performance of our models, we conduct a human evaluation study with expert musicians. The evaluation shows that our best model creates arrangements that are almost as good as ground truth examples. Additionally, we propose MuTE, an automated evaluation metric for music translation tasks, and show that it correlates with human ratings.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1vnrRdzLG0EjjbSHkv0fR7DMy1O-STkVA)</b>",
      "authors": [
        "Gover, Matan*",
        " Zewi, Oded"
      ],
      "authors_and_affil": [
        "Matan Gover (Simply)*",
        " Oded Zewi (Simply)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04D92FRX5W",
      "day": "1",
      "keywords": [
        "Domain knowledge -> machine learning/artificial intelligence for music",
        " MIR fundamentals and methodology -> symbolic music processing",
        " Musical features and properties -> musical style and genre",
        " MIR tasks -> music synthesis and transformation",
        "MIR tasks -> music generation"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Virtually",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000003.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1Pj2q7_K2PhZQKvYemSIyLKh6or6d2V4w",
      "session": [
        "1"
      ],
      "slack_channel": "p1-03-gover",
      "title": "Music Translation: Generating Piano Arrangements in Different Playing Levels",
      "video": "https://drive.google.com/uc?export=preview&id=1vnrRdzLG0EjjbSHkv0fR7DMy1O-STkVA"
    },
    "forum": "37",
    "id": "37",
    "pic_id": "https://drive.google.com/open?id=1zbK7SGgSxTlJXBJ4G1zV_O4kZU8hXM15",
    "position": "03",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "Automatic Music Transcription (AMT), in particular the problem of automatically extracting notes from audio, has seen much recent progress via the training of neural network models on musical audio recordings paired with aligned ground-truth note labels.  However, progress is currently limited by the difficulty of obtaining such note labels for natural audio recordings at scale.  In this paper, we take advantage of the fact that for monophonic music, the transcription problem is much easier and largely solved via modern pitch-tracking methods.  Specifically, we show that we are able to combine recordings of real monophonic music (and their transcriptions) into artificial and musically-incoherent mixtures, greatly increasing the scale of labeled training data.  By pretraining on these mixtures, we can use a larger neural network model and significantly improve upon the state of the art in multi-instrument polyphonic transcription.  We demonstrate this improvement across a variety of datasets and in a ``zero-shot'' setting where the model has not been trained on any data from the evaluation domain.",
      "abstract": "Automatic Music Transcription (AMT), in particular the problem of automatically extracting notes from audio, has seen much recent progress via the training of neural network models on musical audio recordings paired with aligned ground-truth note labels.  However, progress is currently limited by the difficulty of obtaining such note labels for natural audio recordings at scale.  In this paper, we take advantage of the fact that for monophonic music, the transcription problem is much easier and largely solved via modern pitch-tracking methods.  Specifically, we show that we are able to combine recordings of real monophonic music (and their transcriptions) into artificial and musically-incoherent mixtures, greatly increasing the scale of labeled training data.  By pretraining on these mixtures, we can use a larger neural network model and significantly improve upon the state of the art in multi-instrument polyphonic transcription.  We demonstrate this improvement across a variety of datasets and in a ``zero-shot'' setting where the model has not been trained on any data from the evaluation domain.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1mhRr6iciJlhkZpDOsaNCVhKc-SdmjM3Z)</b>",
      "authors": [
        "Simon, Ian*",
        " Gardner, Joshua",
        " Hawthorne, Curtis",
        " Manilow, Ethan",
        " Engel, Jesse"
      ],
      "authors_and_affil": [
        "Ian Simon (Google Research, Brain Team)*",
        " Joshua Gardner (Google Research, Brain Team)",
        " Curtis Hawthorne (Google Research, Brain Team)",
        " Ethan Manilow (Google Research, Brain Team)",
        " Jesse Engel (Google Research, Brain Team)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CK88202F",
      "day": "1",
      "keywords": [
        "MIR tasks -> music transcription and annotation",
        "Domain knowledge -> machine learning/artificial intelligence for music",
        " MIR fundamentals and methodology -> symbolic music processing",
        " Evaluation, datasets, and reproducibility -> novel datasets and use cases",
        " MIR fundamentals and methodology -> web mining, and natural language processing",
        " MIR fundamentals and methodology -> music signal processing"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Virtually",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000004.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1y1pEnQ0xy_xo5cIoRLbTgXOy9wbRc57P",
      "session": [
        "1"
      ],
      "slack_channel": "p1-04-simon",
      "title": "Scaling Polyphonic Transcription with Mixtures of Monophonic Transcriptions",
      "video": "https://drive.google.com/uc?export=preview&id=1mhRr6iciJlhkZpDOsaNCVhKc-SdmjM3Z"
    },
    "forum": "287",
    "id": "287",
    "pic_id": "https://drive.google.com/open?id=1-5-gfWb2dYm6A7jFDjTyZ5t8qLXPwtvx",
    "position": "04",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "An ideal audio retrieval system efficiently and robustly recognizes a short query snippet from an extensive database. However, the performance of well-known audio fingerprinting systems falls short at high signal distortion levels. This paper presents an audio retrieval system that generates noise and reverberation robust audio fingerprints using the contrastive learning framework. Using these fingerprints, the method performs a comprehensive search to identify the query audio and precisely estimate its timestamp in the reference audio. Our framework involves training a CNN to maximize the similarity between pairs of embeddings extracted from clean audio and its corresponding distorted and time-shifted version. We employ a channel-wise spectral-temporal attention mechanism to capture salient time indices and spectral bands in the CNN features. The attention mechanism enables the CNN to better discriminate the audio by giving more weight to the salient spectral-temporal patches in the signal. Experimental results indicate that our system is efficient in computation and memory usage while being more accurate, particularly at higher distortion levels, than competing state-of-the-art systems and scalable to a larger database. ",
      "abstract": "An ideal audio retrieval system efficiently and robustly recognizes a short query snippet from an extensive database. However, the performance of well-known audio fingerprinting systems falls short at high signal distortion levels. This paper presents an audio retrieval system that generates noise and reverberation robust audio fingerprints using the contrastive learning framework. Using these fingerprints, the method performs a comprehensive search to identify the query audio and precisely estimate its timestamp in the reference audio. Our framework involves training a CNN to maximize the similarity between pairs of embeddings extracted from clean audio and its corresponding distorted and time-shifted version. We employ a channel-wise spectral-temporal attention mechanism to capture salient time indices and spectral bands in the CNN features. The attention mechanism enables the CNN to better discriminate the audio by giving more weight to the salient spectral-temporal patches in the signal. Experimental results indicate that our system is efficient in computation and memory usage while being more accurate, particularly at higher distortion levels, than competing state-of-the-art systems and scalable to a larger database. <br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1G96VBsQfpdqjnGhdFlj8ZA1U0l3D8q_d)</b>",
      "authors": [
        "Singh, Anup*",
        " Demuynck, Kris",
        " Arora, Vipul"
      ],
      "authors_and_affil": [
        "Anup Singh (IDLab, Department of Electronics and Information Systems, imec - Ghent University, Belgium, Department of Electrical Engineering, Indian Institute of Technology Kanpur, India)*",
        " Kris Demuynck (IDLab, Department of Electronics and Information Systems, imec - Ghent University, Belgium)",
        " Vipul Arora (Department of Electrical Engineering, Indian Institute of Technology Kanpur, India)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CMQUQM8U",
      "day": "1",
      "keywords": [
        "MIR tasks -> fingerprinting",
        "MIR tasks -> indexing and querying"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Bengaluru",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000005.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1LzpJd3xJpw2Nv5S4kOCbpkvupENNH9_9",
      "session": [
        "1"
      ],
      "slack_channel": "p1-05-singh",
      "title": "Attention-based audio embeddings for query-by-example",
      "video": "https://drive.google.com/uc?export=preview&id=1G96VBsQfpdqjnGhdFlj8ZA1U0l3D8q_d"
    },
    "forum": "100",
    "id": "100",
    "pic_id": "https://drive.google.com/open?id=12mad31jT1ycN8UOpq0lWeMPrVI80ezo3",
    "position": "05",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "The use of point-set representations of music enable repeated pattern discovery\nto be performed on polyphonic music. The discovery of patterns containing polyphony is also enabled by the use of point-set representations. The SIA and SIATEC algorithms discover repeated patterns in point-sets by\ncomputing maximal translatable patterns and their translational equivalence classes.\nWhile the algorithms are relatively efficient, their application to larger pieces\nof music is not viable due to quadratic space complexity.\nThis paper introcudes a novel algorithm, SIATEC-C, for repeated pattern discovery in point-set representations of music. The algorithm discovers repeated patterns and finds all of their occurrences, while\nrunning with subquadratic space complexity. The algorithm can also provide significant running\ntime improvements over the comparable SIATEC algorithm.\nThe computational performance of the algorithm is compared with SIATEC. The accuracy of the algorithm\nis also evaluated on the JKU-PDD data set.",
      "abstract": "The use of point-set representations of music enable repeated pattern discovery\nto be performed on polyphonic music. The discovery of patterns containing polyphony is also enabled by the use of point-set representations. The SIA and SIATEC algorithms discover repeated patterns in point-sets by\ncomputing maximal translatable patterns and their translational equivalence classes.\nWhile the algorithms are relatively efficient, their application to larger pieces\nof music is not viable due to quadratic space complexity.\nThis paper introcudes a novel algorithm, SIATEC-C, for repeated pattern discovery in point-set representations of music. The algorithm discovers repeated patterns and finds all of their occurrences, while\nrunning with subquadratic space complexity. The algorithm can also provide significant running\ntime improvements over the comparable SIATEC algorithm.\nThe computational performance of the algorithm is compared with SIATEC. The accuracy of the algorithm\nis also evaluated on the JKU-PDD data set.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1TsSHHxL4DJ2OuYyUyDpVkF1CwwEj2Lt8)</b>",
      "authors": [
        "Bj\u00f6rklund, Otso*"
      ],
      "authors_and_affil": [
        "Otso Bj\u00f6rklund (University of Helsinki)*"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CKB84P4J",
      "day": "1",
      "keywords": [
        "MIR fundamentals and methodology -> symbolic music processing",
        "MIR tasks -> pattern matching and detection"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Virtually",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000006.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1DFqkRIunJGAHGYWIAIbJrIwg0dLNPaC2",
      "session": [
        "1"
      ],
      "slack_channel": "p1-06-bj\u00f6rklund",
      "title": "SIATEC-C: Computationally efficient repeated pattern discovery in polyphonic music",
      "video": "https://drive.google.com/uc?export=preview&id=1TsSHHxL4DJ2OuYyUyDpVkF1CwwEj2Lt8"
    },
    "forum": "61",
    "id": "61",
    "pic_id": "https://drive.google.com/open?id=1aKycjVCV_TI_2mlIJ5cD7RFsFhXAwaZR",
    "position": "06",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "Self-supervised learning has steadily been gaining traction in recent years. In music information retrieval (MIR), one promising recent application of self-supervised learning is the CLMR framework (contrastive learning of musical representations). CLMR has shown good performance, achieving results on par with state-of-the-art end-to-end classification models, but it is strictly an encoding framework. It suffers the characteristic limitation of any encoder that it cannot explicitly combine multi-timescale information, whereas a characteristic feature of human audio perception is that we tend to perceive all frequencies simultaneously. To this end, we propose a generalization of CLMR that learns to extract and explicitly combine representations across different frequency resolutions, which we coin the tailed U-Net (TUNe). TUNe architectures combine multi-timescale information during a decoding phase, similar to U-Net architectures used in computer vision and source separation, but have a tail added to reduce sample-level information to a smaller pre-defined number of representation dimensions. The size of the decoding phase is a hyperparameter, and in the case of a zero-layer decoding phase, TUNe reduces to CLMR. The best TUNe architectures, however, require less training time to match CLMR performance, have superior transfer learning performance, and are competitive with state-of-the-art models even at dramatically reduced dimensionalities.\n",
      "abstract": "Self-supervised learning has steadily been gaining traction in recent years. In music information retrieval (MIR), one promising recent application of self-supervised learning is the CLMR framework (contrastive learning of musical representations). CLMR has shown good performance, achieving results on par with state-of-the-art end-to-end classification models, but it is strictly an encoding framework. It suffers the characteristic limitation of any encoder that it cannot explicitly combine multi-timescale information, whereas a characteristic feature of human audio perception is that we tend to perceive all frequencies simultaneously. To this end, we propose a generalization of CLMR that learns to extract and explicitly combine representations across different frequency resolutions, which we coin the tailed U-Net (TUNe). TUNe architectures combine multi-timescale information during a decoding phase, similar to U-Net architectures used in computer vision and source separation, but have a tail added to reduce sample-level information to a smaller pre-defined number of representation dimensions. The size of the decoding phase is a hyperparameter, and in the case of a zero-layer decoding phase, TUNe reduces to CLMR. The best TUNe architectures, however, require less training time to match CLMR performance, have superior transfer learning performance, and are competitive with state-of-the-art models even at dramatically reduced dimensionalities.\n<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1kQbuoxXuO1i1oOevGfvLLGJxYzmzx346)</b>",
      "authors": [
        "V\u00e9lez V\u00e1squez, Marcel A*",
        " Burgoyne, John Ashley"
      ],
      "authors_and_affil": [
        "Marcel A V\u00e9lez V\u00e1squez (Music Cognition Group \u00b7 Institute for Logic, Language, and Computation \u00b7 University of Amsterdam)*",
        " John Ashley Burgoyne (Music Cognition Group \u00b7 Institute for Logic, Language, and Computation \u00b7 University of Amsterdam)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CCP2DG5C",
      "day": "1",
      "keywords": [
        "Domain knowledge -> machine learning/artificial intelligence for music",
        " Musical features and properties -> musical affect, emotion and mood",
        " Musical features and properties -> musical style and genre",
        " MIR fundamentals and methodology -> music signal processing",
        " Musical features and properties -> representations of music",
        "Domain knowledge -> representations of music"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Bengaluru",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000007.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1Gi0Zi7lOvfBM8EVNHvHJ6pUh9VXiwEs7",
      "session": [
        "1"
      ],
      "slack_channel": "p1-07-v\u00e1squez",
      "title": "Tailed U-Net: Multi-Scale Music Representation Learning",
      "video": "https://drive.google.com/uc?export=preview&id=1kQbuoxXuO1i1oOevGfvLLGJxYzmzx346"
    },
    "forum": "109",
    "id": "109",
    "pic_id": "https://drive.google.com/open?id=1DW5rwje8Zjy9MlVC_cYzi7FBnS8GXYLX",
    "position": "07",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "A vocoder is a conditional audio generation model that converts acoustic features such as mel-spectrograms into waveforms. Taking inspiration from Differentiable Digital Signal Processing (DDSP), we propose a new vocoder named SawSing for singing voices. SawSing synthesizes the harmonic part of singing voices by filtering a sawtooth source signal with a linear time-variant finite impulse response filter whose coefficients are estimated from the input mel-spectrogram by a neural network. As this approach enforces phase continuity, SawSing can generate singing voices without the phase-discontinuity glitch of many existing vocoders. Moreover, the source-filter assumption provides an inductive bias that allows SawSing to be trained on a small amount of data. Our evaluation shows that SawSing converges much faster and outperforms state-of-the-art generative adversarial network- and diffusion-based vocoders in a resource-limited scenario with only 3 training recordings and a 3-hour training time.",
      "abstract": "A vocoder is a conditional audio generation model that converts acoustic features such as mel-spectrograms into waveforms. Taking inspiration from Differentiable Digital Signal Processing (DDSP), we propose a new vocoder named SawSing for singing voices. SawSing synthesizes the harmonic part of singing voices by filtering a sawtooth source signal with a linear time-variant finite impulse response filter whose coefficients are estimated from the input mel-spectrogram by a neural network. As this approach enforces phase continuity, SawSing can generate singing voices without the phase-discontinuity glitch of many existing vocoders. Moreover, the source-filter assumption provides an inductive bias that allows SawSing to be trained on a small amount of data. Our evaluation shows that SawSing converges much faster and outperforms state-of-the-art generative adversarial network- and diffusion-based vocoders in a resource-limited scenario with only 3 training recordings and a 3-hour training time.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1SDxWncOrKS39FNOKv09dnFGFUWN_gLZp)</b>",
      "authors": [
        "Wu, Da-Yi*",
        " Hsiao, Wen-Yi",
        " Yang, Fu-Rong",
        " Friedman, Oscar D",
        " Jackson, Warren",
        " bruzenak, scott",
        " Liu, Yi-Wen ",
        " Yang, Yi-Hsuan"
      ],
      "authors_and_affil": [
        "Da-Yi Wu (Academia Sinica)*",
        " Wen-Yi Hsiao (Taiwan AI Labs)",
        " Fu-Rong Yang (National Tsing Hua University)",
        " Oscar D Friedman (470 Music Group)",
        " Warren Jackson (PARC)",
        " scott bruzenak (470 Music Group)",
        " Yi-Wen  Liu (National Tsing Hua University)",
        " Yi-Hsuan Yang (Academia Sinica, Taiwan AI Labs)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04C4QPEXC7",
      "day": "1",
      "keywords": [
        "Domain knowledge -> machine learning/artificial intelligence for music",
        "MIR tasks -> music synthesis and transformation",
        " Musical features and properties -> timbre, instrumentation, and singing voice",
        " MIR fundamentals and methodology -> music signal processing",
        " MIR tasks -> music generation"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Virtually",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000008.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1C7YKJs6kRSEAV9QHikcDd2yPpMgeqVMo",
      "session": [
        "1"
      ],
      "slack_channel": "p1-08-wu",
      "title": "DDSP-based Singing Vocoders: A New Subtractive-based Synthesizer and A Comprehensive Evaluation",
      "video": "https://drive.google.com/uc?export=preview&id=1SDxWncOrKS39FNOKv09dnFGFUWN_gLZp"
    },
    "forum": "85",
    "id": "85",
    "pic_id": "https://drive.google.com/open?id=1cf0bz_-8AOJ3ylqDPbXmvkIocK0QZgtP",
    "position": "08",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "Self-supervised methods have emerged as a promising avenue for representation learning in the recent years since they alleviate the need for labeled datasets, which are scarce and expensive to acquire. \nContrastive methods are a popular choice for self-supervision in the audio domain, and typically provide a learning signal by forcing the model to be invariant to some transformations of the input. These methods, however, require measures such as negative sampling or some form of regularisation to be taken to prevent the model from collapsing on trivial solutions. \nIn this work, instead of invariance, we propose to use equivariance as a self-supervision signal to learn audio tempo representations from unlabelled data. We derive a simple loss function that prevents the network from collapsing on a trivial solution during training, without requiring any form of regularisation or negative sampling.\nOur experiments show that it is possible to learn meaningful representations for tempo estimation by solely relying on equivariant self-supervision, achieving performance comparable with supervised methods on several benchmarks. \nAs an added benefit, our method only requires moderate compute resources and therefore remains accessible to a wide research community.",
      "abstract": "Self-supervised methods have emerged as a promising avenue for representation learning in the recent years since they alleviate the need for labeled datasets, which are scarce and expensive to acquire. \nContrastive methods are a popular choice for self-supervision in the audio domain, and typically provide a learning signal by forcing the model to be invariant to some transformations of the input. These methods, however, require measures such as negative sampling or some form of regularisation to be taken to prevent the model from collapsing on trivial solutions. \nIn this work, instead of invariance, we propose to use equivariance as a self-supervision signal to learn audio tempo representations from unlabelled data. We derive a simple loss function that prevents the network from collapsing on a trivial solution during training, without requiring any form of regularisation or negative sampling.\nOur experiments show that it is possible to learn meaningful representations for tempo estimation by solely relying on equivariant self-supervision, achieving performance comparable with supervised methods on several benchmarks. \nAs an added benefit, our method only requires moderate compute resources and therefore remains accessible to a wide research community.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1vhS9TpAf5qRwlP6rJXTbrYkZcB6D1TqZ)</b>",
      "authors": [
        "Quinton, Elio*"
      ],
      "authors_and_affil": [
        "Elio Quinton (Universal Music Group)*"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CGCUJQJZ",
      "day": "1",
      "keywords": [
        "Musical features and properties -> rhythm, beat, tempo",
        "Musical features and properties -> representations of music"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Bengaluru",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000009.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1V7heVpCGbctSX1GJA-TOQWPIH5MQ0OKj",
      "session": [
        "1"
      ],
      "slack_channel": "p1-09-quinton",
      "title": "Equivariant self-supervision for musical tempo estimation",
      "video": "https://drive.google.com/uc?export=preview&id=1vhS9TpAf5qRwlP6rJXTbrYkZcB6D1TqZ"
    },
    "forum": "117",
    "id": "117",
    "pic_id": "https://drive.google.com/open?id=1PQ7_Ir1nW8-Z13OwBIRHVpqm_huprFNl",
    "position": "09",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "Tools and methodologies for distinguishing computer-generated melodies from human-composed melodies have a broad range of applications from detecting copyright infringement through the evaluation of generative music systems to facilitating transparent and explainable AI. This paper reviews a data challenge on distinguishing computer-generated melodies from human-composed melodies held in association with the Conference on Sound and Music Technology (CSMT) in 2020. An investigation of the submitted systems and the results are presented first. Besides the structure of the proposed models, the paper investigates two important factors that were identified as contributors to good model performance: the specific music features and the music representation used. Through an analysis of the submissions, important melody-related music features have been identified. Encoding or representation of the music in the context of neural network modes has also been found to significantly impact system performance through an experiment where the top-ranked system was re-implemented with different input representations for comparison purposes. Besides demonstrating the feasibility of developing an objective music composition evaluation system, the investigation presented in this paper also reveals some important limitations of current music composition systems opening opportunities for future work in the community.",
      "abstract": "Tools and methodologies for distinguishing computer-generated melodies from human-composed melodies have a broad range of applications from detecting copyright infringement through the evaluation of generative music systems to facilitating transparent and explainable AI. This paper reviews a data challenge on distinguishing computer-generated melodies from human-composed melodies held in association with the Conference on Sound and Music Technology (CSMT) in 2020. An investigation of the submitted systems and the results are presented first. Besides the structure of the proposed models, the paper investigates two important factors that were identified as contributors to good model performance: the specific music features and the music representation used. Through an analysis of the submissions, important melody-related music features have been identified. Encoding or representation of the music in the context of neural network modes has also been found to significantly impact system performance through an experiment where the top-ranked system was re-implemented with different input representations for comparison purposes. Besides demonstrating the feasibility of developing an objective music composition evaluation system, the investigation presented in this paper also reveals some important limitations of current music composition systems opening opportunities for future work in the community.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1Z3JKyr8si0qTBYch4VK9dQyUEBvY3iJa)</b>",
      "authors": [
        "Li, Yuqiang*",
        " Li, Shengchen",
        " Fazekas, George"
      ],
      "authors_and_affil": [
        "Yuqiang Li (Xi\u2019an Jiaotong-Liverpool University)*",
        " Shengchen Li (Xi\u2019an Jiaotong-Liverpool University)",
        " George Fazekas (Queen Mary University of London)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CK86BGAX",
      "day": "1",
      "keywords": [
        "Evaluation, datasets, and reproducibility -> evaluation metrics",
        " Musical features and properties -> melody and motives",
        "Evaluation, datasets, and reproducibility -> evaluation methodology",
        " Musical features and properties -> rhythm, beat, tempo",
        " Musical features and properties -> representations of music"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Virtually",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000010.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1BJ40v5h1FPGquMXF7c2VVHm2P0EM98R1",
      "session": [
        "1"
      ],
      "slack_channel": "p1-10-li",
      "title": "How Music features and Musical Data Representations Affect Objective Evaluation of Music Composition: A Review of CSMT Data Challenge 2020",
      "video": "https://drive.google.com/uc?export=preview&id=1Z3JKyr8si0qTBYch4VK9dQyUEBvY3iJa"
    },
    "forum": "90",
    "id": "90",
    "pic_id": "https://drive.google.com/open?id=1E2V2AqnYPwxxlPCA_h8p3ia0vGnavqQN",
    "position": "10",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "Existing multi-instrumental datasets tend to be biased toward pop and classical music. In addition, they generally lack high-level annotations such as emotion tags. In this paper, we propose YM2413-MDB, an 80s FM video game music dataset with multi-label emotion annotations. It includes 669 audio and MIDI files of music from Sega and MSX PC games in the 80s using YM2413, a programmable sound generator based on FM. The collected game music is arranged with a subset of 15 monophonic instruments and one drum instrument. They were converted from binary commands of the YM2413 sound chip. Each song was labeled with 19 emotion tags by two annotators and validated by three verifiers to obtain refined tags. We provide the baseline models and results for emotion recognition and emotion-conditioned symbolic music generation using YM2413-MDB.",
      "abstract": "Existing multi-instrumental datasets tend to be biased toward pop and classical music. In addition, they generally lack high-level annotations such as emotion tags. In this paper, we propose YM2413-MDB, an 80s FM video game music dataset with multi-label emotion annotations. It includes 669 audio and MIDI files of music from Sega and MSX PC games in the 80s using YM2413, a programmable sound generator based on FM. The collected game music is arranged with a subset of 15 monophonic instruments and one drum instrument. They were converted from binary commands of the YM2413 sound chip. Each song was labeled with 19 emotion tags by two annotators and validated by three verifiers to obtain refined tags. We provide the baseline models and results for emotion recognition and emotion-conditioned symbolic music generation using YM2413-MDB.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1J4-anjMGCpgeGR5zPPkuiTeZpfr7hj6E)</b>",
      "authors": [
        "Choi, Eunjin*",
        " Chung, Yoonjin",
        " Lee, Seolhee",
        " Jeon, JongIk",
        " Kwon, Taegyun",
        " Nam, Juhan"
      ],
      "authors_and_affil": [
        "Eunjin Choi (Graduate School of Culture Technology, KAIST, South Korea)*",
        " Yoonjin Chung (Graduate School of AI, KAIST, South Korea)",
        " Seolhee Lee (Graduate School of Culture Technology, KAIST, South Korea)",
        " JongIk Jeon (Department of Industrial Design, KAIST, South Korea)",
        " Taegyun Kwon (Graduate School of Culture Technology, KAIST, South Korea)",
        " Juhan Nam (Graduate School of Culture Technology, KAIST, South Korea)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CKB936LS",
      "day": "1",
      "keywords": [
        " MIR fundamentals and methodology -> symbolic music processing",
        " MIR tasks -> music generation",
        " Musical features and properties -> musical affect, emotion and mood",
        "Applications -> gaming, augmented/virtual reality",
        "Evaluation, datasets, and reproducibility -> novel datasets and use cases",
        " Domain knowledge -> machine learning/artificial intelligence for music"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Bengaluru",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000011.pdf",
      "poster_pdf": "https://drive.google.com/open?id=15qp-jjYoLk1RKK0o7SyFhcNArFXNgGCc",
      "session": [
        "1"
      ],
      "slack_channel": "p1-11-choi",
      "title": "YM2413-MDB: A Multi-Instrumental FM Video Game Music Dataset with Emotion Annotations",
      "video": "https://drive.google.com/uc?export=preview&id=1J4-anjMGCpgeGR5zPPkuiTeZpfr7hj6E"
    },
    "forum": "153",
    "id": "153",
    "pic_id": "https://drive.google.com/open?id=1EwEe1u1LnfB5mpcws3bQQTg7zADEAupl",
    "position": "11",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "Popularized by Arnold Schoenberg in the mid-20th century, the method of twelve-tone composition produces musical compositions based on one or more orderings of the equal-tempered chromatic scale. The work of twelve-tone composers is famously challenging to traditional Western tonal and structural sensibilities; even so, group theoretic approaches have determined that 10% of certain composers\u2019 works contain a highly unusual classical symmetry of music. We extend this result by revealing many symmetries that were previously undetected in the works of Schoenberg, Webern, and Berg. Our approach is computational rather than group theoretic, scanning each composition for symmetries of many different cardinalities. Thus, we capture partial symmetries that would be overlooked by more formal means. Moreover, our methods are applicable beyond the narrow scope of twelve-tone composition. We achieve our results by first extending the group-theoretic notion of symmetry to encompass shorter motives that may be repeated and reprised in a given composition, and then comparing the incidence of these symmetries between the work of composers and the space of all possible 12-tone rows. We present four candidate hierarchies of symmetry and show that in each model, between 75% and 95% of actual compositions contained high levels of internal symmetry.",
      "abstract": "Popularized by Arnold Schoenberg in the mid-20th century, the method of twelve-tone composition produces musical compositions based on one or more orderings of the equal-tempered chromatic scale. The work of twelve-tone composers is famously challenging to traditional Western tonal and structural sensibilities; even so, group theoretic approaches have determined that 10% of certain composers\u2019 works contain a highly unusual classical symmetry of music. We extend this result by revealing many symmetries that were previously undetected in the works of Schoenberg, Webern, and Berg. Our approach is computational rather than group theoretic, scanning each composition for symmetries of many different cardinalities. Thus, we capture partial symmetries that would be overlooked by more formal means. Moreover, our methods are applicable beyond the narrow scope of twelve-tone composition. We achieve our results by first extending the group-theoretic notion of symmetry to encompass shorter motives that may be repeated and reprised in a given composition, and then comparing the incidence of these symmetries between the work of composers and the space of all possible 12-tone rows. We present four candidate hierarchies of symmetry and show that in each model, between 75% and 95% of actual compositions contained high levels of internal symmetry.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1nfO9PGQU-rShoT5ZfgvQsNdoCzVDrR-B)</b>",
      "authors": [
        "Venkatesh, Anil*",
        " Sachdev, Viren"
      ],
      "authors_and_affil": [
        "Anil Venkatesh (Department of Mathematics and Computer Science, Adelphi University)*",
        " Viren Sachdev (Department of Mathematics and Computer Science, Adelphi University)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04D92GPTL0",
      "day": "1",
      "keywords": [
        "Domain knowledge -> computational music theory and musicology",
        " Musical features and properties -> structure, segmentation, and form",
        " Musical features and properties -> melody and motives",
        "Domain knowledge -> representations of music"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Virtually",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000012.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1jmL7YfNTUBFOB6Et1wSw_5Qxs15U-Cx5",
      "session": [
        "1"
      ],
      "slack_channel": "p1-12-venkatesh",
      "title": "Detecting Symmetries of All Cardinalities With Application to Musical 12-Tone Rows",
      "video": "https://drive.google.com/uc?export=preview&id=1nfO9PGQU-rShoT5ZfgvQsNdoCzVDrR-B"
    },
    "forum": "133",
    "id": "133",
    "pic_id": "https://drive.google.com/open?id=1Mq_kbHOiQTUVRLRpZRpABkPThOrrMHi3",
    "position": "12",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "In the previous decade, Deep Learning (DL) has proven to be one of the most effective machine learning methods to tackle a wide range of Music Information Retrieval (MIR) tasks. It offers highly expressive learning capacity that can fit any music representation needed for MIR-relevant downstream tasks. However, it has been criticized for sacrificing interpretability. On the other hand, the Bayesian nonparametric (BN) approach promises similar positive properties as DL, such as high flexibility, while being robust to overfitting and preserving interpretability. Therefore, the primary motivation of this work is to explore the potential of Bayesian nonparametric models in comparison to DL models for music representation learning. More specifically, we assess the music representation learned from the Hierarchical Dirichlet Process Gaussian Mixture Model (HDPGMM), an infinite mixture model based on the Bayesian nonparametric approach, to MIR tasks, including classification, auto-tagging, and recommendation. The experimental result suggests that the HDPGMM music representation can outperform DL representations in certain scenarios, and overall comparable.",
      "abstract": "In the previous decade, Deep Learning (DL) has proven to be one of the most effective machine learning methods to tackle a wide range of Music Information Retrieval (MIR) tasks. It offers highly expressive learning capacity that can fit any music representation needed for MIR-relevant downstream tasks. However, it has been criticized for sacrificing interpretability. On the other hand, the Bayesian nonparametric (BN) approach promises similar positive properties as DL, such as high flexibility, while being robust to overfitting and preserving interpretability. Therefore, the primary motivation of this work is to explore the potential of Bayesian nonparametric models in comparison to DL models for music representation learning. More specifically, we assess the music representation learned from the Hierarchical Dirichlet Process Gaussian Mixture Model (HDPGMM), an infinite mixture model based on the Bayesian nonparametric approach, to MIR tasks, including classification, auto-tagging, and recommendation. The experimental result suggests that the HDPGMM music representation can outperform DL representations in certain scenarios, and overall comparable.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1OKpPNQA3fv3oE7wp6YtMAxaTGTGYPFHu)</b>",
      "authors": [
        "Kim, Jaehun*",
        " Liem, Cynthia C. S."
      ],
      "authors_and_affil": [
        "Jaehun Kim (Delft University of Technology)*",
        " Cynthia C. S. Liem (Delft University of Technology)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04C4QQLQHM",
      "day": "1",
      "keywords": [
        " Evaluation, datasets, and reproducibility -> MIR tasks",
        "Applications -> music recommendation and playlist generation",
        " MIR tasks -> similarity metrics",
        "Musical features and properties -> representations of music",
        " Domain knowledge -> machine learning/artificial intelligence for music",
        " MIR tasks -> automatic classification"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Bengaluru",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000013.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1fR3Y9ikhCmtYhWan0_kIqEyZmhpQr-ul",
      "session": [
        "1"
      ],
      "slack_channel": "p1-13-kim",
      "title": "The power of deep without going deep? A study of HDPGMM music representation learning",
      "video": "https://drive.google.com/uc?export=preview&id=1OKpPNQA3fv3oE7wp6YtMAxaTGTGYPFHu"
    },
    "forum": "234",
    "id": "234",
    "pic_id": "https://drive.google.com/open?id=1iv7g-YI6ZV6UgbRa8neYU3Hgj0lHpEVR",
    "position": "13",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "Research on music generation using deep learning has attracted more attention; in particular, Transformer-based models have succeeded in generating coherent musical pieces. Recently, an increasing number of studies have focused on phrases that are smaller musical units, and several studies have addressed phrase-level control. In this study, we propose a method for sequentially generating a piece that enables the control of each phrase length and, consequently, the length of the entire piece. We added PHRASE and a new event, BAR COUNTDOWN, which indicates the number of bars remaining in the phrase, to the existing event-based music representations. To reflect user input indicating the phrase lengths of the piece being generated, we used an autoregressive generation model that adds these two events to the generated event-token sequence based on the user input and uses it as input for the next time step. Subjective listening tests revealed that the pieces generated by our methods possessed designated phrase lengths and ended naturally at the determined length.",
      "abstract": "Research on music generation using deep learning has attracted more attention; in particular, Transformer-based models have succeeded in generating coherent musical pieces. Recently, an increasing number of studies have focused on phrases that are smaller musical units, and several studies have addressed phrase-level control. In this study, we propose a method for sequentially generating a piece that enables the control of each phrase length and, consequently, the length of the entire piece. We added PHRASE and a new event, BAR COUNTDOWN, which indicates the number of bars remaining in the phrase, to the existing event-based music representations. To reflect user input indicating the phrase lengths of the piece being generated, we used an autoregressive generation model that adds these two events to the generated event-token sequence based on the user input and uses it as input for the next time step. Subjective listening tests revealed that the pieces generated by our methods possessed designated phrase lengths and ended naturally at the determined length.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1Vzij-z2_GQnhf2pjWhJFr8Abf8Zyel5o)</b>",
      "authors": [
        "Naruse, Daiki*",
        " Takahata, Tomoyuki",
        " Mukuta, Yusuke",
        " Harada, Tatsuya"
      ],
      "authors_and_affil": [
        "Daiki Naruse (The University of Tokyo, Japan)*",
        " Tomoyuki Takahata (The University of Tokyo, Japan)",
        " Yusuke Mukuta (The University of Tokyo, Japan, RIKEN, Japan)",
        " Tatsuya Harada (The University of Tokyo, Japan, RIKEN, Japan)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CK8884BD",
      "day": "1",
      "keywords": [
        "Domain knowledge -> machine learning/artificial intelligence for music",
        " Domain knowledge -> representations of music",
        " Musical features and properties -> structure, segmentation, and form",
        " Musical features and properties -> representations of music",
        "MIR tasks -> music generation"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Virtually",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000014.pdf",
      "poster_pdf": "https://drive.google.com/open?id=17QxnZvs4cgAwXZXQlFXEWV40zKsY2Y8T",
      "session": [
        "1"
      ],
      "slack_channel": "p1-14-naruse",
      "title": "Pop Music Generation with Controllable Phrase Lengths",
      "video": "https://drive.google.com/uc?export=preview&id=1Vzij-z2_GQnhf2pjWhJFr8Abf8Zyel5o"
    },
    "forum": "322",
    "id": "322",
    "pic_id": "https://drive.google.com/open?id=1_nK4C4awvigUxpDU74xagmolynRW1Wac",
    "position": "14",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "While generative adversarial networks (GANs) have been widely used in research on audio generation, the training of a GAN model is known to be unstable, time consuming, and data inefficient. Among the attempts to ameliorate the training process of GANs, the idea of Projected GAN emerges as an effective solution for GAN-based image generation, establishing the state-of-the-art in different image applications. The core idea is to use a pre-trained classifier to constrain the feature space of the discriminator to stabilize and improve GAN training. This paper investigates whether Projected GAN can similarly improve audio generation, by evaluating the performance of a StyleGAN2-based audio-domain loop generation model with and without using a pre-trained feature space in the discriminator. Moreover, we compare the performance of using a general versus domain-specific classifier as the pre-trained audio classifier. With experiments on both drum loop and synth loop generation, we show that a general audio classifier works better, and that with Projected GAN our loop generation models can converge around 5 times faster without performance degradation.",
      "abstract": "While generative adversarial networks (GANs) have been widely used in research on audio generation, the training of a GAN model is known to be unstable, time consuming, and data inefficient. Among the attempts to ameliorate the training process of GANs, the idea of Projected GAN emerges as an effective solution for GAN-based image generation, establishing the state-of-the-art in different image applications. The core idea is to use a pre-trained classifier to constrain the feature space of the discriminator to stabilize and improve GAN training. This paper investigates whether Projected GAN can similarly improve audio generation, by evaluating the performance of a StyleGAN2-based audio-domain loop generation model with and without using a pre-trained feature space in the discriminator. Moreover, we compare the performance of using a general versus domain-specific classifier as the pre-trained audio classifier. With experiments on both drum loop and synth loop generation, we show that a general audio classifier works better, and that with Projected GAN our loop generation models can converge around 5 times faster without performance degradation.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1CtRVNZ7FPPKD_GEB_jMdS0gh4EFEphoZ)</b>",
      "authors": [
        "Yeh, Yen-Tung *",
        " Yang, Yi-Hsuan",
        " Chen, Bo-Yu"
      ],
      "authors_and_affil": [
        "Yen-Tung  Yeh (Academia Sinica, National Taiwan University)*",
        " Yi-Hsuan Yang (Academia Sinica, Taiwan AI Labs)",
        " Bo-Yu Chen (Academia Sinica, National Taiwan University)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CMQVEKSQ",
      "day": "1",
      "keywords": [
        "Domain knowledge -> machine learning/artificial intelligence for music",
        " MIR tasks -> music synthesis and transformation",
        " Musical features and properties -> representations of music",
        "MIR tasks -> music generation"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Bengaluru",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000015.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1K56Pf8bBlUZ7R0iCX1gR6vDM1-iz4c_G",
      "session": [
        "1"
      ],
      "slack_channel": "p1-15-yeh",
      "title": "Exploiting Pre-trained Feature Networks for Generative Adversarial Networks in Audio-domain Loop Generation",
      "video": "https://drive.google.com/uc?export=preview&id=1CtRVNZ7FPPKD_GEB_jMdS0gh4EFEphoZ"
    },
    "forum": "169",
    "id": "169",
    "pic_id": "https://drive.google.com/open?id=1YwOACtnBvrwrXbLhSNyfaqrUh2AQcSoq",
    "position": "15",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "Creating a pop song melody according to pre-written lyrics is a typical practice for composers. A computational model of how lyrics are set as melodies is important for automatic composition systems, but an end-to-end lyric-to-melody model would require enormous amounts of paired training data. To mitigate the data constraints, we adopt a two-stage approach, dividing the task into lyric-to-rhythm and rhythm-to-melody modules. However, the lyric-to-rhythm task is still challenging due to its multimodality. In this paper, we propose a novel lyric-to-rhythm framework that includes part-of-speech tags to achieve better text-setting, and a Transformer architecture designed to model long-term syllable-to-note associations. For the rhythm-to-melody task, we adapt a proven chord-conditioned melody Transformer, which has achieved state-of-the-art results. Experiments for Chinese lyric-to-melody generation show that the proposed framework is able to model key characteristics of rhythm and pitch distributions in the dataset, and in a subjective evaluation, the melodies generated by our system were rated as similar to or better than those of a state-of-the-art alternative.",
      "abstract": "Creating a pop song melody according to pre-written lyrics is a typical practice for composers. A computational model of how lyrics are set as melodies is important for automatic composition systems, but an end-to-end lyric-to-melody model would require enormous amounts of paired training data. To mitigate the data constraints, we adopt a two-stage approach, dividing the task into lyric-to-rhythm and rhythm-to-melody modules. However, the lyric-to-rhythm task is still challenging due to its multimodality. In this paper, we propose a novel lyric-to-rhythm framework that includes part-of-speech tags to achieve better text-setting, and a Transformer architecture designed to model long-term syllable-to-note associations. For the rhythm-to-melody task, we adapt a proven chord-conditioned melody Transformer, which has achieved state-of-the-art results. Experiments for Chinese lyric-to-melody generation show that the proposed framework is able to model key characteristics of rhythm and pitch distributions in the dataset, and in a subjective evaluation, the melodies generated by our system were rated as similar to or better than those of a state-of-the-art alternative.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1_MyDbDMkhm5zguPcbYdhhixKbvPi690L)</b>",
      "authors": [
        "zhang, daiyu*",
        " Wang, Ju-Chiang",
        " Kosta, Katerina",
        " Smith, Jordan B. L.",
        " Zhou, Shicen"
      ],
      "authors_and_affil": [
        "Daiyu Zhang (ByteDance)*",
        " Ju-Chiang Wang (ByteDance)",
        " Katerina Kosta (ByteDance)",
        " Jordan B. L. Smith (ByteDance)",
        " Shicen Zhou (ByteDance)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04C4QRA7T9",
      "day": "1",
      "keywords": [
        "MIR tasks -> music generation",
        "MIR fundamentals and methodology -> multimodality"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Bengaluru",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000016.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1wbQvC1h9DCL0xK5fEa5Ta7DAIXlxYuPv",
      "session": [
        "1"
      ],
      "slack_channel": "p1-16-zhang",
      "title": "Modeling the rhythm from lyrics for melody generation of pop songs",
      "video": "https://drive.google.com/uc?export=preview&id=1_MyDbDMkhm5zguPcbYdhhixKbvPi690L"
    },
    "forum": "306",
    "id": "306",
    "pic_id": "https://drive.google.com/open?id=1Uc9UyW1mNMb1_plCea6YzIvJvD547x2f",
    "position": "16",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "1"
  },
  {
    "content": {
      "TLDR": "We propose a visual approach for interactive, AI-assisted composition that serves as a compromise between fully automatic and fully manual composition. Instead of generating a whole piece, the AI takes on the role of an assistant that generates short melodies for the composer to choose from and adapt. In an iterative process, the composer queries the AI for continuations or alternative fill-ins, chooses a suggestion, and adds it to the piece. As listening to many suggestions would take time, we explore different ways to visualize them, to allow the composer to focus on the most interesting-looking melodies. We also present the results of a qualitative evaluation with five composers.",
      "abstract": "We propose a visual approach for interactive, AI-assisted composition that serves as a compromise between fully automatic and fully manual composition. Instead of generating a whole piece, the AI takes on the role of an assistant that generates short melodies for the composer to choose from and adapt. In an iterative process, the composer queries the AI for continuations or alternative fill-ins, chooses a suggestion, and adds it to the piece. As listening to many suggestions would take time, we explore different ways to visualize them, to allow the composer to focus on the most interesting-looking melodies. We also present the results of a qualitative evaluation with five composers.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1iwMm4pJoaaX9CiFzRMWt6gisY-ixIs1H)</b>",
      "authors": [
        "Rau, Simeon*",
        " Heyen, Frank",
        " Wagner, Stefan",
        " Sedlmair, Michael"
      ],
      "authors_and_affil": [
        "Simeon Rau (VISUS, University of Stuttgart, Germany)*",
        " Frank Heyen (VISUS, University of Stuttgart, Germany)",
        " Stefan Wagner (ISTE, University of Stuttgart, Germany) ",
        " Michael Sedlmair (VISUS, University of Stuttgart, Germany)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CKB9G3AN",
      "day": "1",
      "keywords": [
        "Human-centered MIR",
        " Human-centered MIR -> music interfaces and services",
        " Musical features and properties -> representations of music",
        " MIR tasks -> similarity metrics",
        "Applications -> music composition"
      ],
      "long_presentation": "TRUE",
      "paper_presentation": "In-person, in Bengaluru",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000017.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1b1B4uejvzqP1k9azq7kPUpTVrqYEHmEH",
      "session": [
        "2"
      ],
      "slack_channel": "p2-01-rau",
      "title": "Visualization for AI-Assisted Composing",
      "video": "https://drive.google.com/uc?export=preview&id=1iwMm4pJoaaX9CiFzRMWt6gisY-ixIs1H"
    },
    "forum": "217",
    "id": "217",
    "pic_id": "https://drive.google.com/open?id=1cUT34kayLNSRJwp3_wya0FXUBIXHx7x_",
    "position": "01",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "2"
  },
  {
    "content": {
      "TLDR": "Various features \u2013 from low-level acoustics, to higher-level statistical regularities, to memory associations \u2013 contribute to the experience of musical enjoyment and pleasure. Recent work suggests that musical surprisal, that is, the unexpectedness of a musical event given its context, may directly predict listeners\u2019 experiences of pleasure and enjoyment during music listening. Understanding how surprisal shapes listeners\u2019 preferences for certain musical pieces has implications for music recommender systems, which are typically content- (both acoustic or semantic) or metadata-based. Here we test a recently developed computational algorithm, called Dynamic-Regularity Extraction (D-REX), that uses Bayesian inference to predict the surprisal that humans experience while listening to music. We demonstrate that the brain tracks musical surprisal as modeled by D-REX by conducting a decoding analysis on the neural signal (collected through magnetoencephalography) of participants listening to music. Thus, we demonstrate the validity of a computational model of musical surprisal, which may remarkably inform the next generation of recommender systems. In addition, we present an open-source neural dataset which will be available for future research to foster approaches combining MIR with cognitive neuroscience, an approach we believe will be a key strategy in characterizing people\u2019s reactions to music.  ",
      "abstract": "Various features \u2013 from low-level acoustics, to higher-level statistical regularities, to memory associations \u2013 contribute to the experience of musical enjoyment and pleasure. Recent work suggests that musical surprisal, that is, the unexpectedness of a musical event given its context, may directly predict listeners\u2019 experiences of pleasure and enjoyment during music listening. Understanding how surprisal shapes listeners\u2019 preferences for certain musical pieces has implications for music recommender systems, which are typically content- (both acoustic or semantic) or metadata-based. Here we test a recently developed computational algorithm, called Dynamic-Regularity Extraction (D-REX), that uses Bayesian inference to predict the surprisal that humans experience while listening to music. We demonstrate that the brain tracks musical surprisal as modeled by D-REX by conducting a decoding analysis on the neural signal (collected through magnetoencephalography) of participants listening to music. Thus, we demonstrate the validity of a computational model of musical surprisal, which may remarkably inform the next generation of recommender systems. In addition, we present an open-source neural dataset which will be available for future research to foster approaches combining MIR with cognitive neuroscience, an approach we believe will be a key strategy in characterizing people\u2019s reactions to music.  <br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=151IWNyIDcSD92I4YbjtbFabc1YobsK4X)</b>",
      "authors": [
        "Abrams, Ellie Bean*",
        " Mu\u00f1oz Vidal, Eva",
        " Pelofi, Claire",
        " Ripoll\u00e9s, Pablo"
      ],
      "authors_and_affil": [
        "Ellie Bean Abrams (Music and Audio Research Laboratory, New York University, Center for Language, Music, and Emotion, New York University, Department of Psychology, New York University)*",
        " Eva Mu\u00f1oz Vidal (Music and Audio Research Laboratory, New York University, Center for Language, Music, and Emotion, New York University, Department of Psychology, New York University)",
        " Claire Pelofi (Music and Audio Research Laboratory, New York University, Center for Language, Music, and Emotion, New York University)",
        " Pablo Ripoll\u00e9s (Music and Audio Research Laboratory, New York University, Center for Language, Music, and Emotion, New York University, Department of Psychology, New York University)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CMQWFYBW",
      "day": "1",
      "keywords": [
        "Domain knowledge -> cognitive MIR",
        " Musical features and properties -> musical affect, emotion and mood",
        "Applications -> music recommendation and playlist generation",
        " Human-centered MIR -> personalization"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Virtually",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000018.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1jQBDLr4IaH20ym-U5jfXoHni3GC8xUDL",
      "session": [
        "2"
      ],
      "slack_channel": "p2-02-abrams",
      "title": "Retrieving musical information from neural data: how cognitive features enrich acoustic ones",
      "video": "https://drive.google.com/uc?export=preview&id=151IWNyIDcSD92I4YbjtbFabc1YobsK4X"
    },
    "forum": "305",
    "id": "305",
    "pic_id": "https://drive.google.com/open?id=1OkcvT7xUU8tzVHvUF5hCZ3SiO5Iulsvm",
    "position": "02",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "2"
  },
  {
    "content": {
      "TLDR": "We propose Beat Transformer, a novel Transformer encoder architecture for joint beat and downbeat tracking. Different from previous models that track beats solely based on the spectrogram of an audio mixture, our model deals with demixed spectrograms with multiple instrument channels. This is inspired by the fact that humans perceive metrical structures from richer musical contexts, such as chord progression and instrumentation. To this end, we develop a Transformer model with both time-wise attention and instrument-wise attention to capture deep-buried metrical cues. Moreover, our model adopts a novel dilated self-attention mechanism, which achieves powerful hierarchical modelling with only linear complexity. Experiments demonstrate a significant improvement in demixed beat tracking over the non-demixed version. Also, Beat Transformer achieves up to 4% point improvement in downbeat tracking accuracy over the TCN architectures. We further discover an interpretable attention pattern that mirrors our understanding of hierarchical metrical structures.",
      "abstract": "We propose Beat Transformer, a novel Transformer encoder architecture for joint beat and downbeat tracking. Different from previous models that track beats solely based on the spectrogram of an audio mixture, our model deals with demixed spectrograms with multiple instrument channels. This is inspired by the fact that humans perceive metrical structures from richer musical contexts, such as chord progression and instrumentation. To this end, we develop a Transformer model with both time-wise attention and instrument-wise attention to capture deep-buried metrical cues. Moreover, our model adopts a novel dilated self-attention mechanism, which achieves powerful hierarchical modelling with only linear complexity. Experiments demonstrate a significant improvement in demixed beat tracking over the non-demixed version. Also, Beat Transformer achieves up to 4% point improvement in downbeat tracking accuracy over the TCN architectures. We further discover an interpretable attention pattern that mirrors our understanding of hierarchical metrical structures.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1Q0LsWMOByUsHhXlkArzV0pB0tSlxmos-)</b>",
      "authors": [
        "Zhao, Jingwei*",
        " Xia, Gus",
        " Wang, Ye"
      ],
      "authors_and_affil": [
        "Jingwei Zhao (Institute of Data Science, NUS, Integrative Sciences and Engineering Programme, NUS Graduate School)*",
        " Gus Xia (Music X Lab, NYU Shanghai, MBZUAI)",
        " Ye Wang (School of Computing, NUS, Institute of Data Science, NUS, Integrative Sciences and Engineering Programme, NUS Graduate School)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CY0M36E5",
      "day": "1",
      "keywords": [
        " Domain knowledge -> machine learning/artificial intelligence for music",
        " Musical features and properties -> rhythm, beat, tempo",
        "Applications -> music retrieval systems",
        "Musical features and properties"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Bengaluru",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000019.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1DZ3voqJPBwMMe2AQRcvkcOS36iInirKG",
      "session": [
        "2"
      ],
      "slack_channel": "p2-03-zhao",
      "title": "Beat Transformer: Demixed Beat and Downbeat Tracking with Dilated Self-Attention",
      "video": "https://drive.google.com/uc?export=preview&id=1Q0LsWMOByUsHhXlkArzV0pB0tSlxmos-"
    },
    "forum": "72",
    "id": "72",
    "pic_id": "https://drive.google.com/open?id=1xfyHMhPnqYcs3PpUGX_biIKMpNMrTQL2",
    "position": "03",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "2"
  },
  {
    "content": {
      "TLDR": "We propose a system for rendering a symbolic piano performance with flexible musical expression. It is necessary to actively control musical expression for creating a new music performance that conveys various emotions or nuances. However, previous approaches were limited to following the composer's guidelines of musical expression or dealing with only a part of the musical attributes. We aim to disentangle the entire musical expression and structural attribute of piano performance using a conditional VAE framework. It stochastically generates expressive parameters from latent representations and given note structures. In addition, we employ self-supervised approaches that force the latent variables to represent target attributes. Finally, we leverage a two-step encoder and decoder that learn hierarchical dependency to enhance the naturalness of the output. Experimental results show that our system can stably generate performance parameters relevant to the given musical scores, learn disentangled representations, and control musical attributes independently of each other.",
      "abstract": "We propose a system for rendering a symbolic piano performance with flexible musical expression. It is necessary to actively control musical expression for creating a new music performance that conveys various emotions or nuances. However, previous approaches were limited to following the composer's guidelines of musical expression or dealing with only a part of the musical attributes. We aim to disentangle the entire musical expression and structural attribute of piano performance using a conditional VAE framework. It stochastically generates expressive parameters from latent representations and given note structures. In addition, we employ self-supervised approaches that force the latent variables to represent target attributes. Finally, we leverage a two-step encoder and decoder that learn hierarchical dependency to enhance the naturalness of the output. Experimental results show that our system can stably generate performance parameters relevant to the given musical scores, learn disentangled representations, and control musical attributes independently of each other.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1n_HZ-Kgko68D0YoROBida8ITvuXoG7gb)</b>",
      "authors": [
        "Rhyu, Seungyeon*",
        " Kim, Sarah",
        " Lee, Kyogu"
      ],
      "authors_and_affil": [
        "Seungyeon Rhyu (Music and Audio Research Group, Seoul National University, South Korea)*",
        " Sarah Kim (Krust Universe, South Korea)",
        " Kyogu Lee (Music and Audio Research Group, Seoul National University, South Korea, Graduate School of AI, AI Institute, Seoul National University, South Korea)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CK86SZ2P",
      "day": "1",
      "keywords": [
        "Musical features and properties -> expression and performative aspects of music",
        "MIR tasks -> music generation"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Virtually",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000020.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1pbzok9hZv7VqxQ-T4wwMA66IpVOLMl-w",
      "session": [
        "2"
      ],
      "slack_channel": "p2-04-rhyu",
      "title": "Sketching the Expression: Flexible Rendering of Expressive Piano Performance with Self-Supervised Learning",
      "video": "https://drive.google.com/uc?export=preview&id=1n_HZ-Kgko68D0YoROBida8ITvuXoG7gb"
    },
    "forum": "151",
    "id": "151",
    "pic_id": "https://drive.google.com/open?id=1-YtYLwtg6K029Cx5XidtQG6jC2kg5x-M",
    "position": "04",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "2"
  },
  {
    "content": {
      "TLDR": "As music has become more available especially on music streaming platforms, people have started to have distinct preferences to fit to their varying listening situations, also known as context. Hence, there has been a growing interest in considering the user's situation when recommending music to users. Previous works have proposed personalized auto-taggers to infer situation-related tags from music content and user's global listening preferences. However, in a practical music retrieval system, these context-aware auto-tagger could be only used by assuming that the context class is explicitly provided by the user. In this work, for designing a fully automatised music retrieval system, we propose to disambiguate the user's listening information from stream data. Namely, we propose a system which can generate a situational playlist for a user at a certain time first by leveraging personalized music auto-taggers, and second by automatically inferring the user's situation from stream data (e.g. device, network) and user's general profile information (e.g. age). Experiments show that such a personalized context-aware music retrieval system is feasible, but the performance suffers in the case of new users, new tracks or when the number of context classes increases. ",
      "abstract": "As music has become more available especially on music streaming platforms, people have started to have distinct preferences to fit to their varying listening situations, also known as context. Hence, there has been a growing interest in considering the user's situation when recommending music to users. Previous works have proposed personalized auto-taggers to infer situation-related tags from music content and user's global listening preferences. However, in a practical music retrieval system, these context-aware auto-tagger could be only used by assuming that the context class is explicitly provided by the user. In this work, for designing a fully automatised music retrieval system, we propose to disambiguate the user's listening information from stream data. Namely, we propose a system which can generate a situational playlist for a user at a certain time first by leveraging personalized music auto-taggers, and second by automatically inferring the user's situation from stream data (e.g. device, network) and user's general profile information (e.g. age). Experiments show that such a personalized context-aware music retrieval system is feasible, but the performance suffers in the case of new users, new tracks or when the number of context classes increases. <br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1Q5wzXTun1KIoKdLnhZ-xJP736VQRhfef)</b>",
      "authors": [
        "Ibrahim, Karim M.*",
        " Epure, Elena V.",
        " Peeters, Geoffroy",
        " Richard, Ga\u00ebl"
      ],
      "authors_and_affil": [
        "Karim M. Ibrahim (LTCI, T\u00e9l\u00e9com Paris, Institut Polytechnique de Paris, Deezer Research)*",
        " Elena V. Epure (Deezer Research)",
        " Geoffroy Peeters (LTCI, T\u00e9l\u00e9com Paris, Institut Polytechnique de Paris)",
        " Ga\u00ebl Richard (LTCI, T\u00e9l\u00e9com Paris, Institut Polytechnique de Paris)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CKB8PDEE",
      "day": "1",
      "keywords": [
        " MIR fundamentals and methodology -> metadata, tags, linked data, and semantic web",
        " Human-centered MIR -> personalization",
        "MIR tasks -> automatic classification",
        " Musical features and properties -> musical affect, emotion and mood",
        "Evaluation, datasets, and reproducibility -> novel datasets and use cases",
        " Human-centered MIR -> user behavior analysis and mining"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Bengaluru",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000021.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1cS7-ehKq1zxEizn1sWvGGZitdr8hAS64",
      "session": [
        "2"
      ],
      "slack_channel": "p2-05-ibrahim",
      "title": "Exploiting Device and Audio Data to Tag Music with User-Aware Listening Contexts",
      "video": "https://drive.google.com/uc?export=preview&id=1Q5wzXTun1KIoKdLnhZ-xJP736VQRhfef"
    },
    "forum": "118",
    "id": "118",
    "pic_id": "https://drive.google.com/open?id=1TgmfGOK_n1KFUMe-X1eMIaiL1lsEGqKM",
    "position": "05",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "2"
  },
  {
    "content": {
      "TLDR": "This paper proposes a model that generates a drum track in the audio domain to play along to a user-provided drum-free recording. Specifically, using paired data of drumless tracks and the corresponding human-made drum tracks, we train a Transformer model to improvise the drum part of an unseen drumless recording. We combine two approaches to encode the input audio. First, we train a vector-quantized variational autoencoder (VQ-VAE) to represent the input audio with discrete codes, which can then be readily used in a Transformer. Second, using an audio- domain beat tracking model, we compute beat-related features of the input audio and use them as embeddings in the Transformer. Instead of generating the drum track directly as waveforms, we use a separate VQ-VAE to encode the mel-spectrogram of a drum track into another set of discrete codes, and train the Transformer to predict the sequence of drum-related discrete codes. The output codes are then converted to a mel-spectrogram with a decoder, and then to the waveform with a vocoder. We report both objective and subjective evaluations of variants of the proposed model, demonstrating that the model with beat information generates drum accompaniment that is rhythmically and stylistically consistent with the input audio.",
      "abstract": "This paper proposes a model that generates a drum track in the audio domain to play along to a user-provided drum-free recording. Specifically, using paired data of drumless tracks and the corresponding human-made drum tracks, we train a Transformer model to improvise the drum part of an unseen drumless recording. We combine two approaches to encode the input audio. First, we train a vector-quantized variational autoencoder (VQ-VAE) to represent the input audio with discrete codes, which can then be readily used in a Transformer. Second, using an audio- domain beat tracking model, we compute beat-related features of the input audio and use them as embeddings in the Transformer. Instead of generating the drum track directly as waveforms, we use a separate VQ-VAE to encode the mel-spectrogram of a drum track into another set of discrete codes, and train the Transformer to predict the sequence of drum-related discrete codes. The output codes are then converted to a mel-spectrogram with a decoder, and then to the waveform with a vocoder. We report both objective and subjective evaluations of variants of the proposed model, demonstrating that the model with beat information generates drum accompaniment that is rhythmically and stylistically consistent with the input audio.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1g_E3V-GrV16hpaKTk4IAbvlK8_FgJTF8)</b>",
      "authors": [
        "Wu, Yueh-Kao*",
        " Chiu, Ching-Yu",
        " Yang, Yi-Hsuan"
      ],
      "authors_and_affil": [
        "Yueh-Kao Wu (Academia Sinica)*",
        " Ching-Yu Chiu (National Cheng Kung University)",
        " Yi-Hsuan Yang (Taiwan AI Labs)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CMQVCF2Q",
      "day": "1",
      "keywords": [
        "Domain knowledge -> machine learning/artificial intelligence for music",
        " Musical features and properties -> rhythm, beat, tempo",
        "MIR tasks -> music generation"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Virtually",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000022.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1kzWgTqJOxZyovhRzsrSMi42bQT5NC_CZ",
      "session": [
        "2"
      ],
      "slack_channel": "p2-06-wu",
      "title": "Jukedrummer: Conditional Beat-aware Audio-domain Drum Accompaniment Generation via Transformer VQ-VAE",
      "video": "https://drive.google.com/uc?export=preview&id=1g_E3V-GrV16hpaKTk4IAbvlK8_FgJTF8"
    },
    "forum": "164",
    "id": "164",
    "pic_id": "https://drive.google.com/open?id=1y-TG4fD3WmNiIvaNdnV5imS3N1_FEyTD",
    "position": "06",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "2"
  },
  {
    "content": {
      "TLDR": "Music contains hierarchical structures beyond beats and measures. While hierarchical structure annotations are helpful for music information retrieval and computer musicology, such annotations are scarce in current digital music databases. In this paper, we explore a data-driven approach to automatically extract hierarchical metrical structures from scores. We propose a new model with a Temporal Convolutional Network-Conditional Random Field (TCN-CRF) architecture. Given a symbolic music score, our model takes in an arbitrary number of voices in a beat-quantized form, and predicts a 4-level hierarchical metrical structure from downbeat-level to section-level. We also annotate a dataset using RWC-POP MIDI files to facilitate training and evaluation. We show by experiments that the proposed method performs better than the rule-based approach under different orchestration settings. We also perform some simple musicological analysis on the model predictions. All demos, datasets and pre-trained models are publicly available on Github.",
      "abstract": "Music contains hierarchical structures beyond beats and measures. While hierarchical structure annotations are helpful for music information retrieval and computer musicology, such annotations are scarce in current digital music databases. In this paper, we explore a data-driven approach to automatically extract hierarchical metrical structures from scores. We propose a new model with a Temporal Convolutional Network-Conditional Random Field (TCN-CRF) architecture. Given a symbolic music score, our model takes in an arbitrary number of voices in a beat-quantized form, and predicts a 4-level hierarchical metrical structure from downbeat-level to section-level. We also annotate a dataset using RWC-POP MIDI files to facilitate training and evaluation. We show by experiments that the proposed method performs better than the rule-based approach under different orchestration settings. We also perform some simple musicological analysis on the model predictions. All demos, datasets and pre-trained models are publicly available on Github.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1jWSzDrSgqMBs91Mi0uRIGMR0BU3YuuNw)</b>",
      "authors": [
        "Jiang, Junyan*",
        " Chin, Daniel",
        " Zhang, Yixiao",
        " Xia, Gus"
      ],
      "authors_and_affil": [
        "Junyan Jiang (Music X Lab, NYU Shanghai, MBZUAI)*",
        " Daniel Chin (Music X Lab, NYU Shanghai, MBZUAI)",
        " Yixiao Zhang (Music X Lab, NYU Shanghai, Centre for Digital Music, QMUL)",
        " Gus Xia (Music X Lab, NYU Shanghai, MBZUAI)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04D92GTCQG",
      "day": "1",
      "keywords": [
        " MIR fundamentals and methodology -> symbolic music processing",
        "Musical features and properties -> structure, segmentation, and form",
        " Musical features and properties",
        "Domain knowledge -> computational music theory and musicology",
        " Musical features and properties -> rhythm, beat, tempo",
        " MIR tasks -> pattern matching and detection"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Virtually",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000023.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1WBja87MuYEsYuAh2FtPfQzTL6aDNg2Hg",
      "session": [
        "2"
      ],
      "slack_channel": "p2-07-jiang",
      "title": "Learning Hierarchical Metrical Structure Beyond Measures",
      "video": "https://drive.google.com/uc?export=preview&id=1jWSzDrSgqMBs91Mi0uRIGMR0BU3YuuNw"
    },
    "forum": "147",
    "id": "147",
    "pic_id": "https://drive.google.com/open?id=1bJUUFdMLR9-GDKCyTxtNuIHzsSOZHav7",
    "position": "07",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "2"
  },
  {
    "content": {
      "TLDR": "The extraction of harmonic information from musical audio is fundamental for several music information retrieval tasks. In this paper, we propose novel harmonic audio features based on the perceptually-inspired tonal interval vector space, computed as the Fourier transform of chroma vectors. Our contribution includes mid-level features for musical dissonance, chromaticity, dyadicity, triadicity, diminished quality, diatonicity, and whole-toneness. Moreover, we quantify the perceptual relationship between short- and long-term harmonic structures, tonal dispersion, harmonic changes, and complexity. Beyond the computation on fixed-size windows, we propose a context-sensitive harmonic segmentation approach. We assess the robustness of the new harmonic features in style classification tasks regarding classical music periods and composers. Our results align with, slightly outperforming, existing features and suggest that other musical properties than those in state-of-the-art literature are partially captured. We discuss the features regarding their musical interpretation and compare the different feature groups regarding their effectiveness for discriminating classical music periods and composers.",
      "abstract": "The extraction of harmonic information from musical audio is fundamental for several music information retrieval tasks. In this paper, we propose novel harmonic audio features based on the perceptually-inspired tonal interval vector space, computed as the Fourier transform of chroma vectors. Our contribution includes mid-level features for musical dissonance, chromaticity, dyadicity, triadicity, diminished quality, diatonicity, and whole-toneness. Moreover, we quantify the perceptual relationship between short- and long-term harmonic structures, tonal dispersion, harmonic changes, and complexity. Beyond the computation on fixed-size windows, we propose a context-sensitive harmonic segmentation approach. We assess the robustness of the new harmonic features in style classification tasks regarding classical music periods and composers. Our results align with, slightly outperforming, existing features and suggest that other musical properties than those in state-of-the-art literature are partially captured. We discuss the features regarding their musical interpretation and compare the different feature groups regarding their effectiveness for discriminating classical music periods and composers.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1vc8HTUSlxOdplTBk3cIzHSyfxJa8hNEs)</b>",
      "authors": [
        "Almeida, Francisco C. F.*",
        " Bernardes, Gilberto",
        " Weiss, Christof"
      ],
      "authors_and_affil": [
        "Francisco C. F. Almeida (Univ. Porto, Faculty of Engineering & INESC TEC)*",
        " Gilberto Bernardes (Univ. Porto, Faculty of Engineering & INESC TEC)",
        " Christof Weiss (International Audio Laboratories Erlangen)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CCP3735L",
      "day": "1",
      "keywords": [
        "Musical features and properties",
        "MIR fundamentals and methodology -> music signal processing",
        " Musical features and properties -> musical style and genre",
        " Musical features and properties -> harmony, chords and tonality",
        " MIR tasks -> automatic classification"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Virtually",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000024.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1jm3Xip_D1P9V1Y4XFCuzEvYTL-1i6Qyj",
      "session": [
        "2"
      ],
      "slack_channel": "p2-08-almeida",
      "title": "Mid-level Harmonic Audio Features for Musical Style Classification",
      "video": "https://drive.google.com/uc?export=preview&id=1vc8HTUSlxOdplTBk3cIzHSyfxJa8hNEs"
    },
    "forum": "204",
    "id": "204",
    "pic_id": "https://drive.google.com/open?id=1b2UHoWcv9d8Bp5uC42rYmfW1MM87_6gW",
    "position": "08",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "2"
  },
  {
    "content": {
      "TLDR": "Given the recent advances in music source separation and automatic mixing, removing audio effects in music tracks is a meaningful step toward developing an automated remixing system. This paper focuses on removing distortion audio effects applied to guitar tracks in music production. We explore whether effect removal can be solved by neural networks designed for source separation and audio effect modeling.\nOur approach proves particularly effective for effects that mix the processed and clean signals. The models achieve better quality and significantly faster inference compared to state-of-the-art solutions based on sparse optimization. We demonstrate that the models are suitable not only for declipping but also for other types of distortion effects. By discussing the results, we stress the usefulness of multiple evaluation metrics to assess different aspects of reconstruction in distortion effect removal.",
      "abstract": "Given the recent advances in music source separation and automatic mixing, removing audio effects in music tracks is a meaningful step toward developing an automated remixing system. This paper focuses on removing distortion audio effects applied to guitar tracks in music production. We explore whether effect removal can be solved by neural networks designed for source separation and audio effect modeling.\nOur approach proves particularly effective for effects that mix the processed and clean signals. The models achieve better quality and significantly faster inference compared to state-of-the-art solutions based on sparse optimization. We demonstrate that the models are suitable not only for declipping but also for other types of distortion effects. By discussing the results, we stress the usefulness of multiple evaluation metrics to assess different aspects of reconstruction in distortion effect removal.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=16u1SK0_iqfzqbIvY1TlfQ_yWRG1yYJov)</b>",
      "authors": [
        "Imort, Johannes",
        " Fabbro, Giorgio*",
        " Martinez Ramirez, Marco A",
        " Uhlich, Stefan",
        " Koyama, Yuichiro",
        " Mitsufuji, Yuki"
      ],
      "authors_and_affil": [
        "Johannes Imort (RWTH Aachen University, Germany)",
        " Giorgio Fabbro (Sony Europe B.V., Stuttgart, Germany)*",
        " Marco A Martinez Ramirez (Sony Group Corporation, Tokyo, Japan)",
        " Stefan Uhlich (Sony Europe B.V., Stuttgart, Germany)",
        " Yuichiro Koyama (Sony Group Corporation, Tokyo, Japan)",
        " Yuki Mitsufuji (Sony Group Corporation, Tokyo, Japan)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04C4QPMVK9",
      "day": "1",
      "keywords": [
        "MIR fundamentals and methodology -> music signal processing",
        " MIR tasks -> sound source separation",
        "Applications -> performance, and production",
        " MIR tasks -> music synthesis and transformation",
        " Domain knowledge -> machine learning/artificial intelligence for music"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Bengaluru",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000025.pdf",
      "poster_pdf": "https://drive.google.com/open?id=14-dTdMOLibNXOCFe3UFUjgLlT9tGacJc",
      "session": [
        "2"
      ],
      "slack_channel": "p2-09-imort",
      "title": "Distortion Audio Effects: Learning How to Recover the Clean Signal",
      "video": "https://drive.google.com/uc?export=preview&id=16u1SK0_iqfzqbIvY1TlfQ_yWRG1yYJov"
    },
    "forum": "113",
    "id": "113",
    "pic_id": "https://drive.google.com/open?id=1Kmlvn7_i2OpBDk_4OQV3D_w0gsUpAY5v",
    "position": "09",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "2"
  },
  {
    "content": {
      "TLDR": "Optical Music Recognition (OMR) systems typically consider workflows that include several steps, such as staff detection, symbol recognition, and semantic reconstruction. However, fine-tuning these systems is costly due to the specific data labeling process that has to be performed to train models for each of these steps. In this paper, we present the first segmentation-free full-page OMR system that receives a page image and directly outputs the transcription in a single step. This model requires only the annotations of full score pages, which greatly alleviates the task of manual labeling. The model has been tested with early music written in mensural notation, for which the presented approach is especially beneficial. Results show that this methodology provides a solution with promising results and establishes a new line of research for holistic transcription of music score pages.",
      "abstract": "Optical Music Recognition (OMR) systems typically consider workflows that include several steps, such as staff detection, symbol recognition, and semantic reconstruction. However, fine-tuning these systems is costly due to the specific data labeling process that has to be performed to train models for each of these steps. In this paper, we present the first segmentation-free full-page OMR system that receives a page image and directly outputs the transcription in a single step. This model requires only the annotations of full score pages, which greatly alleviates the task of manual labeling. The model has been tested with early music written in mensural notation, for which the presented approach is especially beneficial. Results show that this methodology provides a solution with promising results and establishes a new line of research for holistic transcription of music score pages.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1ChqPLzsVay8fH5Ve-E4FDbvWTFZnauyD)</b>",
      "authors": [
        "R\u00edos-Vila, Antonio*",
        " Inesta, Jose M.",
        " Calvo-Zaragoza, Jorge"
      ],
      "authors_and_affil": [
        "Antonio R\u00edos-Vila (U.I for Computer Research, University of Alicante, Spain)*",
        " Jos\u00e9 M. I\u00f1esta (U.I for Computer Research, University of Alicante, Spain)",
        " Jorge Calvo-Zaragoza (U.I for Computer Research, University of Alicante, Spain)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CKB7SXH8",
      "day": "1",
      "keywords": [
        " MIR tasks -> music transcription and annotation",
        "Domain knowledge -> machine learning/artificial intelligence for music",
        "MIR tasks -> optical music recognition"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Virtually",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000026.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1bqt_DrqYkabIYJiZ-pOOasCXEN6ZKXGu",
      "session": [
        "2"
      ],
      "slack_channel": "p2-10-r\u00edos-vila",
      "title": "End-to-End Full-Page Optical Music Recognition for Mensural Notation",
      "video": "https://drive.google.com/uc?export=preview&id=1ChqPLzsVay8fH5Ve-E4FDbvWTFZnauyD"
    },
    "forum": "33",
    "id": "33",
    "pic_id": "https://drive.google.com/open?id=1bJg7r0R-M-_JaQby-2Ui4Ii70iBFPKUG",
    "position": "10",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "2"
  },
  {
    "content": {
      "TLDR": "Vocoders are models capable of transforming a low-dimensional spectral representation of an audio signal, typically the mel spectrogram, to a waveform. Modern speech generation pipelines use a vocoder as their final component. Recent vocoder models developed for speech achieve high degree of realism, such that it is natural to wonder how they would perform on music signals. Compared to speech, the heterogeneity and structure of the musical sound texture offers new challenges. In this work we focus on one specific artifact that some vocoder models designed for speech tend to exhibit when applied to music: the perceived instability of pitch when synthesizing sustained notes. We argue that the characteristic sound of this artifact is due to the lack of horizontal phase coherence, which is often the result of using a time-domain target space with a model that is invariant to time-shifts, such as a convolutional neural network. \n\nWe propose a new vocoder model that is specifically designed for music. Key to improving the pitch stability is the choice of a shift-invariant target space that consists of the magnitude spectrum and the phase gradient. We discuss the reasons that inspired us to re-formulate the vocoder task, outline a working example, and evaluate it on musical signals. Our method results in 60% and 10% improved reconstruction of sustained notes and chords with respect to existing models, \nusing a novel harmonic error metric.",
      "abstract": "Vocoders are models capable of transforming a low-dimensional spectral representation of an audio signal, typically the mel spectrogram, to a waveform. Modern speech generation pipelines use a vocoder as their final component. Recent vocoder models developed for speech achieve high degree of realism, such that it is natural to wonder how they would perform on music signals. Compared to speech, the heterogeneity and structure of the musical sound texture offers new challenges. In this work we focus on one specific artifact that some vocoder models designed for speech tend to exhibit when applied to music: the perceived instability of pitch when synthesizing sustained notes. We argue that the characteristic sound of this artifact is due to the lack of horizontal phase coherence, which is often the result of using a time-domain target space with a model that is invariant to time-shifts, such as a convolutional neural network. \n\nWe propose a new vocoder model that is specifically designed for music. Key to improving the pitch stability is the choice of a shift-invariant target space that consists of the magnitude spectrum and the phase gradient. We discuss the reasons that inspired us to re-formulate the vocoder task, outline a working example, and evaluate it on musical signals. Our method results in 60% and 10% improved reconstruction of sustained notes and chords with respect to existing models, \nusing a novel harmonic error metric.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1A8OcItwfN9P0OKLQU122KjwNVDrjx0KK)</b>",
      "authors": [
        "Di Giorgi, Bruno*",
        " Levy, Mark",
        " Sharp, Richard"
      ],
      "authors_and_affil": [
        "Bruno Di Giorgi (Apple)*",
        " Mark Levy (Apple)",
        " Richard Sharp (Apple)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CCP34MK8",
      "day": "1",
      "keywords": [
        "MIR tasks -> music synthesis and transformation",
        "MIR fundamentals and methodology -> music signal processing"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Bengaluru",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000027.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1jpiR9Y-p0POeWMv3pHDo723_kLmK7Wgb",
      "session": [
        "2"
      ],
      "slack_channel": "p2-11-di-giorgi",
      "title": "Mel Spectrogram Inversion with Stable Pitch",
      "video": "https://drive.google.com/uc?export=preview&id=1A8OcItwfN9P0OKLQU122KjwNVDrjx0KK"
    },
    "forum": "188",
    "id": "188",
    "pic_id": "https://drive.google.com/open?id=1TWl73QhOmqsG_OWF3x_3c295dZm0ZfCk",
    "position": "11",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "2"
  },
  {
    "content": {
      "TLDR": "In this paper, we introduce LA-Chorus, a chorus detection model based on latent feature augmentation and ResNet FPN architecture. Our contributions in LA-Chorus are three-fold. Firstly, we propose a method for implicitly augmenting chorus data in the latent space during the train7 ing stage. Compared to augmentations on audio surfaces such as time stretching and pitch shifting, latent augmentations indicate changes at a higher level in original audio, thereby increasing the diversity and sufficiency in training. Second, we apply Feature Pyramid Network (FPN) to generate additional embeddings from low dimension to high dimension, consequently achieving a multi-scale training paradigm. Lastly, we release Di-Chorus, a new open-source dataset of diverse genres and languages for the community of music structure analysis. In conjunction with other public datasets, we conduct comprehensive ex18 periments to evaluate the performance of LA-Chorus compared to other state-of-the-art models, which demonstrate the out-performance of LA-Chorus and the effectiveness of proposed latent feature augmentation.",
      "abstract": "In this paper, we introduce LA-Chorus, a chorus detection model based on latent feature augmentation and ResNet FPN architecture. Our contributions in LA-Chorus are three-fold. Firstly, we propose a method for implicitly augmenting chorus data in the latent space during the train7 ing stage. Compared to augmentations on audio surfaces such as time stretching and pitch shifting, latent augmentations indicate changes at a higher level in original audio, thereby increasing the diversity and sufficiency in training. Second, we apply Feature Pyramid Network (FPN) to generate additional embeddings from low dimension to high dimension, consequently achieving a multi-scale training paradigm. Lastly, we release Di-Chorus, a new open-source dataset of diverse genres and languages for the community of music structure analysis. In conjunction with other public datasets, we conduct comprehensive ex18 periments to evaluate the performance of LA-Chorus compared to other state-of-the-art models, which demonstrate the out-performance of LA-Chorus and the effectiveness of proposed latent feature augmentation.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1N9zeHi5tkvYKhjv0r16xgSBS5GzXW5bH)</b>",
      "authors": [
        "Du, Xingjian*",
        " Liang, Huidong",
        " Wan, Yuan",
        " Lin, Yuheng",
        " Chen, Ke",
        " Zhu, Bilei",
        " Ma, Zejun"
      ],
      "authors_and_affil": [
        "Xingjian Du (ByteDance)*",
        " Huidong Liang (ByteDance)",
        " Yuan Wan (ByteDance AI Lab)",
        " Yuheng Lin (ByteDance AI Lab)",
        " Ke Chen (University of California San Diego)",
        " Bilei Zhu (ByteDance AI Lab)",
        " Zejun Ma (Bytedance)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CKB8Q2GJ",
      "day": "1",
      "keywords": [
        "MIR tasks -> music transcription and annotation",
        ""
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Virtually",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000028.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1F5J93qfLB_enOWdzi3lGneBYrmVdAQCj",
      "session": [
        "2"
      ],
      "slack_channel": "p2-12-du",
      "title": "Latent feature augmentation for chorus detection",
      "video": "https://drive.google.com/uc?export=preview&id=1N9zeHi5tkvYKhjv0r16xgSBS5GzXW5bH"
    },
    "forum": "119",
    "id": "119",
    "pic_id": "https://drive.google.com/open?id=1617ycXzfATdFwdsM-Y9uusY2gYN3H1wT",
    "position": "12",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "2"
  },
  {
    "content": {
      "TLDR": "We propose AccoMontage2, a system capable of doing full-length song harmonization and accompaniment arrangement based on a lead melody. Following AccoMontage, this study focuses on generating piano arrangements for popular/folk songs and it carries on the generalized template-based retrieval method. The novelties of this study are twofold. First, we invent a harmonization module (which AccoMontage does not have). This module generates structured and coherent full-length chord progression by optimizing and balancing three loss terms: a micro-level loss for note-wise dissonance, a meso-level loss for phrase-template matching, and a macro-level loss for full piece coherency. Second, we develop a graphical user interface which allows users to select different styles of chord progression and piano texture. Currently, chord progression styles include Pop, R&B, and Dark, while piano texture styles include several levels of voicing density and rhythmic complexity. Experimental results show that both our harmonization and arrangement results significantly outperform the baselines. Lastly, we release AccoMontage2 as an online application as well as the organized chord progression templates as a public dataset. ",
      "abstract": "We propose AccoMontage2, a system capable of doing full-length song harmonization and accompaniment arrangement based on a lead melody. Following AccoMontage, this study focuses on generating piano arrangements for popular/folk songs and it carries on the generalized template-based retrieval method. The novelties of this study are twofold. First, we invent a harmonization module (which AccoMontage does not have). This module generates structured and coherent full-length chord progression by optimizing and balancing three loss terms: a micro-level loss for note-wise dissonance, a meso-level loss for phrase-template matching, and a macro-level loss for full piece coherency. Second, we develop a graphical user interface which allows users to select different styles of chord progression and piano texture. Currently, chord progression styles include Pop, R&B, and Dark, while piano texture styles include several levels of voicing density and rhythmic complexity. Experimental results show that both our harmonization and arrangement results significantly outperform the baselines. Lastly, we release AccoMontage2 as an online application as well as the organized chord progression templates as a public dataset. <br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1OF59ctqQTEilC-oSKuIscvN7FUVH-ghA)</b>",
      "authors": [
        "Yi, Li*",
        " Hu, Haochen",
        " Zhao, Jingwei",
        " Xia, Gus"
      ],
      "authors_and_affil": [
        "Li Yi (Music X Lab, NYU Shanghai, MBZUAI)*",
        " Haochen Hu (Music X Lab, NYU Shanghai, MBZUAI)",
        " Jingwei Zhao (Institute of Data Science, NUS)",
        " Gus Xia (Music X Lab, NYU Shanghai, MBZUAI)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04D92HF0QY",
      "day": "1",
      "keywords": [
        " Human-centered MIR -> music interfaces and services",
        " Evaluation, datasets, and reproducibility -> MIR tasks",
        "MIR tasks",
        "Evaluation, datasets, and reproducibility -> evaluation methodology",
        " MIR tasks -> music synthesis and transformation",
        " MIR tasks -> music generation"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Virtually",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000029.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1D_f5QafclHSYkfYDYibajD2JTiLF9Xly",
      "session": [
        "2"
      ],
      "slack_channel": "p2-13-yi",
      "title": "AccoMontage2: A Complete Harmonization and Accompaniment Arrangement System",
      "video": "https://drive.google.com/uc?export=preview&id=1OF59ctqQTEilC-oSKuIscvN7FUVH-ghA"
    },
    "forum": "218",
    "id": "218",
    "pic_id": "https://drive.google.com/open?id=1QAHUKa6RmcjyHSpHhgnf9MOoGm43K3hx",
    "position": "13",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "2"
  },
  {
    "content": {
      "TLDR": "In this work, we provide a broad comparative analysis of strategies for pre-training audio understanding models for several tasks in the music domain, including labelling of genre, era, origin, mood, instrumentation, key, pitch, vocal characteristics, tempo and sonority. Specifically, we explore how the domain of pre-training datasets (music or generic audio) and the pre-training methodology (supervised or unsupervised) affects the adequacy of the resulting audio embeddings for downstream tasks.\n\nWe show that models trained via supervised learning on large-scale expert-annotated music datasets achieve state-of-the-art performance in a wide range of music labelling tasks, each with novel content and vocabularies. This can be done in an efficient manner with models containing less than 100 million parameters that require no fine-tuning or reparameterization for downstream tasks, making this approach practical for industry-scale audio catalogs.\n\nWithin the class of unsupervised learning strategies, we show that the domain of the training dataset can significantly impact the performance of representations learned by the model. We find that restricting the domain of the pre-training dataset to music allows for training with smaller batch sizes while achieving state-of-the-art in unsupervised learning---and in some cases, supervised learning---for music understanding.\n\nWe also corroborate that, while achieving state-of-the-art performance on many tasks, supervised learning can cause models to specialize to the supervised information provided, somewhat compromising a model's generality.\n",
      "abstract": "In this work, we provide a broad comparative analysis of strategies for pre-training audio understanding models for several tasks in the music domain, including labelling of genre, era, origin, mood, instrumentation, key, pitch, vocal characteristics, tempo and sonority. Specifically, we explore how the domain of pre-training datasets (music or generic audio) and the pre-training methodology (supervised or unsupervised) affects the adequacy of the resulting audio embeddings for downstream tasks.\n\nWe show that models trained via supervised learning on large-scale expert-annotated music datasets achieve state-of-the-art performance in a wide range of music labelling tasks, each with novel content and vocabularies. This can be done in an efficient manner with models containing less than 100 million parameters that require no fine-tuning or reparameterization for downstream tasks, making this approach practical for industry-scale audio catalogs.\n\nWithin the class of unsupervised learning strategies, we show that the domain of the training dataset can significantly impact the performance of representations learned by the model. We find that restricting the domain of the pre-training dataset to music allows for training with smaller batch sizes while achieving state-of-the-art in unsupervised learning---and in some cases, supervised learning---for music understanding.\n\nWe also corroborate that, while achieving state-of-the-art performance on many tasks, supervised learning can cause models to specialize to the supervised information provided, somewhat compromising a model's generality.\n<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1z-1-ViOy301bJ-nrvqtdPe8LgdLIS-5A)</b>",
      "authors": [
        "McCallum, Matthew C*",
        " Korzeniowski, Filip",
        " Oramas, Sergio",
        " Gouyon, Fabien",
        " Ehmann, Andreas"
      ],
      "authors_and_affil": [
        "Matthew C McCallum (SiriusXM, USA)*",
        " Filip Korzeniowski (SiriusXM, USA)",
        " Sergio Oramas (SiriusXM, USA)",
        " Fabien Gouyon (SiriusXM, USA)",
        " Andreas Ehmann (SiriusXM, USA)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04D92J316U",
      "day": "1",
      "keywords": [
        "Applications -> digital libraries and archives",
        " Domain knowledge -> representations of music",
        "Musical features and properties -> representations of music",
        " Applications -> music retrieval systems",
        " Domain knowledge -> machine learning/artificial intelligence for music",
        " MIR tasks -> automatic classification"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Bengaluru",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000030.pdf",
      "poster_pdf": "https://drive.google.com/open?id=16LQ3MZJb4HZrbe6fPohKNkCBtar9ZXp4",
      "session": [
        "2"
      ],
      "slack_channel": "p2-14-mccallum",
      "title": "Supervised and Unsupervised Learning of Audio Representations for Music Understanding",
      "video": "https://drive.google.com/uc?export=preview&id=1z-1-ViOy301bJ-nrvqtdPe8LgdLIS-5A"
    },
    "forum": "285",
    "id": "285",
    "pic_id": "https://drive.google.com/open?id=1ihOKFFuBiC0wc8ANilHWHhwfcly5c7fe",
    "position": "14",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "2"
  },
  {
    "content": {
      "TLDR": "Creating a complex work of art like music necessitates profound creativity. With recent advancements in Deep Learning and powerful models such as Transformers, there has been huge progress in automatic music generation. In an accompaniment generation context, creating a coherent drum pattern with apposite fills and improvisations at proper locations in a song is a challenging task even for an experienced drummer. Drum beats tend to follow a repetitive pattern through stanzas with fills/improvisation at section boundaries. In this work, we tackle the task of drum pattern generation conditioned on the accompanying music played by four melodic instruments \u2013 Piano, Guitar, Bass and Strings. We use the transformer sequence to sequence model to generate a basic drum pattern conditioned on the melodic accompaniment to find that improvisation is largely absent, attributed possibly to its relatively low representation in the training data. We propose a novelty function that represents the extent of improvisation in a specific bar relative to its neighbors. We train a model to detect improvisation positions from the melodic accompaniment tracks. Finally, we use a novel BERT inspired in-filling architecture, to learn the structure of both the drums and melody to in-fill elements of improvised music.",
      "abstract": "Creating a complex work of art like music necessitates profound creativity. With recent advancements in Deep Learning and powerful models such as Transformers, there has been huge progress in automatic music generation. In an accompaniment generation context, creating a coherent drum pattern with apposite fills and improvisations at proper locations in a song is a challenging task even for an experienced drummer. Drum beats tend to follow a repetitive pattern through stanzas with fills/improvisation at section boundaries. In this work, we tackle the task of drum pattern generation conditioned on the accompanying music played by four melodic instruments \u2013 Piano, Guitar, Bass and Strings. We use the transformer sequence to sequence model to generate a basic drum pattern conditioned on the melodic accompaniment to find that improvisation is largely absent, attributed possibly to its relatively low representation in the training data. We propose a novelty function that represents the extent of improvisation in a specific bar relative to its neighbors. We train a model to detect improvisation positions from the melodic accompaniment tracks. Finally, we use a novel BERT inspired in-filling architecture, to learn the structure of both the drums and melody to in-fill elements of improvised music.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1aINKqGpGW86_tANRiw6zTH5iz9GbqTB6)</b>",
      "authors": [
        "Dahale, Rishabh A*",
        " Talwadker, Vaibhav Vinayak",
        " Rao, Preeti",
        " Verma, Prateek"
      ],
      "authors_and_affil": [
        "Rishabh A Dahale (Department of Electrical Engineering, Indian Institute of Technology Bombay, India)*",
        " Vaibhav Vinayak Talwadker (Department of Electrical Engineering, Indian Institute of Technology Bombay, India)",
        " Preeti Rao (Department of Electrical Engineering, Indian Institute of Technology Bombay, India)",
        " Prateek Verma (Stanford University)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CGCW7N4V",
      "day": "1",
      "keywords": [
        "Musical features and properties -> structure, segmentation, and form",
        "MIR tasks -> music generation"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Bengaluru",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000031.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1L1Jz73TIk7MMWrwaOETWEjfJaOktROEY",
      "session": [
        "2"
      ],
      "slack_channel": "p2-15-dahale",
      "title": "Generating Coherent Drum Accompaniment with Fills and Improvisations",
      "video": "https://drive.google.com/uc?export=preview&id=1aINKqGpGW86_tANRiw6zTH5iz9GbqTB6"
    },
    "forum": "320",
    "id": "320",
    "pic_id": "https://drive.google.com/open?id=14M6XklTLCbkVR9I2hcstszVGEoaOAFty",
    "position": "15",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "2"
  },
  {
    "content": {
      "TLDR": "Although audio to score alignment is a classic Music Information Retrieval problem, it has not been defined uniquely with the scope of musical scenarios representing its core. The absence of a unified vision makes it difficult to pinpoint its state-of-the-art and determine directions for improvement. To get past this bottleneck, it is necessary to consolidate datasets and evaluation methodologies to allow comprehensive benchmarking. In our review of prior work, we demonstrate the extent of variation in problem scope, datasets, and evaluation practices across audio to score alignment research. To circumvent the high cost of creating large-scale datasets with various instruments, styles, performance conditions, and musician proficiency from scratch, the research community could generate ground truth approximations from non-audio to score alignment datasets which include a temporal mapping between a music score and its corresponding audio. We show a methodology for adapting the Aligned Scores and Performances dataset, created originally for beat tracking and music transcription. We filter the dataset semi- automatically by applying a set of Dynamic Time Warping based Audio to Score Alignment methods using out-of-the-box Chroma and Constant-Q Transform extraction algorithms, suitable for the characteristics of the piano performances of the dataset. We use the results to discuss the limitations of the generated ground truths and data adaptation method. While the adapted dataset does not provide the necessary diversity for solving the initial problem, we conclude with ideas for expansion, and identify future directions for curating more comprehensive datasets through data adaptation, or synthesis.",
      "abstract": "Although audio to score alignment is a classic Music Information Retrieval problem, it has not been defined uniquely with the scope of musical scenarios representing its core. The absence of a unified vision makes it difficult to pinpoint its state-of-the-art and determine directions for improvement. To get past this bottleneck, it is necessary to consolidate datasets and evaluation methodologies to allow comprehensive benchmarking. In our review of prior work, we demonstrate the extent of variation in problem scope, datasets, and evaluation practices across audio to score alignment research. To circumvent the high cost of creating large-scale datasets with various instruments, styles, performance conditions, and musician proficiency from scratch, the research community could generate ground truth approximations from non-audio to score alignment datasets which include a temporal mapping between a music score and its corresponding audio. We show a methodology for adapting the Aligned Scores and Performances dataset, created originally for beat tracking and music transcription. We filter the dataset semi- automatically by applying a set of Dynamic Time Warping based Audio to Score Alignment methods using out-of-the-box Chroma and Constant-Q Transform extraction algorithms, suitable for the characteristics of the piano performances of the dataset. We use the results to discuss the limitations of the generated ground truths and data adaptation method. While the adapted dataset does not provide the necessary diversity for solving the initial problem, we conclude with ideas for expansion, and identify future directions for curating more comprehensive datasets through data adaptation, or synthesis.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1WyBwLn2_jKPDXXk8papoV2DcbBZV5O0u)</b>",
      "authors": [
        "Morsi, Alia*",
        " Serra, Xavier"
      ],
      "authors_and_affil": [
        "Alia Morsi (Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain)*",
        " Xavier Serra (Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CMQX0LG4",
      "day": "1",
      "keywords": [
        "MIR fundamentals and methodology -> music signal processing",
        " Evaluation, datasets, and reproducibility -> novel datasets and use cases",
        "Domain knowledge -> representations of music",
        " MIR tasks -> indexing and querying"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Bengaluru",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000032.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1gBfXdMY_HhoxT2BZ_IRikmQvTsbFkqc1",
      "session": [
        "2"
      ],
      "slack_channel": "p2-16-morsi",
      "title": "Bottlenecks and solutions for audio to score alignment research",
      "video": "https://drive.google.com/uc?export=preview&id=1WyBwLn2_jKPDXXk8papoV2DcbBZV5O0u"
    },
    "forum": "231",
    "id": "231",
    "pic_id": "https://drive.google.com/open?id=1vvQDqpLXH_87yqXRYBSwQMwdnDWPYzOK",
    "position": "16",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "2"
  },
  {
    "content": {
      "TLDR": "Work on musical gesture and embodied cognition suggests a rich complementarity between audio and movement information in musical performance. Pose estimation algorithms now make it possible (in contrast to Motion Capture)  to collect rich movement information from unconstrained performances of indefinite length. Vocal performances of Indian art music  offer the opportunity to carry out multimodal analysis using this information, combing musician\u2019s body movements (i.e. pose and gesture data) with audio features. In this work we investigate raga identification from 12 s excerpts from a dataset of 3 singers and 9 ragas using the combination of audio and visual representations that are each semantically salient on their own.  While gesture based classification is relatively weak by itself, we show that combining latent representations from the pre-trained unimodal networks can surpass the already high performance obtained by audio features. ",
      "abstract": "Work on musical gesture and embodied cognition suggests a rich complementarity between audio and movement information in musical performance. Pose estimation algorithms now make it possible (in contrast to Motion Capture)  to collect rich movement information from unconstrained performances of indefinite length. Vocal performances of Indian art music  offer the opportunity to carry out multimodal analysis using this information, combing musician\u2019s body movements (i.e. pose and gesture data) with audio features. In this work we investigate raga identification from 12 s excerpts from a dataset of 3 singers and 9 ragas using the combination of audio and visual representations that are each semantically salient on their own.  While gesture based classification is relatively weak by itself, we show that combining latent representations from the pre-trained unimodal networks can surpass the already high performance obtained by audio features. <br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=11wQvhr8uswrb9_o_6im3M_ndtHk6p_Xl)</b>",
      "authors": [
        "Clayton, Martin",
        " Rao, Preeti*",
        " Shikarpur, Nithya Nadig",
        " Roychowdhury, Sujoy",
        " Li, Jin"
      ],
      "authors_and_affil": [
        "Martin Clayton (Department of Music, Durham University, United Kingdom)",
        " Preeti Rao (Department of Electrical Engineering, Indian Institute of Technology Bombay, India)*",
        " Nithya Shikarpur (Department of Electrical Engineering, Indian Institute of Technology Bombay, India)",
        " Sujoy Roychowdhury (Department of Electrical Engineering, Indian Institute of Technology Bombay, India)",
        " Jin Li (Department of Music, Durham University, United Kingdom)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CY0M7ERF",
      "day": "2",
      "keywords": [
        "Evaluation, datasets, and reproducibility -> novel datasets and use cases",
        " MIR tasks -> automatic classification",
        "Domain knowledge -> computational ethnomusicology"
      ],
      "long_presentation": "TRUE",
      "paper_presentation": "In-person, in Bengaluru",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000033.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1g9Vo9_o7IBXIl1z_Jm5mGZ7HQG6csZBH",
      "session": [
        "3"
      ],
      "slack_channel": "p3-01-rao",
      "title": "Raga Classification From Vocal Performances Using Multimodal Analysis  ",
      "video": "https://drive.google.com/uc?export=preview&id=11wQvhr8uswrb9_o_6im3M_ndtHk6p_Xl"
    },
    "forum": "83",
    "id": "83",
    "pic_id": "https://drive.google.com/open?id=1uEqTDoKqUYOrmWWqCZJte_fufywP83Ab",
    "position": "01",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "3"
  },
  {
    "content": {
      "TLDR": "Music streaming platforms allow users to enjoy music from all over the globe.\nSuch opportunity speeds up cultural exchange between different countries, a process often associated with globalization. While such an exchange could lead to more diverse music consumption, empirical evidence on its influence on online music consumption is limited. Besides, the extent to which music recommender systems foster exchange or amplify globalization in music remains an understudied problem.\n\nIn this paper, we present findings from an empirical study to detect traces of globalization in domestic vs. foreign online music consumption. Besides, we investigate if popular recommendation algorithms, specifically ItemKNN and \nNeuMF, are prone to amplifying globalization processes. Our experiments on Last.fm listening data show nuanced patterns of globalization in music consumption. We observe a strong position of US music in all considered countries. In countries such as Sweden, Great Britain, or Brazil, US music shows various levels of coexistence with domestic music. We find that Finland is least influenced by US music, while greatly consuming and 'exporting' domestic music.  With respect to recommendation algorithms, ItemKNN tends to recommend domestic music to users of many countries, while NeuMF contributes to accelerating globalization and shifting balance towards dominance of US music on the market.",
      "abstract": "Music streaming platforms allow users to enjoy music from all over the globe.\nSuch opportunity speeds up cultural exchange between different countries, a process often associated with globalization. While such an exchange could lead to more diverse music consumption, empirical evidence on its influence on online music consumption is limited. Besides, the extent to which music recommender systems foster exchange or amplify globalization in music remains an understudied problem.\n\nIn this paper, we present findings from an empirical study to detect traces of globalization in domestic vs. foreign online music consumption. Besides, we investigate if popular recommendation algorithms, specifically ItemKNN and \nNeuMF, are prone to amplifying globalization processes. Our experiments on Last.fm listening data show nuanced patterns of globalization in music consumption. We observe a strong position of US music in all considered countries. In countries such as Sweden, Great Britain, or Brazil, US music shows various levels of coexistence with domestic music. We find that Finland is least influenced by US music, while greatly consuming and 'exporting' domestic music.  With respect to recommendation algorithms, ItemKNN tends to recommend domestic music to users of many countries, while NeuMF contributes to accelerating globalization and shifting balance towards dominance of US music on the market.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1LXaANmzwMqXYUMtuA1hfHrdT-lBs056n)</b>",
      "authors": [
        "Lesota, Oleg*",
        " Parada-Cabaleiro, Emilia",
        " Lex, Elisabeth",
        " Rekabsaz, Navid",
        " Brandl, Stefan",
        " Schedl, Markus"
      ],
      "authors_and_affil": [
        "Oleg Lesota (Johannes Kepler University)*",
        " Emilia Parada-Cabaleiro (Johannes Kepler University Linz)",
        " Stefan Brandl (Johannes Kepler University Linz)",
        " Elisabeth Lex (TU Graz)",
        " Navid Rekabsaz (JKU)",
        " Markus Schedl (Johannes Kepler University Linz)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04D92H4F96",
      "day": "2",
      "keywords": [
        "Human-centered MIR -> user behavior analysis and mining",
        "Applications -> music recommendation and playlist generation",
        " Philosophical and ethical discussions -> legal and societal aspects of MIR"
      ],
      "long_presentation": "TRUE",
      "paper_presentation": "In-person, in Bengaluru",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000034.pdf",
      "poster_pdf": "https://drive.google.com/open?id=11NiZccrqg7WmNy7IQcJiuAzGoq-WpZYp",
      "session": [
        "3"
      ],
      "slack_channel": "p3-02-lesota",
      "title": "Traces of Globalization in Online Music Consumption Patterns and Results of Recommendation Algorithms",
      "video": "https://drive.google.com/uc?export=preview&id=1LXaANmzwMqXYUMtuA1hfHrdT-lBs056n"
    },
    "forum": "165",
    "id": "165",
    "pic_id": "https://drive.google.com/open?id=1-8zl2BbnZs3HrkLEymXn3loWxzdLGMHX",
    "position": "02",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "3"
  },
  {
    "content": {
      "TLDR": "Anglo-American popular culture has been said to be intricately connected to global popular culture, both shaping and being shaped by popular trends worldwide, yet few research has examined this issue empirically. Our research quantitatively maps the extent of these cultural influences in popular music consumption, by using network analyses to explore cross-cultural popularity in music from 30 countries corresponding to 6 cultural regions (N = 4863 unique songs over six timepoints from 2019-2021). Using Top100 charts from these countries, we constructed a network based on the co-occurrence of songs in charts, and used eigencentrality as an indicator of cross-cultural song popularity. We then compared the country-of-origin of the artists, arousal music features, and socioeconomic indicators. Songs from artists with Anglo-American backgrounds tended to have higher eigencentrality overall, and mixed effects regressions showed that eigencentrality was negatively associated with danceability, and positively associated with spectral energy, and the migrant population of the country (of the charts). Next, using community detection, we observed 11 separate 'communities' in the network. Most communities appeared to be limited by region/culture, but Anglo-American music seemed disproportionally able to transcend cultural boundaries far beyond their geographical borders. We also discuss implications pertaining to cultural hegemony, and the effectiveness of our method in estimating cross-cultural popularity.",
      "abstract": "Anglo-American popular culture has been said to be intricately connected to global popular culture, both shaping and being shaped by popular trends worldwide, yet few research has examined this issue empirically. Our research quantitatively maps the extent of these cultural influences in popular music consumption, by using network analyses to explore cross-cultural popularity in music from 30 countries corresponding to 6 cultural regions (N = 4863 unique songs over six timepoints from 2019-2021). Using Top100 charts from these countries, we constructed a network based on the co-occurrence of songs in charts, and used eigencentrality as an indicator of cross-cultural song popularity. We then compared the country-of-origin of the artists, arousal music features, and socioeconomic indicators. Songs from artists with Anglo-American backgrounds tended to have higher eigencentrality overall, and mixed effects regressions showed that eigencentrality was negatively associated with danceability, and positively associated with spectral energy, and the migrant population of the country (of the charts). Next, using community detection, we observed 11 separate 'communities' in the network. Most communities appeared to be limited by region/culture, but Anglo-American music seemed disproportionally able to transcend cultural boundaries far beyond their geographical borders. We also discuss implications pertaining to cultural hegemony, and the effectiveness of our method in estimating cross-cultural popularity.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1lvSF-lwH7TjyzsdlMVvqzHAIVOKaNFXQ)</b>",
      "authors": [
        "Liew, Kongmeng*",
        " Mishra, Vipul",
        " Zhou, Yangyang",
        " Epure, Elena V.",
        " Hennequin, Romain",
        " Wakamiya, Shoko",
        " Aramaki, Eiji"
      ],
      "authors_and_affil": [
        "Kongmeng Liew (Nara Institute of Science and Technology)*",
        " Vipul Mishra (Nara Institute of Science and Technology)",
        " Yangyang Zhou (Nara Institute of Science and Technology)",
        " Elena V. Epure (Deezer Research)",
        " Romain Hennequin (Deezer Research)",
        " Shoko Wakamiya (Nara Institute of Science and Technology)",
        " Eiji Aramaki (Nara Institute of Science and Technology)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CCP41U0N",
      "day": "2",
      "keywords": [
        "Applications -> digital libraries and archives",
        "Domain knowledge -> computational music theory and musicology",
        " Domain knowledge -> computational ethnomusicology",
        " Philosophical and ethical discussions -> legal and societal aspects of MIR"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Virtually",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000035.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1tY74PkqPSXwcUAlSyhhEZgzsTWshZqqi",
      "session": [
        "3"
      ],
      "slack_channel": "p3-03-liew",
      "title": "Network Analyses for Cross-Cultural Music Popularity",
      "video": "https://drive.google.com/uc?export=preview&id=1lvSF-lwH7TjyzsdlMVvqzHAIVOKaNFXQ"
    },
    "forum": "303",
    "id": "303",
    "pic_id": "https://drive.google.com/open?id=1JcFbhDCb5H7fqktu7HJngVEiausZ7f_i",
    "position": "03",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "3"
  },
  {
    "content": {
      "TLDR": "The Middle Byzantine notation (MBn) is used to capture the plainchant melodies of eastern Orthodox Christian music from the middle of the 12th century until 1814. In the context of this research, we study the evolution of a subgenre of Byzantine music known as Heirmologic. We present three Heirmologic corpora spanning the periods before, during and after the 16th century. We discuss the challenges we faced during the digitisation process, and the steps we took to overcome them. For the analysis of the three corpora, we apply the three methods, namely notational texture, melodic arch similarity, and Jensen-Shannon distances of Markovian models, the second of which is novel and inspired by the idea of melodic arches. Through these methods, we aim at highlighting the differences of the corpora in order to obtain an outline of the evolution of the subgenre. We observe that the post 16th century Heirmologic pieces are more similar to the 16th century ones, while there is a greater difference with the pre 16th century pieces. This indicates that the 16th century constitutes a turning point in the melodic features of the Heirmologic subgenre.",
      "abstract": "The Middle Byzantine notation (MBn) is used to capture the plainchant melodies of eastern Orthodox Christian music from the middle of the 12th century until 1814. In the context of this research, we study the evolution of a subgenre of Byzantine music known as Heirmologic. We present three Heirmologic corpora spanning the periods before, during and after the 16th century. We discuss the challenges we faced during the digitisation process, and the steps we took to overcome them. For the analysis of the three corpora, we apply the three methods, namely notational texture, melodic arch similarity, and Jensen-Shannon distances of Markovian models, the second of which is novel and inspired by the idea of melodic arches. Through these methods, we aim at highlighting the differences of the corpora in order to obtain an outline of the evolution of the subgenre. We observe that the post 16th century Heirmologic pieces are more similar to the 16th century ones, while there is a greater difference with the pre 16th century pieces. This indicates that the 16th century constitutes a turning point in the melodic features of the Heirmologic subgenre.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=11_bDg_L2_UVMJ8-B9gNf_bKwoapY_s2W)</b>",
      "authors": [
        "Polykarpidis, Polykarpos*",
        " KALOFONOS, DIONYSIOS",
        " Balageorgos, Dimitrios",
        " Anagnostopoulou, Christina"
      ],
      "authors_and_affil": [
        "Polykarpos Polykarpidis (National and Kapodistrian University of Athens)*",
        " DIONYSIOS KALOFONOS (Independent researcher)",
        " Dimitrios Balageorgos (National and Kapodistrian University of Athens)",
        " Christina Anagnostopoulou (National and Kapodistrian University of Athens)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CY0LRXKK",
      "day": "2",
      "keywords": [
        " MIR fundamentals and methodology -> symbolic music processing",
        "Domain knowledge -> computational ethnomusicology",
        " Domain knowledge -> computational music theory and musicology",
        "Evaluation, datasets, and reproducibility -> novel datasets and use cases",
        " Musical features and properties -> representations of music",
        " MIR tasks -> similarity metrics"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Virtually",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000036.pdf",
      "poster_pdf": "https://drive.google.com/open?id=12jRG0khHbiWj4JmVlUefVw_4r3vvzfCu",
      "session": [
        "3"
      ],
      "slack_channel": "p3-04-polykarpidis",
      "title": "Three related corpora in Middle Byzantine music notation and a preliminary comparative analysis",
      "video": "https://drive.google.com/uc?export=preview&id=11_bDg_L2_UVMJ8-B9gNf_bKwoapY_s2W"
    },
    "forum": "51",
    "id": "51",
    "pic_id": "https://drive.google.com/open?id=1_0B0WZi06iQa7m_LULRcvn0F9rEFY7b4",
    "position": "04",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "3"
  },
  {
    "content": {
      "TLDR": "The Guzheng is a kind of traditional Chinese instruments with diverse playing techniques. Instrument playing techniques (IPT) play an important role in musical performance. However, most of the existing works for IPT detection show low efficiency for variable-length audio and provide no assurance in the generalization as they rely on a single sound bank for training and testing. In this study, we propose an end-to-end Guzheng playing technique detection system using Fully Convolutional Networks that can be applied to variable-length audio. Because each Guzheng playing technique is applied to a note, a dedicated onset detector is trained to divide an audio into several notes and its predictions are fused with frame-wise IPT predictions. During fusion, we add the IPT predictions frame by frame inside each note and get the IPT with the highest probability within each note as the final output of that note. We create a new dataset named GZ_IsoTech from multiple sound banks and real-world recordings for Guzheng performance analysis. Our approach achieves 87.97% in frame-level accuracy and 80.76% in note-level F1-score, outperforming existing works by a large margin, which indicates the effectiveness of our proposed method in IPT detection.",
      "abstract": "The Guzheng is a kind of traditional Chinese instruments with diverse playing techniques. Instrument playing techniques (IPT) play an important role in musical performance. However, most of the existing works for IPT detection show low efficiency for variable-length audio and provide no assurance in the generalization as they rely on a single sound bank for training and testing. In this study, we propose an end-to-end Guzheng playing technique detection system using Fully Convolutional Networks that can be applied to variable-length audio. Because each Guzheng playing technique is applied to a note, a dedicated onset detector is trained to divide an audio into several notes and its predictions are fused with frame-wise IPT predictions. During fusion, we add the IPT predictions frame by frame inside each note and get the IPT with the highest probability within each note as the final output of that note. We create a new dataset named GZ_IsoTech from multiple sound banks and real-world recordings for Guzheng performance analysis. Our approach achieves 87.97% in frame-level accuracy and 80.76% in note-level F1-score, outperforming existing works by a large margin, which indicates the effectiveness of our proposed method in IPT detection.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1Gp00ewNuGQ3FVY27TT-rv4RHrnL2q2Mx)</b>",
      "authors": [
        "Li, Dichucheng*",
        " Wu, Yulun",
        " Li, Qinyu",
        " Zhao, Jiahao",
        " Yu, Yi",
        " Xia, Fan",
        " Li, Wei"
      ],
      "authors_and_affil": [
        "Dichucheng Li (Fudan University)*",
        " Yulun Wu (Fudan University)",
        " Qinyu Li (Sichuan Conservatory Of Music)",
        " Jiahao Zhao (Fudan University)",
        " Yi Yu (NII)",
        " Fan Xia (Sichuan Conservatory of Music )",
        " Wei Li (Fudan University)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04D92FURNU",
      "day": "2",
      "keywords": [
        "MIR tasks -> music transcription and annotation",
        "Evaluation, datasets, and reproducibility -> MIR tasks",
        " Musical features and properties -> expression and performative aspects of music",
        " Evaluation, datasets, and reproducibility -> novel datasets and use cases",
        " Musical features and properties -> representations of music",
        " MIR tasks"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Virtually",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000037.pdf",
      "poster_pdf": "https://drive.google.com/open?id=14M-JDQqI1LVEW7Uwe_yVhb1P3P1hMOEt",
      "session": [
        "3"
      ],
      "slack_channel": "p3-05-li",
      "title": "Playing Technique Detection by Fusing Note Onset Information in Guzheng Performance",
      "video": "https://drive.google.com/uc?export=preview&id=1Gp00ewNuGQ3FVY27TT-rv4RHrnL2q2Mx"
    },
    "forum": "48",
    "id": "48",
    "pic_id": "https://drive.google.com/open?id=1YT7fAyYnHpnOLh-igf9cJ0ksqMkwhBNA",
    "position": "05",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "3"
  },
  {
    "content": {
      "TLDR": "Iranian dastg\u0101hi music is considered as the classical repertory of contemporary Iran. In the 19th century, the melodic modes that developed during its long history were grouped in categories, each of them known as dastg\u0101h. The dastg\u0101hi system presents unique features, that have been object of musicological study since its inception. However, computational methods for its research are still scarce, due in good part to the lack of open, well curated corpora. The aim of the KUG Dastg\u0101hi Corpus (KDC) is to contribute to the development of computational corpus driven research for this tradition. KDC is created following the FAIR principles, and in close collaboration with performers and scholars, who contribute to it with annotations and qualitative evaluations. Besides presenting the first version of KDC, in this paper we explore the possibilities that Iranian dastg\u0101hi music offers to computational research. In order to test the performance of state-of-the-art technologies applied to this music tradition, we present preliminary results for several analytical tasks, and discuss thei opportunities and limitations learnt in the process.",
      "abstract": "Iranian dastg\u0101hi music is considered as the classical repertory of contemporary Iran. In the 19th century, the melodic modes that developed during its long history were grouped in categories, each of them known as dastg\u0101h. The dastg\u0101hi system presents unique features, that have been object of musicological study since its inception. However, computational methods for its research are still scarce, due in good part to the lack of open, well curated corpora. The aim of the KUG Dastg\u0101hi Corpus (KDC) is to contribute to the development of computational corpus driven research for this tradition. KDC is created following the FAIR principles, and in close collaboration with performers and scholars, who contribute to it with annotations and qualitative evaluations. Besides presenting the first version of KDC, in this paper we explore the possibilities that Iranian dastg\u0101hi music offers to computational research. In order to test the performance of state-of-the-art technologies applied to this music tradition, we present preliminary results for several analytical tasks, and discuss thei opportunities and limitations learnt in the process.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=12CPNBBB11-Yg32qGYCn-BwL6pPHuoxSI)</b>",
      "authors": [
        "Nikzat, Babak*",
        " Caro Repetto, Rafael"
      ],
      "authors_and_affil": [
        "Babak Nikzat (University of Music and Performing Arts Graz)*",
        " Rafael Caro Repetto (Kunstuniversit\u00e4t Graz)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CY0MEP41",
      "day": "2",
      "keywords": [
        "Domain knowledge -> computational music theory and musicology",
        " Evaluation, datasets, and reproducibility -> novel datasets and use cases",
        " Musical features and properties -> melody and motives",
        "Domain knowledge -> computational ethnomusicology"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Bengaluru",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000038.pdf",
      "poster_pdf": "https://drive.google.com/open?id=12M_y7UPwqF6bdOB35RbwPs9owXe6oB1B",
      "session": [
        "3"
      ],
      "slack_channel": "p3-06-nikzat",
      "title": "KDC: an open corpus for computational research of dastg\u0101hi music",
      "video": "https://drive.google.com/uc?export=preview&id=12CPNBBB11-Yg32qGYCn-BwL6pPHuoxSI"
    },
    "forum": "111",
    "id": "111",
    "pic_id": "https://drive.google.com/open?id=1jf16ypILamWbpUvcxInEZbMbFCoKAo3K",
    "position": "06",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "3"
  },
  {
    "content": {
      "TLDR": "The existing MIR research on genre classification primarily focuses on how to classify a song into the \u201ccorrect\u201d genre while downplaying the fact that genres mutate over time and in response to social change in terms of their musical properties. Songs claiming the same genre can sound very different if they are released years apart, and genres may revive musical traditions from the past. In this paper, I show that the performance of genre classifiers fluctuates as genres evolve. Unsatisfactory performance of the classifiers may not indicate algorithmic flaws but rather the change of genre characteristics. I demonstrate this by studying the case of Chinese Hip-Hop music. Specifically, I collected and analyzed 69,427 songs from four genres (Hip-Hop, Pop, Rock, and Folk) released on a Chinese music platform between 2009 and 2019. Using classifiers trained from the songs in different year cohorts to predict the genre of all the songs, I show how genre classifiers can be used to detect the stylistic shift in Hip-Hop that happened during this period. The paper thus offers a novel, sociological perspective on contending with the much-challenged idea of improving genre classification accuracy for its own sake. However, instead of questioning the effort, I argue that MIR research on genre classification can be helpful for studying genre as a social construct and cultural phenomenon if the pursuit of prediction performance and the cultural meaning of inaccurate prediction are carefully balanced.",
      "abstract": "The existing MIR research on genre classification primarily focuses on how to classify a song into the \u201ccorrect\u201d genre while downplaying the fact that genres mutate over time and in response to social change in terms of their musical properties. Songs claiming the same genre can sound very different if they are released years apart, and genres may revive musical traditions from the past. In this paper, I show that the performance of genre classifiers fluctuates as genres evolve. Unsatisfactory performance of the classifiers may not indicate algorithmic flaws but rather the change of genre characteristics. I demonstrate this by studying the case of Chinese Hip-Hop music. Specifically, I collected and analyzed 69,427 songs from four genres (Hip-Hop, Pop, Rock, and Folk) released on a Chinese music platform between 2009 and 2019. Using classifiers trained from the songs in different year cohorts to predict the genre of all the songs, I show how genre classifiers can be used to detect the stylistic shift in Hip-Hop that happened during this period. The paper thus offers a novel, sociological perspective on contending with the much-challenged idea of improving genre classification accuracy for its own sake. However, instead of questioning the effort, I argue that MIR research on genre classification can be helpful for studying genre as a social construct and cultural phenomenon if the pursuit of prediction performance and the cultural meaning of inaccurate prediction are carefully balanced.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1N_CcdvwxUOHU_fGO6b0TRhaPS4JZPZ3n)</b>",
      "authors": [
        "Nie, Ke*"
      ],
      "authors_and_affil": [
        "Ke Nie (University of California, San Diego)*"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04C4QQ0WCX",
      "day": "2",
      "keywords": [
        "Domain knowledge -> machine learning/artificial intelligence for music",
        "Philosophical and ethical discussions -> legal and societal aspects of MIR",
        " Evaluation, datasets, and reproducibility -> evaluation methodology",
        " Musical features and properties -> musical style and genre",
        " Evaluation, datasets, and reproducibility -> evaluation metrics",
        " MIR tasks -> automatic classification"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Virtually",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000039.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1XTV9bCJ5QZe2-FOgDldCVUKB7soWMU_j",
      "session": [
        "3"
      ],
      "slack_channel": "p3-07-nie",
      "title": "Inaccurate Prediction or Genre Evolution? Rethinking Genre Classification",
      "video": "https://drive.google.com/uc?export=preview&id=1N_CcdvwxUOHU_fGO6b0TRhaPS4JZPZ3n"
    },
    "forum": "148",
    "id": "148",
    "pic_id": "https://drive.google.com/open?id=1LS4URElVU1MG_JDFyp88qQCK29D6JPo_",
    "position": "07",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "3"
  },
  {
    "content": {
      "TLDR": "Carnatic Music is a South Indian art and devotional music practice in which melodic patterns (motifs and phrases), known as sa\u00f1c\u0101ras, play a crucial structural and expressive role. We demonstrate how the combination of transposition invariant features learnt by a Complex Autoencoder (CAE) and predominant pitch tracks extracted using a Frequency-Temporal Attention Network (FTA-Net) can be used to annotate and group regions of variable-length, repeated, melodic patterns in audio recordings of multiple Carnatic Music performances. These models are trained on novel/expert-curated datasets of hundreds of Carnatic audio recordings and the extraction process tailored to account for the unique characteristics of sa\u00f1c\u0101ras in Carnatic Music. Experimental results show that the proposed method is able to identify 54% of all sa\u00f1caras annotated by a professional Carnatic vocalist. Code to reproduce and interact with these results is available online.",
      "abstract": "Carnatic Music is a South Indian art and devotional music practice in which melodic patterns (motifs and phrases), known as sa\u00f1c\u0101ras, play a crucial structural and expressive role. We demonstrate how the combination of transposition invariant features learnt by a Complex Autoencoder (CAE) and predominant pitch tracks extracted using a Frequency-Temporal Attention Network (FTA-Net) can be used to annotate and group regions of variable-length, repeated, melodic patterns in audio recordings of multiple Carnatic Music performances. These models are trained on novel/expert-curated datasets of hundreds of Carnatic audio recordings and the extraction process tailored to account for the unique characteristics of sa\u00f1c\u0101ras in Carnatic Music. Experimental results show that the proposed method is able to identify 54% of all sa\u00f1caras annotated by a professional Carnatic vocalist. Code to reproduce and interact with these results is available online.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1TeKu8Xips2y7bmwPOUjA6K4ilwOadhet)</b>",
      "authors": [
        "Nuttall, Thomas*",
        " Plaja-Roglans, Gen\u00eds",
        " Pearson, Lara",
        " Serra, Xavier"
      ],
      "authors_and_affil": [
        "Thomas Nuttall (Music Technology Group, Universitat Pompeu Fabra, Barcelona)*",
        " Gen\u00eds Plaja-Roglans (Music Technology Group, Universitat Pompeu Fabra, Barcelona)",
        " Lara Pearson (Max Planck Institute for Empirical Aesthetics)",
        " Xavier Serra (Music Technology Group, Universitat Pompeu Fabra, Barcelona)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CGCW91RT",
      "day": "2",
      "keywords": [
        " Musical features and properties -> melody and motives",
        "MIR tasks -> pattern matching and detection",
        "Domain knowledge -> computational ethnomusicology",
        " Musical features and properties -> timbre, instrumentation, and singing voice",
        " Domain knowledge -> computational music theory and musicology"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Bengaluru",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000040.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1ISIt3wM8Q8pKZhKbhVWg-Q7ISr-ECRTD",
      "session": [
        "3"
      ],
      "slack_channel": "p3-08-nuttall",
      "title": "In Search of Sa\u00f1c\u0101ras: Tradition-informed Repeated Melodic Pattern Recognition in Carnatic Music",
      "video": "https://drive.google.com/uc?export=preview&id=1TeKu8Xips2y7bmwPOUjA6K4ilwOadhet"
    },
    "forum": "334",
    "id": "334",
    "pic_id": "https://drive.google.com/open?id=1ysRgRh7oUb0piTR7uRQCpIiwQjgdSMlM",
    "position": "08",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "3"
  },
  {
    "content": {
      "TLDR": "Chinese national pentatonic modes, with five tones of Gong, Shang, Jue, Zhi and Yu as the core, play an essential role in traditional Chinese music culture. After the early twentieth century, with the development of new Chinese music, the ancient Chinese theory of scales gradually developed into a new pentatonic modes theory under the influence of western music. In this paper, we briefly introduce our self-built CNPM (Chinese National Pentatonic Modes) Dataset, then design residual convolutional neural network models to identify which TongGong system the mode belongs, the pitch of tonic, the mode pattern and the mode type from audio signals, in combination with musical domain knowledge. We use both single-task and multi-task models with three strategies for identification, and compare them with a simple template-based baseline method. In experiments, we use seven accuracy metrics to evaluate the models. The results on identifying both the tonic pitch and the pattern of mode correctly achieve an average accuracy of 69.65%. As an initial research on automatic Chinese national pentatonic modes recognition, this work will contribute to the development of multicultural music information retrieval, computational ethnomusicology and five-tone music therapy.",
      "abstract": "Chinese national pentatonic modes, with five tones of Gong, Shang, Jue, Zhi and Yu as the core, play an essential role in traditional Chinese music culture. After the early twentieth century, with the development of new Chinese music, the ancient Chinese theory of scales gradually developed into a new pentatonic modes theory under the influence of western music. In this paper, we briefly introduce our self-built CNPM (Chinese National Pentatonic Modes) Dataset, then design residual convolutional neural network models to identify which TongGong system the mode belongs, the pitch of tonic, the mode pattern and the mode type from audio signals, in combination with musical domain knowledge. We use both single-task and multi-task models with three strategies for identification, and compare them with a simple template-based baseline method. In experiments, we use seven accuracy metrics to evaluate the models. The results on identifying both the tonic pitch and the pattern of mode correctly achieve an average accuracy of 69.65%. As an initial research on automatic Chinese national pentatonic modes recognition, this work will contribute to the development of multicultural music information retrieval, computational ethnomusicology and five-tone music therapy.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1SmG4hcog1aGDre-ai_mBVSkNiURd2mky)</b>",
      "authors": [
        "Wang, Zhaowen",
        " Che, Mingjin",
        " Yang, Yue",
        " Meng , Wen wu ",
        " Li, Qinyu",
        " Xia, Fan",
        " Li, Wei*"
      ],
      "authors_and_affil": [
        "Zhaowen Wang (Central Conservatory of Music)",
        " Mingjin Che (Sichuan Conservatory of Music)",
        " Yue Yang (Central Conservatory of Music)",
        " Wen wu  Meng  (Sichuan Conservatory of Music)",
        " Qinyu Li (Sichuan Conservatory Of Music)",
        " Fan Xia (Sichuan Conservatory of Music )",
        " Wei Li (Fudan University)*"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CCP1UBPY",
      "day": "2",
      "keywords": [
        "MIR tasks -> automatic classification",
        " Musical features and properties",
        " Evaluation, datasets, and reproducibility -> novel datasets and use cases",
        "Domain knowledge -> computational ethnomusicology"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Virtually",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000041.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1OO5QLt-Ll_rSK65ElTU1VpnnsWY5oTdI",
      "session": [
        "3"
      ],
      "slack_channel": "p3-09-wang",
      "title": "Automatic Chinese National Pentatonic Modes Recognition Using Convolutional Neural Network",
      "video": "https://drive.google.com/uc?export=preview&id=1SmG4hcog1aGDre-ai_mBVSkNiURd2mky"
    },
    "forum": "63",
    "id": "63",
    "pic_id": "https://drive.google.com/open?id=1IPqfItMzfupr0WxPdq9AT3RPOH6Q9ZCW",
    "position": "09",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "3"
  },
  {
    "content": {
      "TLDR": "New datasets of non-Western traditional music contribute to the development of knowledge in MIR and allow computational techniques to inform ethnomusicology. We present an annotated dataset of traditional vocal polyphony from two regions of the Republic of Georgia with disparate musical characteristics. The audio for each song consists of four polyphonic recordings of one performance from different microphones. We present a process and workflow that we use to annotate the dataset, which takes advantage of the salience of individual voices in each recording. The process results in an $f_0$ estimate for each vocal part.",
      "abstract": "New datasets of non-Western traditional music contribute to the development of knowledge in MIR and allow computational techniques to inform ethnomusicology. We present an annotated dataset of traditional vocal polyphony from two regions of the Republic of Georgia with disparate musical characteristics. The audio for each song consists of four polyphonic recordings of one performance from different microphones. We present a process and workflow that we use to annotate the dataset, which takes advantage of the salience of individual voices in each recording. The process results in an $f_0$ estimate for each vocal part.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1z3l8KSI2W3EubdKicfBRsxFPbXc3iHhP)</b>",
      "authors": [
        "Gillman, David*",
        " Kutlay, Atalay",
        " Goyat, Uday"
      ],
      "authors_and_affil": [
        "David Gillman (New College of Florida)*",
        " Uday Goyat (Georgia Institute of Technology)",
        " Atalay Kutlay (New College of Florida)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CKB8CK0A",
      "day": "2",
      "keywords": [
        "MIR tasks -> music transcription and annotation",
        " Musical features and properties -> timbre, instrumentation, and singing voice",
        "Applications -> music heritage and sustainability",
        " Evaluation, datasets, and reproducibility -> evaluation metrics"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Virtually",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000042.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1WCurTzvut5fdWEAi4GvkWuH2b6SNwqzM",
      "session": [
        "3"
      ],
      "slack_channel": "p3-10-gillman",
      "title": "Teach Yourself Georgian Folk Songs Dataset: A Annotated Corpus Of Traditional Vocal Polyphony",
      "video": "https://drive.google.com/uc?export=preview&id=1z3l8KSI2W3EubdKicfBRsxFPbXc3iHhP"
    },
    "forum": "81",
    "id": "81",
    "pic_id": "https://drive.google.com/open?id=1iUO9AOV5BYS29bZeqMAmIv7b4WNeX7CP",
    "position": "10",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "3"
  },
  {
    "content": {
      "TLDR": "Beat and downbeat tracking models have improved significantly in recent years with the introduction of deep learning methods. However, despite these improvements, several challenges remain. Particularly, the adaptation of available models to underrepresented music traditions in MIR is usually synonymous with collecting and annotating large amounts of data, which is impractical and time-consuming. Transfer learning, data augmentation, and fine-tuning techniques have been used quite successfully in related tasks and are known to alleviate this bottleneck. Furthermore, when studying these music traditions, models are not required to generalize to multiple mainstream music genres but to perform well in more constrained, homogeneous conditions. In this work, we investigate simple yet effective strategies to adapt beat and downbeat tracking models to two different Latin American music traditions and analyze the feasibility of these adaptations in real-world applications concerning the data and computational requirements. Contrary to common belief, our findings show it is possible to achieve good performance by spending just a few minutes annotating a portion of the data and training a model in a standard CPU machine, with the precise amount of resources needed depending on the task and the complexity of the dataset.",
      "abstract": "Beat and downbeat tracking models have improved significantly in recent years with the introduction of deep learning methods. However, despite these improvements, several challenges remain. Particularly, the adaptation of available models to underrepresented music traditions in MIR is usually synonymous with collecting and annotating large amounts of data, which is impractical and time-consuming. Transfer learning, data augmentation, and fine-tuning techniques have been used quite successfully in related tasks and are known to alleviate this bottleneck. Furthermore, when studying these music traditions, models are not required to generalize to multiple mainstream music genres but to perform well in more constrained, homogeneous conditions. In this work, we investigate simple yet effective strategies to adapt beat and downbeat tracking models to two different Latin American music traditions and analyze the feasibility of these adaptations in real-world applications concerning the data and computational requirements. Contrary to common belief, our findings show it is possible to achieve good performance by spending just a few minutes annotating a portion of the data and training a model in a standard CPU machine, with the precise amount of resources needed depending on the task and the complexity of the dataset.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1bphH5JcIcq-4v8AMWlIk81snYc2fVe7J)</b>",
      "authors": [
        "Maia, Lucas S*",
        " Rocamora, Mart\u00edn",
        " Biscainho, Luiz W P ",
        " Fuentes, Magdalena"
      ],
      "authors_and_affil": [
        "Lucas S Maia (Federal University of Rio de Janeiro)*",
        " Mart\u00edn Rocamora (Universidad de la Rep\u00fablica)",
        " Luiz W P  Biscainho (UFRJ)",
        " Magdalena Fuentes (New York University)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CMQW7C5A",
      "day": "2",
      "keywords": [
        " Human-centered MIR -> personalization",
        " Domain knowledge -> computational ethnomusicology",
        " Domain knowledge -> machine learning/artificial intelligence for music",
        "Musical features and properties -> rhythm, beat, tempo",
        "Applications -> music heritage and sustainability"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Virtually",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000043.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1iPl1-HQiUcCQERIUEZEp7NLRPrD77gjZ",
      "session": [
        "3"
      ],
      "slack_channel": "p3-11-maia",
      "title": "Adapting meter tracking models to Latin American music",
      "video": "https://drive.google.com/uc?export=preview&id=1bphH5JcIcq-4v8AMWlIk81snYc2fVe7J"
    },
    "forum": "272",
    "id": "272",
    "pic_id": "https://drive.google.com/open?id=1AJYM4KL8DOtgytZjToJusjiV8ibKJ-hP",
    "position": "11",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "3"
  },
  {
    "content": {
      "TLDR": "Computational Musicology and Music Information Retrieval (MIR) address the core musical question under study from a different perspective, often a combination of top-down vs. bottom-up approaches. However, the evaluation metrics for MIR tend to capture the model accuracy in terms of the goal. For instance, mode (melodic framework) recognition is implemented with a goal to evaluate and compare melodic analysis approaches, but it is worth investigating if at all it lends itself as one befitting proxy task. In this work, we aim to review whether the model actually learns the task it is intended for. This is particularly relevant in non-Eurogenetic music repertoires where the grammatical rules are rather prescriptive. We employ methodologies that combine domain-knowledge and data-driven optimizations as a possible way for a comprehensive understanding of these relationships. This is tested on Makam which is one of the understudied corpora in MIR. We evaluate an array of feature-engineering methods on the largest mode recognition dataset curated for Ottoman-Turkish makam music, composed of 1000 recordings in 50 makams. We adapted the time-delayed melody surfaces (TDMS) feature, which in combination with support vector machine (SVM) classifier yields 77.2% recognition accuracy, comparable to the current state-of-the-art. We also address (ethno)musicology-driven tasks with a view to gathering deeper insights into this music, such as tuning, intonation, and melodic similarity. We aim to propose avenues to extend the study to makam characterization over the mere goal of recognizing the mode, to better understand the (dis)similarity space and other plausible musically interesting facets.",
      "abstract": "Computational Musicology and Music Information Retrieval (MIR) address the core musical question under study from a different perspective, often a combination of top-down vs. bottom-up approaches. However, the evaluation metrics for MIR tend to capture the model accuracy in terms of the goal. For instance, mode (melodic framework) recognition is implemented with a goal to evaluate and compare melodic analysis approaches, but it is worth investigating if at all it lends itself as one befitting proxy task. In this work, we aim to review whether the model actually learns the task it is intended for. This is particularly relevant in non-Eurogenetic music repertoires where the grammatical rules are rather prescriptive. We employ methodologies that combine domain-knowledge and data-driven optimizations as a possible way for a comprehensive understanding of these relationships. This is tested on Makam which is one of the understudied corpora in MIR. We evaluate an array of feature-engineering methods on the largest mode recognition dataset curated for Ottoman-Turkish makam music, composed of 1000 recordings in 50 makams. We adapted the time-delayed melody surfaces (TDMS) feature, which in combination with support vector machine (SVM) classifier yields 77.2% recognition accuracy, comparable to the current state-of-the-art. We also address (ethno)musicology-driven tasks with a view to gathering deeper insights into this music, such as tuning, intonation, and melodic similarity. We aim to propose avenues to extend the study to makam characterization over the mere goal of recognizing the mode, to better understand the (dis)similarity space and other plausible musically interesting facets.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=16w-ry78fw4S-5HEUvnJcYXEWsia3F5UN)</b>",
      "authors": [
        "Ganguli, Kaustuv Kanti*",
        " \u015eent\u00fcrk, Sertan",
        " Guedes, Carlos"
      ],
      "authors_and_affil": [
        "Kaustuv Kanti Ganguli (Zayed University)*",
        " Sertan \u015eent\u00fcrk (Kobalt Music Group / Independent Researcher)",
        " Carlos Guedes (NYU Abu Dhabi)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CGCVGXKP",
      "day": "2",
      "keywords": [
        " Evaluation, datasets, and reproducibility -> MIR tasks",
        "Domain knowledge -> computational ethnomusicology",
        "Evaluation, datasets, and reproducibility -> evaluation methodology",
        " Philosophical and ethical discussions -> philosophical and methodological foundations",
        " MIR tasks -> automatic classification"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Bengaluru",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000044.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1-0vY1Qu69VcngK5sKnq0B8zExsegxTjw",
      "session": [
        "3"
      ],
      "slack_channel": "p3-12-ganguli",
      "title": "Critiquing Task- versus Goal-oriented Approaches: A Case for Makam Recognition",
      "video": "https://drive.google.com/uc?export=preview&id=16w-ry78fw4S-5HEUvnJcYXEWsia3F5UN"
    },
    "forum": "235",
    "id": "235",
    "pic_id": "https://drive.google.com/open?id=1t_pw1VcjjbJJXzJtdJsKy6B-HCF6aQDO",
    "position": "12",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "3"
  },
  {
    "content": {
      "TLDR": "Studying under-represented music traditions under the MIR scope is crucial, not only for developing novel analysis tools, but also for unveiling musical functions that might prove useful in studying world musics. This paper presents a dataset for Greek Traditional and Folk music that includes 1570 pieces, summing in around 80 hours of data. The dataset incorporates YouTube timestamped links for retrieving audio and video, along with rich metadata information with regards to instrumentation, geography and genre, among others. The content has been collected from a Greek documentary series that is available online, where academics present music traditions of Greece with live music and dance performance during the show, along with discussions about social, cultural and musicological aspects of the presented music. Therefore, this procedure has resulted in a significant wealth of descriptions regarding a variety of aspects, such as musical genre, places of origin and musical instruments. In addition, the audio recordings were performed under strict production-level specifications, in terms of recording equipment, leading to very clean and homogeneous audio content. In this work, apart from presenting the dataset in detail, we propose a baseline deep-learning classification approach to recognize the involved musicological attributes. The dataset, the baseline classification methods and the models are provided in public repositories. Future directions for further refining the dataset are also discussed.",
      "abstract": "Studying under-represented music traditions under the MIR scope is crucial, not only for developing novel analysis tools, but also for unveiling musical functions that might prove useful in studying world musics. This paper presents a dataset for Greek Traditional and Folk music that includes 1570 pieces, summing in around 80 hours of data. The dataset incorporates YouTube timestamped links for retrieving audio and video, along with rich metadata information with regards to instrumentation, geography and genre, among others. The content has been collected from a Greek documentary series that is available online, where academics present music traditions of Greece with live music and dance performance during the show, along with discussions about social, cultural and musicological aspects of the presented music. Therefore, this procedure has resulted in a significant wealth of descriptions regarding a variety of aspects, such as musical genre, places of origin and musical instruments. In addition, the audio recordings were performed under strict production-level specifications, in terms of recording equipment, leading to very clean and homogeneous audio content. In this work, apart from presenting the dataset in detail, we propose a baseline deep-learning classification approach to recognize the involved musicological attributes. The dataset, the baseline classification methods and the models are provided in public repositories. Future directions for further refining the dataset are also discussed.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1gOXyVuIhiESwav2kfQLKX2DQ1S_96lUJ)</b>",
      "authors": [
        "Papaioannou, Charilaos*",
        " Valiantzas, Ioannis",
        " Giannakopoulos, Theodore",
        " Kaliakatsos-Papakostas, Maximos",
        " Potamianos, Alexandros"
      ],
      "authors_and_affil": [
        "Charilaos Papaioannou (School of ECE, National Technical University of Athens)*",
        " Ioannis Valiantzas (Department of Music Studies, National and Kapodistrian University Of Athens)",
        " Theodore Giannakopoulos (NCSR Demokritos)",
        " Maximos Kaliakatsos-Papakostas (Athena RC)",
        " Alexandros Potamianos (National Technical University of Athens)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CGCVLJMB",
      "day": "2",
      "keywords": [
        "Evaluation, datasets, and reproducibility -> novel datasets and use cases",
        "Domain knowledge -> computational ethnomusicology"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Bengaluru",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000045.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1Xt4yhB5xGArAHL6EKpe1OUABxTQkHwfA",
      "session": [
        "3"
      ],
      "slack_channel": "p3-13-papaioannou",
      "title": "A Dataset for Greek Traditional and Folk Music: Lyra",
      "video": "https://drive.google.com/uc?export=preview&id=1gOXyVuIhiESwav2kfQLKX2DQ1S_96lUJ"
    },
    "forum": "245",
    "id": "245",
    "pic_id": "https://drive.google.com/open?id=1vApZRJ_aVN-q1dwzDfi_045vK77OCNse",
    "position": "13",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "3"
  },
  {
    "content": {
      "TLDR": "In this paper, we focus on singing techniques within the scope of music information retrieval research. We investigate how singers use singing techniques using real-world recordings of famous solo singers in Japanese popular music songs (J-POP). First, we built a new dataset of singing techniques. The dataset consists of 168 commercial J-POP songs, and each song is annotated using various singing techniques with timestamps and vocal pitch contours. We also present descriptive statistics of singing techniques on the dataset to clarify what and how often singing techniques appear. We further explored the difficulty of the automatic detection of singing techniques using previously proposed machine learning techniques. In the detection, we also investigate the effectiveness of auxiliary information (i.e., pitch and distribution of label duration), not only providing the baseline. The best result achieves 40.4% at macro-average F-measure on nine-way multi-class detection. We provide the annotation of the dataset and its detail on the appendix website.",
      "abstract": "In this paper, we focus on singing techniques within the scope of music information retrieval research. We investigate how singers use singing techniques using real-world recordings of famous solo singers in Japanese popular music songs (J-POP). First, we built a new dataset of singing techniques. The dataset consists of 168 commercial J-POP songs, and each song is annotated using various singing techniques with timestamps and vocal pitch contours. We also present descriptive statistics of singing techniques on the dataset to clarify what and how often singing techniques appear. We further explored the difficulty of the automatic detection of singing techniques using previously proposed machine learning techniques. In the detection, we also investigate the effectiveness of auxiliary information (i.e., pitch and distribution of label duration), not only providing the baseline. The best result achieves 40.4% at macro-average F-measure on nine-way multi-class detection. We provide the annotation of the dataset and its detail on the appendix website.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1jz37KNjy-TKBSwd01jMxO8GwqA44ic4R)</b>",
      "authors": [
        "Yamamoto, Yuya*",
        " Nam, Juhan",
        " Terasawa, Hiroko"
      ],
      "authors_and_affil": [
        "Yuya Yamamoto (University of Tsukuba)*",
        " Juhan Nam (KAIST)",
        " Hiroko Terasawa (University of Tsukuba)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CMQWDUUU",
      "day": "2",
      "keywords": [
        "Musical features and properties -> expression and performative aspects of music",
        " Evaluation, datasets, and reproducibility -> novel datasets and use cases",
        " Musical features and properties -> timbre, instrumentation, and singing voice",
        "Applications -> performance, and production",
        " Domain knowledge -> computational music theory and musicology",
        " MIR tasks -> automatic classification"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Bengaluru",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000046.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1PrxS8bfAamdo3l3xmSdO9NSEnW9Av-ke",
      "session": [
        "3"
      ],
      "slack_channel": "p3-14-yamamoto",
      "title": "Analysis and detection of singing techniques in repertoires of J-POP solo singers",
      "video": "https://drive.google.com/uc?export=preview&id=1jz37KNjy-TKBSwd01jMxO8GwqA44ic4R"
    },
    "forum": "296",
    "id": "296",
    "pic_id": "https://drive.google.com/open?id=1NevCH-gs0euK96cskq0bicLEmHnZMGAk",
    "position": "14",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "3"
  },
  {
    "content": {
      "TLDR": "Rhythm quantisation is an essential part of converting performance MIDI recordings into musical scores. Previous works on rhythm quantisation are limited to the use of probabilistic or statistical methods. In this paper, we propose a MIDI-to-score quantisation method using a convolutional-recurrent neural network (CRNN) trained on MIDI note sequences to predict whether notes are on beats. Then, we expand the CRNN model to predict the quantised times for all beat and non-beat notes. Furthermore, we enable the model to predict the key signatures, time signatures, and hand parts of all notes. Our proposed performance MIDI-to-score system achieves significantly better performance compared to commercial software evaluated on the MV2H metric. We release the toolbox for converting performance MIDI into MIDI scores at: https://github.com/cheriell/PM2S .",
      "abstract": "Rhythm quantisation is an essential part of converting performance MIDI recordings into musical scores. Previous works on rhythm quantisation are limited to the use of probabilistic or statistical methods. In this paper, we propose a MIDI-to-score quantisation method using a convolutional-recurrent neural network (CRNN) trained on MIDI note sequences to predict whether notes are on beats. Then, we expand the CRNN model to predict the quantised times for all beat and non-beat notes. Furthermore, we enable the model to predict the key signatures, time signatures, and hand parts of all notes. Our proposed performance MIDI-to-score system achieves significantly better performance compared to commercial software evaluated on the MV2H metric. We release the toolbox for converting performance MIDI into MIDI scores at: https://github.com/cheriell/PM2S .<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1_UAm2QXD12IhwzmaR2nCHqBclWZj0Dd3)</b>",
      "authors": [
        "Liu, Lele*",
        " Kong, Qiuqiang",
        " Morfi, Veronica",
        " Benetos, Emmanouil"
      ],
      "authors_and_affil": [
        "Lele Liu (Queen Mary University of London)*",
        " Qiuqiang Kong (ByteDance)",
        " Veronica Morfi (Queen Mary University of London)",
        " Emmanouil Benetos (Queen Mary University of London)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04C4QRE9P1",
      "day": "2",
      "keywords": [
        "MIR tasks -> music transcription and annotation",
        "Domain knowledge -> machine learning/artificial intelligence for music",
        " Musical features and properties -> rhythm, beat, tempo",
        " MIR fundamentals and methodology -> symbolic music processing"
      ],
      "long_presentation": "TRUE",
      "paper_presentation": "In-person, in Bengaluru",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000047.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1iBziT4mXvY8sj3Q3zYvSiAs3mdRzRFDJ",
      "session": [
        "4"
      ],
      "slack_channel": "p4-01-liu",
      "title": "Performance MIDI-to-score conversion by neural beat tracking",
      "video": "https://drive.google.com/uc?export=preview&id=1_UAm2QXD12IhwzmaR2nCHqBclWZj0Dd3"
    },
    "forum": "335",
    "id": "335",
    "pic_id": "https://drive.google.com/open?id=1ZSJreE-WDJPheZ6_mcFMMwun8GnzPNQx",
    "position": "01",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "Since most of music has repetitive structures from motifs to phrases, repeating musical ideas can be a basic operation for music composition. The basic block that we focus on is conceptualized as loops which are essential ingredients of music. Furthermore, meaningful note patterns can be formed in a finite space, so it is sufficient to represent them with combinations of discrete symbols as done in other domains. In this work, we propose symbolic music loop generation via learning discrete representations. We first extract loops from MIDI datasets using a loop detector and then learn an autoregressive model trained by discrete latent codes of the extracted loops. We show that our model outperforms well-known music generative models in terms of both fidelity and diversity, evaluating on random space. Our code and supplementary materials are available at https://github.com/sjhan91/Loop_VQVAE_Official.",
      "abstract": "Since most of music has repetitive structures from motifs to phrases, repeating musical ideas can be a basic operation for music composition. The basic block that we focus on is conceptualized as loops which are essential ingredients of music. Furthermore, meaningful note patterns can be formed in a finite space, so it is sufficient to represent them with combinations of discrete symbols as done in other domains. In this work, we propose symbolic music loop generation via learning discrete representations. We first extract loops from MIDI datasets using a loop detector and then learn an autoregressive model trained by discrete latent codes of the extracted loops. We show that our model outperforms well-known music generative models in terms of both fidelity and diversity, evaluating on random space. Our code and supplementary materials are available at https://github.com/sjhan91/Loop_VQVAE_Official.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1gCIBEdycNqh6biSEXvnEr_PPw8jC0cGf)</b>",
      "authors": [
        "Han, Sangjun*",
        " Ihm, Hyeongrae",
        " Lee, Moontae",
        " Lim, Woohyung"
      ],
      "authors_and_affil": [
        "Sangjun Han (LG AI Research)*",
        " Hyeongrae Ihm (LG AI Research)",
        " Moontae Lee (University of Illinois at Chicago)",
        " Woohyung Lim (LG AI Research)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CGCU2S0M",
      "day": "2",
      "keywords": [
        "Domain knowledge -> machine learning/artificial intelligence for music",
        " MIR fundamentals and methodology -> symbolic music processing",
        " Musical features and properties -> expression and performative aspects of music",
        " MIR fundamentals and methodology -> multimodality",
        "MIR tasks -> music generation"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Virtually",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000048.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1BQ5-xEjzL7cMUjiljAp3d2DkGyK9qwvo",
      "session": [
        "4"
      ],
      "slack_channel": "p4-02-han",
      "title": "Symbolic Music Loop Generation with Neural Discrete Representations",
      "video": "https://drive.google.com/uc?export=preview&id=1gCIBEdycNqh6biSEXvnEr_PPw8jC0cGf"
    },
    "forum": "66",
    "id": "66",
    "pic_id": "https://drive.google.com/open?id=16wdCJIQK8wnkfsSp3h9t4lwWIcA8mxuQ",
    "position": "02",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "Music mixing traditionally involves recording instruments in the form of clean, individual tracks and blending them into a final mixture using audio effects and expert knowledge (e.g., a mixing engineer). The automation of music production tasks has become an emerging field in recent years, where rule-based methods and machine learning approaches have been explored. Nevertheless, the lack of dry or clean instrument recordings limits the performance of such models, which is still far from professional human-made mixes. We explore whether we can use out-of-domain data such as wet or processed multitrack music recordings and repurpose it to train supervised deep learning models that can bridge the current gap in automatic mixing quality. To achieve this we propose a novel data preprocessing method that allows the models to perform automatic music mixing. We also redesigned a listening test method for evaluating music mixing systems. We validate our results through such subjective tests using highly experienced mixing engineers as participants.",
      "abstract": "Music mixing traditionally involves recording instruments in the form of clean, individual tracks and blending them into a final mixture using audio effects and expert knowledge (e.g., a mixing engineer). The automation of music production tasks has become an emerging field in recent years, where rule-based methods and machine learning approaches have been explored. Nevertheless, the lack of dry or clean instrument recordings limits the performance of such models, which is still far from professional human-made mixes. We explore whether we can use out-of-domain data such as wet or processed multitrack music recordings and repurpose it to train supervised deep learning models that can bridge the current gap in automatic mixing quality. To achieve this we propose a novel data preprocessing method that allows the models to perform automatic music mixing. We also redesigned a listening test method for evaluating music mixing systems. We validate our results through such subjective tests using highly experienced mixing engineers as participants.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1cEgcVYs0PxgsXHdJx5sLOpxW6GivpY4J)</b>",
      "authors": [
        "Martinez Ramirez, Marco A*",
        " Liao, WeiHsiang",
        " Nagashima, Chihiro",
        " Fabbro, Giorgio",
        " Uhlich, Stefan",
        " Mitsufuji, Yuki"
      ],
      "authors_and_affil": [
        "Marco A Martinez Ramirez (Sony Group Corporation, Tokyo, Japan)*",
        " WeiHsiang Liao (Sony Group Corporation, Tokyo, Japan)",
        " Chihiro Nagashima (Sony Group Corporation, Tokyo, Japan)",
        " Giorgio Fabbro (Sony Europe B.V., Stuttgart, Germany)",
        " Stefan Uhlich (Sony Europe B.V., Stuttgart, Germany)",
        " Yuki Mitsufuji (Sony Group Corporation, Tokyo, Japan)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CGCTK9NZ",
      "day": "2",
      "keywords": [
        "Domain knowledge -> machine learning/artificial intelligence for music",
        " Musical features and properties -> musical affect, emotion and mood",
        "Applications -> performance, and production",
        " MIR tasks -> music synthesis and transformation",
        " MIR fundamentals and methodology -> music signal processing"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Bengaluru",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000049.pdf",
      "poster_pdf": "https://drive.google.com/open?id=11_9PlJlhKdYVEOvm8YGOuW3FeBJ-VZpo",
      "session": [
        "4"
      ],
      "slack_channel": "p4-03-ramirez",
      "title": "Automatic music mixing with deep learning and out-of-domain data",
      "video": "https://drive.google.com/uc?export=preview&id=1cEgcVYs0PxgsXHdJx5sLOpxW6GivpY4J"
    },
    "forum": "11",
    "id": "11",
    "pic_id": "https://drive.google.com/open?id=102P91LePzzQTV-zdeul4BX8q5REUsdWH",
    "position": "03",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "Music style translation aims to generate variations of existing pieces of music by altering the style-related characteristics of the original piece while content, such as the melody, remains unchanged. These alterations could involve timbre translation, re-harmonization, or music rearrangement. Previous studies have achieved promising results utilizing time-frequency and symbolic music representations. Music style translation on raw audio has also been investigated and applied to single-instrument pieces. Although processing raw audio is more challenging, it provides richer information about timbres, dynamics, and articulations.\nIn this paper, we introduce Music-STAR, the first audio-based translation system that translates the existing instruments in a piece into a set of target instruments without using source separation. To conduct our experiments, we also present an audio dataset that contains two-track pieces performed by two instrument sets alongside their stems. We carry out subjective and objective evaluations to compare Music-STAR with a variety of baseline methods and demonstrate its superiority.",
      "abstract": "Music style translation aims to generate variations of existing pieces of music by altering the style-related characteristics of the original piece while content, such as the melody, remains unchanged. These alterations could involve timbre translation, re-harmonization, or music rearrangement. Previous studies have achieved promising results utilizing time-frequency and symbolic music representations. Music style translation on raw audio has also been investigated and applied to single-instrument pieces. Although processing raw audio is more challenging, it provides richer information about timbres, dynamics, and articulations.\nIn this paper, we introduce Music-STAR, the first audio-based translation system that translates the existing instruments in a piece into a set of target instruments without using source separation. To conduct our experiments, we also present an audio dataset that contains two-track pieces performed by two instrument sets alongside their stems. We carry out subjective and objective evaluations to compare Music-STAR with a variety of baseline methods and demonstrate its superiority.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1jQifWWW-m1J2KvfORdDMM97DaTpDrf28)</b>",
      "authors": [
        "Alinoori, Mahshid*",
        " Tzerpos, Vassilios"
      ],
      "authors_and_affil": [
        "Mahshid Alinoori (Department of Electrical Engineering and Computer Science, York University, Canada)*",
        " Vassilios Tzerpos (Department of Electrical Engineering and Computer Science, York University, Canada)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CCP2N0NA",
      "day": "2",
      "keywords": [
        "MIR tasks -> music synthesis and transformation",
        " Musical features and properties -> timbre, instrumentation, and singing voice",
        "Musical features and properties -> musical style and genre"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Virtually",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000050.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1gu839MnctG_GecdOT81yvT6GGovAQ8B3",
      "session": [
        "4"
      ],
      "slack_channel": "p4-04-alinoori",
      "title": "Music-STAR: a Style Translation system for Audio-based Re-instrumentation",
      "video": "https://drive.google.com/uc?export=preview&id=1jQifWWW-m1J2KvfORdDMM97DaTpDrf28"
    },
    "forum": "139",
    "id": "139",
    "pic_id": "https://drive.google.com/open?id=1u1jXxXFS60oERhXk2whvvYVSgTIh1sv_",
    "position": "04",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "Music signals are difficult to interpret from their low-level features, perhaps even more than images: e.g. highlighting part of a spectrogram or an image is often insufficient to convey high-level ideas that are genuinely relevant to humans. In computer vision, concept learning was therein proposed to adjust explanations to the right abstraction level (e.g. detect clinical concepts from radiographs). These methods have yet to be used for MIR.\n\nIn this paper, we adapt concept learning to the realm of music, with its particularities. For instance, music concepts are typically non-independent and of mixed nature (e.g. genre, instruments, mood), unlike previous work that assumed disentangled concepts.\nWe propose a method to learn numerous music concepts from audio and then automatically hierarchise them to expose their mutual relationships. We conduct experiments on datasets of playlists from a music streaming service, serving as a few annotated examples for diverse concepts. Evaluations show that the mined hierarchies are aligned with both ground-truth hierarchies of concepts -- when available -- and with proxy sources of concept similarity in the general case.",
      "abstract": "Music signals are difficult to interpret from their low-level features, perhaps even more than images: e.g. highlighting part of a spectrogram or an image is often insufficient to convey high-level ideas that are genuinely relevant to humans. In computer vision, concept learning was therein proposed to adjust explanations to the right abstraction level (e.g. detect clinical concepts from radiographs). These methods have yet to be used for MIR.\n\nIn this paper, we adapt concept learning to the realm of music, with its particularities. For instance, music concepts are typically non-independent and of mixed nature (e.g. genre, instruments, mood), unlike previous work that assumed disentangled concepts.\nWe propose a method to learn numerous music concepts from audio and then automatically hierarchise them to expose their mutual relationships. We conduct experiments on datasets of playlists from a music streaming service, serving as a few annotated examples for diverse concepts. Evaluations show that the mined hierarchies are aligned with both ground-truth hierarchies of concepts -- when available -- and with proxy sources of concept similarity in the general case.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1-iaBvVq0MRFBdm1wnl5TlOpkKodct-YD)</b>",
      "authors": [
        "Afchar, Darius*",
        " Hennequin, Romain",
        " Guigue, Vincent"
      ],
      "authors_and_affil": [
        "Darius Afchar (Deezer Research)*",
        " Romain Hennequin (Deezer Research)",
        " Vincent Guigue (LIP6)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CMQU38A0",
      "day": "2",
      "keywords": [
        "Domain knowledge -> machine learning/artificial intelligence for music",
        " Musical features and properties -> musical affect, emotion and mood",
        " Musical features and properties -> musical style and genre",
        "Domain knowledge -> representations of music",
        " MIR fundamentals and methodology -> music signal processing",
        " MIR tasks -> automatic classification"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Bengaluru",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000051.pdf",
      "poster_pdf": "https://drive.google.com/open?id=12qKnF-rdZocB5ibGX2GXm0kIHP2GnBJo",
      "session": [
        "4"
      ],
      "slack_channel": "p4-05-afchar",
      "title": "Learning Unsupervised Hierarchies of Audio Concepts",
      "video": "https://drive.google.com/uc?export=preview&id=1-iaBvVq0MRFBdm1wnl5TlOpkKodct-YD"
    },
    "forum": "29",
    "id": "29",
    "pic_id": "https://drive.google.com/open?id=1kkweuOto1zjTcZVj8ttOSRMOUTxA0jIZ",
    "position": "05",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "Song embeddings are a key component of most music recommendation engines. \nIn this work, we study the hyper-parameter optimization of behavioral song embeddings based on Word2Vec on a selection of downstream tasks, namely next-song recommendation, false neighbor rejection, and artist and genre clustering. We present new optimization objectives and metrics to monitor the effects of hyper-parameter optimization. We show that single-objective optimization can cause side effects on the non optimized metrics and propose a simple multi-objective optimization to mitigate these effects.\nWe find that next-song recommendation quality of Word2Vec is anti-correlated with song popularity, and we show how song embedding optimization can balance performance across different popularity levels.\nWe then show potential positive downstream effects on the task of play prediction.\nFinally, we provide useful insights on the effects of training dataset scale by testing hyper-parameter optimization on an industry-scale dataset.",
      "abstract": "Song embeddings are a key component of most music recommendation engines. \nIn this work, we study the hyper-parameter optimization of behavioral song embeddings based on Word2Vec on a selection of downstream tasks, namely next-song recommendation, false neighbor rejection, and artist and genre clustering. We present new optimization objectives and metrics to monitor the effects of hyper-parameter optimization. We show that single-objective optimization can cause side effects on the non optimized metrics and propose a simple multi-objective optimization to mitigate these effects.\nWe find that next-song recommendation quality of Word2Vec is anti-correlated with song popularity, and we show how song embedding optimization can balance performance across different popularity levels.\nWe then show potential positive downstream effects on the task of play prediction.\nFinally, we provide useful insights on the effects of training dataset scale by testing hyper-parameter optimization on an industry-scale dataset.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=14JKPPbprtzNGzgWYc6WZyF0KVfnLyrid)</b>",
      "authors": [
        "Quadrana, Massimo*",
        " Larreche-Mouly, Antoine",
        " Mauch, Matthias"
      ],
      "authors_and_affil": [
        "Massimo Quadrana (Apple)*",
        " Antoine Larreche-Mouly (Apple)",
        " Matthias Mauch (Apple)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04D92HMC80",
      "day": "2",
      "keywords": [
        " Human-centered MIR -> personalization",
        "Applications -> music recommendation and playlist generation",
        "Applications -> music retrieval systems",
        " Evaluation, datasets, and reproducibility -> evaluation methodology",
        " Evaluation, datasets, and reproducibility -> evaluation metrics",
        " Human-centered MIR -> user behavior analysis and mining"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Virtually",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000052.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1EsCrX2A77sjsveov_RMPYHBWpGplILMh",
      "session": [
        "4"
      ],
      "slack_channel": "p4-06-quadrana",
      "title": "Multi-objective Hyper-parameter Optimization of Behavioral Song Embeddings",
      "video": "https://drive.google.com/uc?export=preview&id=14JKPPbprtzNGzgWYc6WZyF0KVfnLyrid"
    },
    "forum": "240",
    "id": "240",
    "pic_id": "https://drive.google.com/open?id=1g-nyma06cOyMbIZJVsAVUt8jZHwrTZbx",
    "position": "06",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "Computational models of expressive piano performance rely on attributes like tempo, timing, dynamics and pedalling. Despite some promising models for performance assessment and performance rendering, results are limited by the scale, breadth and uniformity of existing datasets. In this paper, we present ATEPP, a dataset that contains 1000 hours of performances of standard piano repertoire by 49 world-renowned pianists, organized and aligned by compositions and movements for comparative studies. Scores in MusicXML format are also available for around half of the tracks. We first evaluate and verify the use of transcribed MIDI for representing expressive performance with a listening evaluation that involves recent transcription models. Then, the process of sourcing and curating the dataset is outlined, including composition entity resolution and a pipeline for audio matching and solo filtering. Finally, we conduct baseline experiments for performer identification and performance rendering on our datasets, demonstrating its potential in generalizing expressive features of individual performing style.",
      "abstract": "Computational models of expressive piano performance rely on attributes like tempo, timing, dynamics and pedalling. Despite some promising models for performance assessment and performance rendering, results are limited by the scale, breadth and uniformity of existing datasets. In this paper, we present ATEPP, a dataset that contains 1000 hours of performances of standard piano repertoire by 49 world-renowned pianists, organized and aligned by compositions and movements for comparative studies. Scores in MusicXML format are also available for around half of the tracks. We first evaluate and verify the use of transcribed MIDI for representing expressive performance with a listening evaluation that involves recent transcription models. Then, the process of sourcing and curating the dataset is outlined, including composition entity resolution and a pipeline for audio matching and solo filtering. Finally, we conduct baseline experiments for performer identification and performance rendering on our datasets, demonstrating its potential in generalizing expressive features of individual performing style.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1jtKPOklu0VSGfQkXaO-yKqbHxI2ixNx2)</b>",
      "authors": [
        "Zhang, Huan*",
        " Tang, Jingjing",
        " Rafee, Syed RM",
        " Dixon, Simon",
        " Fazekas, George",
        " Wiggins, Geraint A."
      ],
      "authors_and_affil": [
        "Huan Zhang (Queen Mary University of London)*",
        " Jingjing Tang (Queen Mary University of London)",
        " Syed RM Rafee (Queen Mary University of London)",
        " Simon Dixon (Queen Mary University of London)",
        " George Fazekas (Queen Mary University of London)",
        " Geraint A. Wiggins (Vrije Universiteit Brussel)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CCP21PDL",
      "day": "2",
      "keywords": [
        " MIR fundamentals and methodology -> symbolic music processing",
        " MIR fundamentals and methodology -> metadata, tags, linked data, and semantic web",
        " Musical features and properties -> expression and performative aspects of music",
        "Applications -> performance, and production",
        "Evaluation, datasets, and reproducibility -> novel datasets and use cases",
        " Domain knowledge -> machine learning/artificial intelligence for music"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Bengaluru",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000053.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1oFKridMHFoDZDUAY-0XbxtDUReuLOdOa",
      "session": [
        "4"
      ],
      "slack_channel": "p4-07-zhang",
      "title": "ATEPP: A Dataset of Automatically Transcribed Expressive Piano Performance",
      "video": "https://drive.google.com/uc?export=preview&id=1jtKPOklu0VSGfQkXaO-yKqbHxI2ixNx2"
    },
    "forum": "70",
    "id": "70",
    "pic_id": "https://drive.google.com/open?id=1ClnMf16jVdvey5wwSAMy2ezVOQHJfZDP",
    "position": "07",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "Automatic lyrics transcription (ALT), which can be regarded as automatic speech recognition (ASR) on singing voice, is an interesting and practical topic in academia and industry. ALT has not been well developed mainly due to the dearth of the paired singing voice and lyrics datasets for model training. Considering that there is a large amount of ASR training data, a straightforward method is to leverage ASR data to enhance ALT training. However, the improvement is marginal when training the ALT system directly with ASR data, because of the gap between the singing voice and standard speech data which is rooted in music-specific acoustic characteristics in singing voice. In this paper, we propose PDAugment, a data augmentation method that adjusts pitch and duration of speech at syllable level under the guidance of music scores to help ALT training. Specifically, we adjust the pitch and duration of each syllable in natural speech to those of the corresponding note extracted from music scores, to narrow the gap between natural speech and singing voice. Experiments on DSing30 and Dali corpus show that the ALT system equipped with our PDAugment outperforms previous state-of-the-art systems by 5.9% and 18.1% WERs respectively, demonstrating the effectiveness of PDAugment for ALT.",
      "abstract": "Automatic lyrics transcription (ALT), which can be regarded as automatic speech recognition (ASR) on singing voice, is an interesting and practical topic in academia and industry. ALT has not been well developed mainly due to the dearth of the paired singing voice and lyrics datasets for model training. Considering that there is a large amount of ASR training data, a straightforward method is to leverage ASR data to enhance ALT training. However, the improvement is marginal when training the ALT system directly with ASR data, because of the gap between the singing voice and standard speech data which is rooted in music-specific acoustic characteristics in singing voice. In this paper, we propose PDAugment, a data augmentation method that adjusts pitch and duration of speech at syllable level under the guidance of music scores to help ALT training. Specifically, we adjust the pitch and duration of each syllable in natural speech to those of the corresponding note extracted from music scores, to narrow the gap between natural speech and singing voice. Experiments on DSing30 and Dali corpus show that the ALT system equipped with our PDAugment outperforms previous state-of-the-art systems by 5.9% and 18.1% WERs respectively, demonstrating the effectiveness of PDAugment for ALT.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1w1zfWAEhYI4oJtVEmwWf-TxZ_Qd5wVMz)</b>",
      "authors": [
        "Zhang, Chen",
        " Yu, Jiaxing",
        " Chang, LuChin",
        " Tan, Xu",
        " Chen, Jiawei",
        " Qin, Tao",
        " Zhang, Kejun*"
      ],
      "authors_and_affil": [
        "Chen Zhang (Zhejiang University)",
        " Jiaxing Yu (Zhejiang University)",
        " LuChin Chang (Zhejiang University )",
        " Xu Tan (Microsoft Research Asia)",
        " Jiawei Chen (South China University of Technology)",
        " Tao Qin (Microsoft Research Asia)",
        " Kejun Zhang (Zhejiang University)*"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04D92H794G",
      "day": "2",
      "keywords": [
        "MIR fundamentals and methodology -> web mining, and natural language processing",
        "MIR fundamentals and methodology -> lyrics and other textual data"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Virtually",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000054.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1YIm3VNvWdeZ5Ljjj69gAD0OeLf59VQJo",
      "session": [
        "4"
      ],
      "slack_channel": "p4-08-zhang",
      "title": "PDAugment: Data Augmentation by Pitch and Duration Adjustments for Automatic Lyrics Transcription",
      "video": "https://drive.google.com/uc?export=preview&id=1w1zfWAEhYI4oJtVEmwWf-TxZ_Qd5wVMz"
    },
    "forum": "180",
    "id": "180",
    "pic_id": "https://drive.google.com/open?id=1WtUm8foit1SlCh8cf41aqw6iXoPMs3N6",
    "position": "08",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "Standard evaluation metrics such as the Inception score and Fr\u00e9chet Audio Distance provide a general audio quality distance metric between the synthesized audio and reference clean audio. However, the sensitivity of these metrics to variations in the statistical parameters that define an audio texture is not well studied. In this work, we provide a systematic study of the sensitivity of some of the existing audio quality evaluation metrics to parameter variations in audio textures. Furthermore, we also study three more potentially parameter-sensitive metrics for audio texture synthesis, (a) a Gram matrix based distance, (b) an Accumulated Gram metric using a summarized version of the Gram matrices, and (c) a cochlear-model based statistical features metric. These metrics use deep features that summarize the statistics of any given audio texture, thus being inherently sensitive to variations in the statistical parameters that define an audio texture. We study and evaluate the sensitivity of existing standard metrics as well as Gram matrix and cochlear-model based metrics in response to control-parameter variations for audio textures across a wide range of texture and parameter types, and validate with subjective evaluation. We find that each of the metrics is sensitive to different sets of texture-parameter types. This is the first step towards investigating objective metrics for assessing parameter sensitivity in audio textures.",
      "abstract": "Standard evaluation metrics such as the Inception score and Fr\u00e9chet Audio Distance provide a general audio quality distance metric between the synthesized audio and reference clean audio. However, the sensitivity of these metrics to variations in the statistical parameters that define an audio texture is not well studied. In this work, we provide a systematic study of the sensitivity of some of the existing audio quality evaluation metrics to parameter variations in audio textures. Furthermore, we also study three more potentially parameter-sensitive metrics for audio texture synthesis, (a) a Gram matrix based distance, (b) an Accumulated Gram metric using a summarized version of the Gram matrices, and (c) a cochlear-model based statistical features metric. These metrics use deep features that summarize the statistics of any given audio texture, thus being inherently sensitive to variations in the statistical parameters that define an audio texture. We study and evaluate the sensitivity of existing standard metrics as well as Gram matrix and cochlear-model based metrics in response to control-parameter variations for audio textures across a wide range of texture and parameter types, and validate with subjective evaluation. We find that each of the metrics is sensitive to different sets of texture-parameter types. This is the first step towards investigating objective metrics for assessing parameter sensitivity in audio textures.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1AYr3JCRtjSpidj6Yuk76Lsnf8VRPVO0l)</b>",
      "authors": [
        "Gupta, Chitralekha*",
        " Wei, Yize",
        " Gong, Zequn",
        " Kamath, Purnima",
        " Li, Zhuoyao",
        " Wyse, Lonce"
      ],
      "authors_and_affil": [
        "Chitralekha Gupta (National University of Singapore)*",
        " Yize Wei (National University of Singapore)",
        " Zequn Gong (National University of Singapore)",
        " Purnima Kamath (National University of Singapore)",
        " Zhuoyao Li (National University of Singapore)",
        " Lonce Wyse (National University of Singapore)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CY0MDC8H",
      "day": "2",
      "keywords": [
        " MIR tasks -> music synthesis and transformation",
        " Evaluation, datasets, and reproducibility -> novel datasets and use cases",
        "Evaluation, datasets, and reproducibility -> evaluation metrics",
        "Evaluation, datasets, and reproducibility -> evaluation methodology"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Bengaluru",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000055.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1ehr0qU4earuZdO-FcDTswOleeAB4yK-x",
      "session": [
        "4"
      ],
      "slack_channel": "p4-09-gupta",
      "title": "Parameter Sensitivity of Deep-Feature based Evaluation Metrics for Audio Textures",
      "video": "https://drive.google.com/uc?export=preview&id=1AYr3JCRtjSpidj6Yuk76Lsnf8VRPVO0l"
    },
    "forum": "107",
    "id": "107",
    "pic_id": "https://drive.google.com/open?id=1mzUe0z4BUL0n6f8aHTP-MU0TyQkdd8VA",
    "position": "09",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "Multi-modal music classification creates supervised models trained on features from different sources (modalities): the audio signal, the score, lyrics, album covers, expert tags, etc. A concept of \u201cmulti-group feature importance\u201d not only helps to measure the individual relevance of features of a feature type under investigation (such as the instruments present in a piece), but also serves to quantify the potential for further improving classification by adding features from other feature types or extracted from different kinds of sources, based on a multi-objective analysis of feature sets after evolutionary feature selection. In this study, we investigate the stability of feature group importance when different classification methods and different measures of classification quality are applied. Since musical scores are particularly helpful in deriving semantically meaningful, robust genre characteristics, we focus on the feature groups analyzed by the jSymbolic feature extraction software, which describe properties associated with instrumentation, basic pitch statistics, melody, chords, tempo, and other rhythmic aspects. These symbolic features are analyzed in the context of musical information drawn from five other modalities, and experiments are conducted involving two datasets, one small and one large. The results show that, although some feature groups can remain similarly important compared to others, differences can also be evident in various applications, and can depend on the particular classifier and evaluation measure being used. Insights drawn from this type of analysis can potentially be helpful in effectively matching specific features or feature groups to particular classifiers and evaluation measures in future feature-based MIR research.",
      "abstract": "Multi-modal music classification creates supervised models trained on features from different sources (modalities): the audio signal, the score, lyrics, album covers, expert tags, etc. A concept of \u201cmulti-group feature importance\u201d not only helps to measure the individual relevance of features of a feature type under investigation (such as the instruments present in a piece), but also serves to quantify the potential for further improving classification by adding features from other feature types or extracted from different kinds of sources, based on a multi-objective analysis of feature sets after evolutionary feature selection. In this study, we investigate the stability of feature group importance when different classification methods and different measures of classification quality are applied. Since musical scores are particularly helpful in deriving semantically meaningful, robust genre characteristics, we focus on the feature groups analyzed by the jSymbolic feature extraction software, which describe properties associated with instrumentation, basic pitch statistics, melody, chords, tempo, and other rhythmic aspects. These symbolic features are analyzed in the context of musical information drawn from five other modalities, and experiments are conducted involving two datasets, one small and one large. The results show that, although some feature groups can remain similarly important compared to others, differences can also be evident in various applications, and can depend on the particular classifier and evaluation measure being used. Insights drawn from this type of analysis can potentially be helpful in effectively matching specific features or feature groups to particular classifiers and evaluation measures in future feature-based MIR research.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1314qVAvj9ehZlgm29k6YJxI8IXqaM4WY)</b>",
      "authors": [
        "Vatolkin, Igor*",
        " McKay, Cory"
      ],
      "authors_and_affil": [
        "Igor Vatolkin (Department of Computer Science, TU Dortmund University)*",
        " Cory McKay (Department of Liberal and Creative Arts, Marianopolis College)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CKB9UD7C",
      "day": "2",
      "keywords": [
        " MIR fundamentals and methodology -> symbolic music processing",
        "MIR fundamentals and methodology -> music signal processing",
        " Musical features and properties -> timbre, instrumentation, and singing voice",
        "MIR fundamentals and methodology -> multimodality",
        " Musical features and properties -> harmony, chords and tonality",
        " MIR tasks -> automatic classification"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Virtually",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000056.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1kAQDz6jpiOgdMTHXW6RqKIc9QJpora1n",
      "session": [
        "4"
      ],
      "slack_channel": "p4-10-vatolkin",
      "title": "Stability of Symbolic Feature Group Importance in the Context of Multi-Modal Music Classification",
      "video": "https://drive.google.com/uc?export=preview&id=1314qVAvj9ehZlgm29k6YJxI8IXqaM4WY"
    },
    "forum": "256",
    "id": "256",
    "pic_id": "https://drive.google.com/open?id=148fVwwzWbAoqYcnP70H2tsNiCnHAgekv",
    "position": "10",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "The performance of machine learning (ML) models is known to be affected by discrepancies between training (source) and real-world (target) data distributions. This problem is referred to as domain shift and is commonly approached using domain adaptation (DA) methods. As one relevant scenario, automatic piano transcription algorithms in music learning applications potentially suffer from domain shift since pianos are recorded in different acoustic conditions using various devices. Yet, most currently available datasets for piano transcription only cover ideal recording situations with high-quality microphones.\n Consequently, a transcription model trained on these datasets will face a mismatch between source and target data in real-world scenarios.\n To address this issue, we employ a recently proposed dataset which includes annotated piano recordings covering typical real-life recording settings for a piano learning application on mobile devices.\n We first quantify the influence of the domain shift on the performance of a deep learning-based piano multi-pitch estimation (MPE) algorithm.\n Then, we employ and evaluate four unsupervised DA methods to reduce domain shift.\n Our results show that the studied MPE model is surprisingly robust to domain shift in microphone mismatch scenarios and the DA methods do not notably improve the transcription performance.",
      "abstract": "The performance of machine learning (ML) models is known to be affected by discrepancies between training (source) and real-world (target) data distributions. This problem is referred to as domain shift and is commonly approached using domain adaptation (DA) methods. As one relevant scenario, automatic piano transcription algorithms in music learning applications potentially suffer from domain shift since pianos are recorded in different acoustic conditions using various devices. Yet, most currently available datasets for piano transcription only cover ideal recording situations with high-quality microphones.\n Consequently, a transcription model trained on these datasets will face a mismatch between source and target data in real-world scenarios.\n To address this issue, we employ a recently proposed dataset which includes annotated piano recordings covering typical real-life recording settings for a piano learning application on mobile devices.\n We first quantify the influence of the domain shift on the performance of a deep learning-based piano multi-pitch estimation (MPE) algorithm.\n Then, we employ and evaluate four unsupervised DA methods to reduce domain shift.\n Our results show that the studied MPE model is surprisingly robust to domain shift in microphone mismatch scenarios and the DA methods do not notably improve the transcription performance.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/file/d/1fx80IzmtDUmcRxWpdmd-Oj8U_ThZstyN)</b>",
      "authors": [
        "Bittner, Franca*",
        " Gonzalez, Marcel",
        " Richter, Maike L",
        " Lukashevich, Hanna",
        " Abe\u00dfer, Jakob"
      ],
      "authors_and_affil": [
        "Franca Bittner (Semantic Music Technologies Group, Fraunhofer IDMT, Ilmenau, Germany)*",
        " Marcel Gonzalez (Semantic Music Technologies Group, Fraunhofer IDMT, Ilmenau, Germany)",
        " Maike L Richter (Semantic Music Technologies Group, Fraunhofer IDMT, Ilmenau, Germany)",
        " Hanna Lukashevich (Semantic Music Technologies Group, Fraunhofer IDMT, Ilmenau, Germany)",
        " Jakob Abe\u00dfer (Semantic Music Technologies Group, Fraunhofer IDMT, Ilmenau, Germany)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04D92GS2G0",
      "day": "2",
      "keywords": [
        "Domain knowledge -> machine learning/artificial intelligence for music",
        " Evaluation, datasets, and reproducibility -> MIR tasks",
        " Evaluation, datasets, and reproducibility -> novel datasets and use cases",
        " MIR tasks -> music transcription and annotation",
        " MIR fundamentals and methodology -> music signal processing",
        "Applications -> music training and education"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Bengaluru",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000057.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1xnO0ud5zxHf6ZBYlMxvzoBtQ7vE02LuN/view?usp=share_link",
      "session": [
        "4"
      ],
      "slack_channel": "p4-11-bittner",
      "title": "Multi-pitch Estimation meets Microphone Mismatch: Applicability of Domain Adaptation",
      "video": "https://drive.google.com/file/d/1fx80IzmtDUmcRxWpdmd-Oj8U_ThZstyN"
    },
    "forum": "138",
    "id": "138",
    "pic_id": "https://drive.google.com/file/d/13wnMsMPB2DJ7cwYpi2jeAzSWAF135vg_/view?usp=share_link",
    "position": "11",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "Despite the central role that melody plays in music perception, it remains an open challenge in MIR to reliably detect the notes of the melody present in an arbitrary music recording. A key challenge in *melody transcription* is building methods which can handle broad audio containing any number of instrument ensembles and musical styles---existing strategies work well for some melody instruments or styles but not all. To confront this challenge, we leverage representations from Jukebox (Dhariwal et al. 2020), a generative model of broad music audio, thereby improving performance on melody transcription by 20% relative to conventional spectrogram features. Another obstacle in melody transcription is a lack of training data---we derive a new dataset containing 50 hours of melody transcriptions from crowdsourced annotations of broad music. The combination of generative pre-training and a new dataset for this task results in 77% stronger performance on melody transcription relative to the strongest available baseline. By pairing our new melody transcription approach with solutions for beat detection, key estimation, and chord recognition, we build a system capable of transcribing human-readable lead sheets directly from music audio.",
      "abstract": "Despite the central role that melody plays in music perception, it remains an open challenge in MIR to reliably detect the notes of the melody present in an arbitrary music recording. A key challenge in *melody transcription* is building methods which can handle broad audio containing any number of instrument ensembles and musical styles---existing strategies work well for some melody instruments or styles but not all. To confront this challenge, we leverage representations from Jukebox (Dhariwal et al. 2020), a generative model of broad music audio, thereby improving performance on melody transcription by 20% relative to conventional spectrogram features. Another obstacle in melody transcription is a lack of training data---we derive a new dataset containing 50 hours of melody transcriptions from crowdsourced annotations of broad music. The combination of generative pre-training and a new dataset for this task results in 77% stronger performance on melody transcription relative to the strongest available baseline. By pairing our new melody transcription approach with solutions for beat detection, key estimation, and chord recognition, we build a system capable of transcribing human-readable lead sheets directly from music audio.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1VRQPvGYsS5NxvdY-iUQXwhvAh8AQnaCh)</b>",
      "authors": [
        "Donahue, Chris*",
        " Thickstun, John",
        " Liang, Percy"
      ],
      "authors_and_affil": [
        "Chris Donahue (Stanford University)*",
        " John Thickstun (Stanford University)",
        " Percy Liang (Stanford University)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CCP4146A",
      "day": "2",
      "keywords": [
        "MIR tasks -> music transcription and annotation",
        "Domain knowledge -> machine learning/artificial intelligence for music",
        " Evaluation, datasets, and reproducibility -> MIR tasks",
        " Evaluation, datasets, and reproducibility -> novel datasets and use cases",
        " Domain knowledge -> representations of music"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Virtually",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000058.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1IpcYQkxfDpC45iZHAkdzBFczyTVzNKWr",
      "session": [
        "4"
      ],
      "slack_channel": "p4-12-donahue",
      "title": "Melody transcription via generative pre-training",
      "video": "https://drive.google.com/uc?export=preview&id=1VRQPvGYsS5NxvdY-iUQXwhvAh8AQnaCh"
    },
    "forum": "300",
    "id": "300",
    "pic_id": "https://drive.google.com/open?id=1JHATdmEjh9NaN2r2NZsa1UhP9qEP_pfA",
    "position": "12",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "Music source separation (MSS) aims at decomposing a music recording into constituent sources, such as a lead instrument and the accompaniment. Despite the difficulties in MSS due to the high correlation of musical sources in time and frequency, deep neural networks (DNNs) have led to substantial improvements to accomplish this task. For training supervised machine learning models such as DNNs, isolated sources are required. In the case of popular music, one can exploit open-source datasets which involve multitrack recordings of vocals, bass, and drums. For western classical music, however, isolated sources are generally not available. In this article, we consider the case of piano concertos, which are composed for a pianist typically accompanied by an orchestra. The lack of multitrack recordings makes training supervised machine learning models for the separation of piano and orchestra challenging. To overcome this problem, we generate artificial training material by randomly mixing sections of the solo piano repertoire (e.g., piano sonatas) and orchestral pieces without piano (e.g., symphonies) to train state-of-the-art DNN models for MSS. As our main contribution, we propose a test-time adaptation (TTA) procedure, which exploits random mixtures of the piano-only and orchestra-only parts in the test data to further improve the separation quality.",
      "abstract": "Music source separation (MSS) aims at decomposing a music recording into constituent sources, such as a lead instrument and the accompaniment. Despite the difficulties in MSS due to the high correlation of musical sources in time and frequency, deep neural networks (DNNs) have led to substantial improvements to accomplish this task. For training supervised machine learning models such as DNNs, isolated sources are required. In the case of popular music, one can exploit open-source datasets which involve multitrack recordings of vocals, bass, and drums. For western classical music, however, isolated sources are generally not available. In this article, we consider the case of piano concertos, which are composed for a pianist typically accompanied by an orchestra. The lack of multitrack recordings makes training supervised machine learning models for the separation of piano and orchestra challenging. To overcome this problem, we generate artificial training material by randomly mixing sections of the solo piano repertoire (e.g., piano sonatas) and orchestral pieces without piano (e.g., symphonies) to train state-of-the-art DNN models for MSS. As our main contribution, we propose a test-time adaptation (TTA) procedure, which exploits random mixtures of the piano-only and orchestra-only parts in the test data to further improve the separation quality.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1RteG0EhWOO2uvE7i9jAq0xc2IqpzmGxb)</b>",
      "authors": [
        "\u00d6zer, Yigitcan*",
        " M\u00fcller, Meinard"
      ],
      "authors_and_affil": [
        "Yigitcan \u00d6zer (International Audio Laboratories Erlangen, Germany)*",
        " Meinard M\u00fcller (International Audio Laboratories Erlangen, Germany)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CMQVHBRA",
      "day": "2",
      "keywords": [
        "MIR tasks -> sound source separation",
        "Applications -> performance, and production",
        " Musical features and properties -> structure, segmentation, and form",
        " Evaluation, datasets, and reproducibility -> evaluation metrics",
        " Domain knowledge -> machine learning/artificial intelligence for music"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Bengaluru",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000059.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1WTzoSalucfsyY7tiqXtR9I64Kh97fJLS",
      "session": [
        "4"
      ],
      "slack_channel": "p4-13-\u00f6zer",
      "title": "Source Separation of Piano Concertos with Test-Time Adaptation",
      "video": "https://drive.google.com/uc?export=preview&id=1RteG0EhWOO2uvE7i9jAq0xc2IqpzmGxb"
    },
    "forum": "187",
    "id": "187",
    "pic_id": "https://drive.google.com/open?id=13XEZ01uTPcqKzb91k4L35nI-alK_YEa-",
    "position": "13",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "This paper discusses part of a larger project to preserve and increase access to Guatemalan music sources written in mensural notation by using a digitization and music information retrieval (MIR) workflow to obtain both digital images and symbolic scores with editorial corrections. The workflow involves MIR tools such as optical music recognition (OMR), automatic voice alignment for mensural notation, editorial correction software, and computational counterpoint error detection. \n In this paper, we evaluate whether the use of automatic counterpoint error-detection tools makes the correction process more efficient. The results confirm that marking illegal dissonances in the score following the rules of Renaissance counterpoint indeed makes the process of editorial correction of scribal errors in Renaissance music more efficient by reducing the time taken and improving the accuracy of such corrections. Moreover, marking the illegal dissonances in the score also allowed us to catch OMR errors that had passed through undetected at a previous stage of the workflow.",
      "abstract": "This paper discusses part of a larger project to preserve and increase access to Guatemalan music sources written in mensural notation by using a digitization and music information retrieval (MIR) workflow to obtain both digital images and symbolic scores with editorial corrections. The workflow involves MIR tools such as optical music recognition (OMR), automatic voice alignment for mensural notation, editorial correction software, and computational counterpoint error detection. \n In this paper, we evaluate whether the use of automatic counterpoint error-detection tools makes the correction process more efficient. The results confirm that marking illegal dissonances in the score following the rules of Renaissance counterpoint indeed makes the process of editorial correction of scribal errors in Renaissance music more efficient by reducing the time taken and improving the accuracy of such corrections. Moreover, marking the illegal dissonances in the score also allowed us to catch OMR errors that had passed through undetected at a previous stage of the workflow.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1WNvEy2FaYUJDdskqJDekvn9HB7LojNIm)</b>",
      "authors": [
        "Thomae Elias, Martha E*",
        " Cumming, Julie",
        " Fujinaga, Ichiro"
      ],
      "authors_and_affil": [
        "Martha E Thomae (Schulich School of Music, McGill University)*",
        " Julie E Cumming (Schulich School of Music, McGill University)",
        " Ichiro Fujinaga (Schulich School of Music, McGill University)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04D92FSND6",
      "day": "2",
      "keywords": [
        " MIR fundamentals and methodology -> symbolic music processing",
        "Applications -> digital libraries and archives",
        "Domain knowledge -> computational music theory and musicology",
        " MIR tasks -> optical music recognition",
        " Applications -> music heritage and sustainability"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Virtually",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000060.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1vCGz2bH2Wg_j6FkY_DG3oXu1IcsVYCve",
      "session": [
        "4"
      ],
      "slack_channel": "p4-14-thomae-elias",
      "title": "Counterpoint Error-Detection Tools for Optical Music Recognition of Renaissance Polyphonic Music",
      "video": "https://drive.google.com/uc?export=preview&id=1WNvEy2FaYUJDdskqJDekvn9HB7LojNIm"
    },
    "forum": "44",
    "id": "44",
    "pic_id": "https://drive.google.com/open?id=1jnrQaG3C-tBPCi2wRXw8xt3Hd8X_gZ0g",
    "position": "14",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "Musical scores are generally analyzed under different aspects, notably melody, harmony, rhythm, but also through their texture, although this last concept is arguably more delicate to formalize. Symbolic texture depicts how sounding components are organized in the score. It outlines the density of elements, their heterogeneity, role and interactions. In this paper, we release a set of manual annotations for each bar of 9 movements among early piano sonatas by W. A. Mozart, totaling 1164 labels that follow a syntax dedicated to piano score texture. A quantitative analysis of the annotations highlights some characteristic textural features in the corpus. In addition, we present and release the implementation of low-level descriptors of symbolic texture. These descriptors can be correlated with texture annotations and used in different machine-learning tasks. Along with provided data, they offer promising applications in computer assisted music analysis and composition.",
      "abstract": "Musical scores are generally analyzed under different aspects, notably melody, harmony, rhythm, but also through their texture, although this last concept is arguably more delicate to formalize. Symbolic texture depicts how sounding components are organized in the score. It outlines the density of elements, their heterogeneity, role and interactions. In this paper, we release a set of manual annotations for each bar of 9 movements among early piano sonatas by W. A. Mozart, totaling 1164 labels that follow a syntax dedicated to piano score texture. A quantitative analysis of the annotations highlights some characteristic textural features in the corpus. In addition, we present and release the implementation of low-level descriptors of symbolic texture. These descriptors can be correlated with texture annotations and used in different machine-learning tasks. Along with provided data, they offer promising applications in computer assisted music analysis and composition.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1UNbASOzT42sxRq9xvbMgpWhf6Ai8p0p9)</b>",
      "authors": [
        "Couturier, Louis*",
        " Bigo, Louis",
        " Leve, Florence"
      ],
      "authors_and_affil": [
        "Louis Couturier (MIS, Universit\u00e9 de Picardie Jules Verne, Amiens, France)*",
        " Louis Bigo (CRIStAL, UMR 9189 CNRS, Universit\u00e9 de Lille, France)",
        " Florence Leve (MIS, Universit\u00e9 de Picardie Jules Verne, Amiens, France, CRIStAL, UMR 9189 CNRS, Universit\u00e9 de Lille, France)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CGCVHLUD",
      "day": "2",
      "keywords": [
        " MIR fundamentals and methodology -> symbolic music processing",
        " Musical features and properties",
        " MIR tasks -> music transcription and annotation",
        "Domain knowledge -> computational music theory and musicology",
        "Evaluation, datasets, and reproducibility -> novel datasets and use cases"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Bengaluru",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000061.pdf",
      "poster_pdf": "https://drive.google.com/open?id=189X9J0QP8Gz6YUrBrankyss41klPHwW6",
      "session": [
        "4"
      ],
      "slack_channel": "p4-15-couturier",
      "title": "A Dataset of Symbolic Texture Annotations in Mozart Piano Sonatas",
      "video": "https://drive.google.com/uc?export=preview&id=1UNbASOzT42sxRq9xvbMgpWhf6Ai8p0p9"
    },
    "forum": "236",
    "id": "236",
    "pic_id": "https://drive.google.com/open?id=10t3C2aus761kdF1ulIte-A9L8wJSTs8m",
    "position": "15",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "Violin performance analysis requires accurate and robust f0 estimates to give feedback on the playing accuracy. Despite the recent advancements in data-driven f0 estimators, their application to performance analysis remains a challenge due to style-specific and dataset-induced biases. In this paper, we address this problem by introducing Violin Etudes, a 27.8-hours violin performance dataset constructed with domain knowledge in instrument pedagogy and a novel automatic f0-labeling paradigm. Experimental results on unseen datasets show that the CREPE f0 estimator trained on Violin Etudes outperforms the widely-used pre-trained version trained on multiple manually-labeled datasets. Further preliminary findings suggest that (i) existing data-driven f0 estimators may overfit to equal temperament, and (ii) iterative re-labeling regularized by our novel Constrained Harmonic Resynthesis method can simultaneously enhance datasets and f0 estimators. Our dataset curation methodology is easily scalable to other instruments owing to the quantity of pedagogical data online. It also supports a range of MIR research directions thanks to the performance difficulty labels from educational institutions.",
      "abstract": "Violin performance analysis requires accurate and robust f0 estimates to give feedback on the playing accuracy. Despite the recent advancements in data-driven f0 estimators, their application to performance analysis remains a challenge due to style-specific and dataset-induced biases. In this paper, we address this problem by introducing Violin Etudes, a 27.8-hours violin performance dataset constructed with domain knowledge in instrument pedagogy and a novel automatic f0-labeling paradigm. Experimental results on unseen datasets show that the CREPE f0 estimator trained on Violin Etudes outperforms the widely-used pre-trained version trained on multiple manually-labeled datasets. Further preliminary findings suggest that (i) existing data-driven f0 estimators may overfit to equal temperament, and (ii) iterative re-labeling regularized by our novel Constrained Harmonic Resynthesis method can simultaneously enhance datasets and f0 estimators. Our dataset curation methodology is easily scalable to other instruments owing to the quantity of pedagogical data online. It also supports a range of MIR research directions thanks to the performance difficulty labels from educational institutions.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1IPSsENILVK8vxVgU3UIxr7jdaXa4igHO)</b>",
      "authors": [
        "Tamer, Nazif Can*",
        " Ramoneda, Pedro",
        " Serra, Xavier"
      ],
      "authors_and_affil": [
        "Nazif Can Tamer (Music Technology Group, Universitat Pompeu Fabra, Barcelona)*",
        " Pedro Ramoneda (Music Technology Group, Universitat Pompeu Fabra, Barcelona)",
        " Xavier Serra (Music Technology Group, Universitat Pompeu Fabra, Barcelona)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04C4QQSSR5",
      "day": "2",
      "keywords": [
        "Evaluation, datasets, and reproducibility -> novel datasets and use cases",
        " MIR tasks -> music transcription and annotation",
        "Applications -> music training and education",
        " MIR fundamentals and methodology -> music signal processing"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Bengaluru",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000062.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1Pe6eIW16fjgO0ETh3Q9sd_JjwJSE_KN8",
      "session": [
        "4"
      ],
      "slack_channel": "p4-16-tamer",
      "title": "Violin Etudes: A Comprehensive Dataset for f0 Estimation and Performance Analysis",
      "video": "https://drive.google.com/uc?export=preview&id=1IPSsENILVK8vxVgU3UIxr7jdaXa4igHO"
    },
    "forum": "247",
    "id": "247",
    "pic_id": "https://drive.google.com/open?id=1lejNaNGIyLL-wdhandNH3zZsyMd0h_U3",
    "position": "16",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "In this work we present a new approach for the task of predicting fingerings for piano music. While prior neural approaches have often treated this as a sequence tagging problem with independent predictions, we put forward a checklist system, trained via reinforcement learning, that maintains a representation of recent predictions in addition to a hidden state, allowing it to learn soft constraints on output structure. We also demonstrate that by modifying input representations --- which in prior work using neural models have often taken the form of one-hot encodings over individual keys on the piano --- to encode relative position on the keyboard to the prior note instead, we can achieve much better performance. Additionally, we reassess the use of raw per-note labeling precision as an evaluation metric, noting that it does not adequately measure the fluency, i.e. human playability, of a model's output. To this end, we compare methods across several statistics which track the frequency of adjacent finger predictions that while independently reasonable would be physically challenging to perform in sequence, and implement a reinforcement learning strategy to minimize these as part of our training loss. Finally through human expert evaluation, we demonstrate significant gains in performability directly attributable to improvements with respect to these metrics.",
      "abstract": "In this work we present a new approach for the task of predicting fingerings for piano music. While prior neural approaches have often treated this as a sequence tagging problem with independent predictions, we put forward a checklist system, trained via reinforcement learning, that maintains a representation of recent predictions in addition to a hidden state, allowing it to learn soft constraints on output structure. We also demonstrate that by modifying input representations --- which in prior work using neural models have often taken the form of one-hot encodings over individual keys on the piano --- to encode relative position on the keyboard to the prior note instead, we can achieve much better performance. Additionally, we reassess the use of raw per-note labeling precision as an evaluation metric, noting that it does not adequately measure the fluency, i.e. human playability, of a model's output. To this end, we compare methods across several statistics which track the frequency of adjacent finger predictions that while independently reasonable would be physically challenging to perform in sequence, and implement a reinforcement learning strategy to minimize these as part of our training loss. Finally through human expert evaluation, we demonstrate significant gains in performability directly attributable to improvements with respect to these metrics.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1RqjP2h5ag5vKCHu74UP_e47X0oITpCfr)</b>",
      "authors": [
        "Srivatsan, Nikita*",
        " Berg-Kirkpatrick, Taylor"
      ],
      "authors_and_affil": [
        "Nikita Srivatsan (Carnegie Mellon University)*",
        " Taylor Berg-Kirkpatrick (UC San Diego)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CGCVTYER",
      "day": "2",
      "keywords": [
        "Domain knowledge -> machine learning/artificial intelligence for music",
        " MIR fundamentals and methodology -> symbolic music processing",
        "Evaluation, datasets, and reproducibility -> evaluation metrics",
        " Musical features and properties -> expression and performative aspects of music",
        " MIR tasks -> music transcription and annotation"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Bengaluru",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000063.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1Xg6yXeePIMzTGwW9cIYD8OrhnMTleR3o",
      "session": [
        "4"
      ],
      "slack_channel": "p4-17-srivatsan",
      "title": "Checklist Models for Improved Output Fluency in Piano Fingering Prediction",
      "video": "https://drive.google.com/uc?export=preview&id=1RqjP2h5ag5vKCHu74UP_e47X0oITpCfr"
    },
    "forum": "278",
    "id": "278",
    "pic_id": "https://drive.google.com/open?id=1T566PJrcDZKQioNyBGuZGIT0gKPtC0mt",
    "position": "17",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "4"
  },
  {
    "content": {
      "TLDR": "Reading, much like music listening, is an immersive experience that transports readers while taking them on an emotional journey. Listening to complementary music has the potential to amplify the reading experience, especially when the music is stylistically cohesive and emotionally relevant. In this paper, we propose the first fully automatic method to build a dense soundtrack for books, which can play high-quality instrumental music for the entirety of the reading duration. Our work employs a unique text processing and music weaving pipeline that determines the context and emotional composition of scenes in a chapter. This allows our method to identify and play relevant excerpts from the soundtrack of the book's movie adaptation. By relying on the movie composer's craftsmanship, our book soundtracks include expert-made motifs and other scene-specific musical characteristics. We validate the design decisions of our approach through a perceptual study. Our readers note that the book soundtrack greatly enhanced their reading experience, due to high immersiveness granted via uninterrupted and style-consistent music, and a heightened emotional state attained via high precision emotion and scene context recognition.",
      "abstract": "Reading, much like music listening, is an immersive experience that transports readers while taking them on an emotional journey. Listening to complementary music has the potential to amplify the reading experience, especially when the music is stylistically cohesive and emotionally relevant. In this paper, we propose the first fully automatic method to build a dense soundtrack for books, which can play high-quality instrumental music for the entirety of the reading duration. Our work employs a unique text processing and music weaving pipeline that determines the context and emotional composition of scenes in a chapter. This allows our method to identify and play relevant excerpts from the soundtrack of the book's movie adaptation. By relying on the movie composer's craftsmanship, our book soundtracks include expert-made motifs and other scene-specific musical characteristics. We validate the design decisions of our approach through a perceptual study. Our readers note that the book soundtrack greatly enhanced their reading experience, due to high immersiveness granted via uninterrupted and style-consistent music, and a heightened emotional state attained via high precision emotion and scene context recognition.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1YIXPR5kcOdosuEa_NY8O6_nNAfppR4be)</b>",
      "authors": [
        "Shriram, Jaidev*",
        " Tapaswi, Makarand",
        " Alluri, Vinoo"
      ],
      "authors_and_affil": [
        "Jaidev Shriram (International Institute of Information Technology, Hyderabad)*",
        " Makarand Tapaswi (International Institute of Information Technology, Hyderabad)",
        " Vinoo Alluri (International Institute of Information Technology, Hyderabad)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CY0N3SAD",
      "day": "3",
      "keywords": [
        "Applications -> music videos, multimodal music systems",
        "Applications -> music retrieval systems",
        " Musical features and properties -> musical affect, emotion and mood",
        " MIR fundamentals and methodology -> web mining, and natural language processing",
        " MIR fundamentals and methodology -> multimodality",
        " Human-centered MIR -> human-computer interaction"
      ],
      "long_presentation": "TRUE",
      "paper_presentation": "Virtually",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000064.pdf",
      "poster_pdf": "https://drive.google.com/open?id=12NsPyqn-BWfzNDxOujGOSN9u5OmRqfw7",
      "session": [
        "5"
      ],
      "slack_channel": "p5-01-shriram",
      "title": "Sonus Texere! Automated Dense Soundtrack Construction for Books using Movie Adaptations",
      "video": "https://drive.google.com/uc?export=preview&id=1YIXPR5kcOdosuEa_NY8O6_nNAfppR4be"
    },
    "forum": "182",
    "id": "182",
    "pic_id": "https://drive.google.com/open?id=1OkDCpMGhQPCTw6-NJkomhIWc4apI_Nd2",
    "position": "01",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "5"
  },
  {
    "content": {
      "TLDR": "Fast and user-controllable music generation could enable novel ways of composing or performing music. However, state-of-the-art music generation systems require large amounts of data and computational resources for training, and are slow at inference. This makes them impractical for real-time interactive use. In this work, we introduce Musika, a music generation system that can be trained on hundreds of hours of music using a single consumer GPU, and that allows for much faster than real-time generation of music of arbitrary length on a consumer CPU. We achieve this by first learning a compact invertible representation of spectrogram magnitudes and phases with adversarial autoencoders, then training a Generative Adversarial Network (GAN) on this representation for a particular music domain. A latent coordinate system enables generating arbitrarily long sequences of excerpts in parallel, while a global context vector allows the music to remain stylistically coherent through time. We perform quantitative evaluations to assess the quality of the generated samples and showcase options for user control in piano and techno music generation. We release the source code and pretrained autoencoder weights at github.com/marcoppasini/musika, such that a GAN can be trained on a new music domain with a single GPU in a matter of hours.",
      "abstract": "Fast and user-controllable music generation could enable novel ways of composing or performing music. However, state-of-the-art music generation systems require large amounts of data and computational resources for training, and are slow at inference. This makes them impractical for real-time interactive use. In this work, we introduce Musika, a music generation system that can be trained on hundreds of hours of music using a single consumer GPU, and that allows for much faster than real-time generation of music of arbitrary length on a consumer CPU. We achieve this by first learning a compact invertible representation of spectrogram magnitudes and phases with adversarial autoencoders, then training a Generative Adversarial Network (GAN) on this representation for a particular music domain. A latent coordinate system enables generating arbitrarily long sequences of excerpts in parallel, while a global context vector allows the music to remain stylistically coherent through time. We perform quantitative evaluations to assess the quality of the generated samples and showcase options for user control in piano and techno music generation. We release the source code and pretrained autoencoder weights at github.com/marcoppasini/musika, such that a GAN can be trained on a new music domain with a single GPU in a matter of hours.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1QEAvbOv9-M-xqHS2is_LZEmCn-nMRucU)</b>",
      "authors": [
        "Pasini, Marco*",
        " Schl\u00fcter, Jan"
      ],
      "authors_and_affil": [
        "Marco Pasini (Institute of Computational Perception, Johannes Kepler University Linz, Austria)*",
        " Jan Schl\u00fcter (Institute of Computational Perception, Johannes Kepler University Linz, Austria)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CGCU6FQD",
      "day": "3",
      "keywords": [
        "MIR tasks -> music generation",
        " MIR tasks -> music synthesis and transformation",
        " Musical features and properties -> rhythm, beat, tempo",
        " Domain knowledge -> machine learning/artificial intelligence for music",
        " Human-centered MIR -> human-computer interaction",
        "Applications -> music composition"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Bengaluru",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000065.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1zfEG0D_LeA7eaEgXwyjNiyY0XaKGa2OP",
      "session": [
        "5"
      ],
      "slack_channel": "p5-02-pasini",
      "title": "Musika! Fast Infinite Waveform Music Generation",
      "video": "https://drive.google.com/uc?export=preview&id=1QEAvbOv9-M-xqHS2is_LZEmCn-nMRucU"
    },
    "forum": "74",
    "id": "74",
    "pic_id": "https://drive.google.com/open?id=13qHfRkly7cxulZcRxBlxrlHy-s1VgkYE",
    "position": "02",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "5"
  },
  {
    "content": {
      "TLDR": "In this work, we propose a permutation invariant language model, SymphonyNet, as a solution for symbolic symphony music generation. We propose a novel Multi-track Multi-instrument Repeatable (MMR) representation for symphonic music and model the music sequence using a Transformer-based auto-regressive language model with specific 3-D positional embedding. To overcome length overflow when modeling extra-long symphony tokens, we also propose a modified Byte Pair Encoding algorithm (Music BPE) for music tokens and introduce a novel linear transformer decoder architecture as a backbone. Meanwhile, we train the decoder to learn automatic orchestration as a joint task by masking instrument information from the input. We also introduce a large-scale symbolic symphony dataset for the advance of symphony generation research. Empirical results show that the proposed approach can generate coherent, novel, complex and harmonious symphony as a pioneer solution for multi-track multi-instrument symbolic music generation.",
      "abstract": "In this work, we propose a permutation invariant language model, SymphonyNet, as a solution for symbolic symphony music generation. We propose a novel Multi-track Multi-instrument Repeatable (MMR) representation for symphonic music and model the music sequence using a Transformer-based auto-regressive language model with specific 3-D positional embedding. To overcome length overflow when modeling extra-long symphony tokens, we also propose a modified Byte Pair Encoding algorithm (Music BPE) for music tokens and introduce a novel linear transformer decoder architecture as a backbone. Meanwhile, we train the decoder to learn automatic orchestration as a joint task by masking instrument information from the input. We also introduce a large-scale symbolic symphony dataset for the advance of symphony generation research. Empirical results show that the proposed approach can generate coherent, novel, complex and harmonious symphony as a pioneer solution for multi-track multi-instrument symbolic music generation.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1VQXKiPKq3ta7o52v7CpIYK9ezoB2GdSR)</b>",
      "authors": [
        "Liu, Jiafeng",
        " Dong, Yuanliang",
        " Cheng, Zehua",
        " Zhang, Xinran",
        " Li, XiaoBing",
        " Yu, Feng",
        " Sun, Maosong*"
      ],
      "authors_and_affil": [
        "Jiafeng Liu (Department of Music AI and Music Information Technology, Central Conservatory of Music)",
        " Yuanliang Dong (Department of Music AI and Music Information Technology, Central Conservatory of Music)",
        " Zehua Cheng (Department of Computer Science, University of Oxford)",
        " Xinran Zhang (Department of Music AI and Music Information Technology, Central Conservatory of Music)",
        " XiaoBing Li (Department of Music AI and Music Information Technology, Central Conservatory of Music)",
        " Feng Yu (Department of Music AI and Music Information Technology, Central Conservatory of Music)",
        " Maosong Sun (Department of Music AI and Music Information Technology, Central Conservatory of Music, Department of Computer Science and Technology, Tsinghua University)*"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CKB81QV8",
      "day": "3",
      "keywords": [
        " MIR fundamentals and methodology -> symbolic music processing",
        "MIR tasks -> music generation",
        " Domain knowledge -> representations of music",
        " Domain knowledge -> machine learning/artificial intelligence for music",
        " Musical features and properties -> representations of music",
        "Applications -> music composition"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Virtually",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000066.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1RzyAAP0f_Sir7ugDaOc-kCYu2yOchsXk",
      "session": [
        "5"
      ],
      "slack_channel": "p5-03-sun",
      "title": "Symphony Generation with Permutation Invariant Language Model",
      "video": "https://drive.google.com/uc?export=preview&id=1VQXKiPKq3ta7o52v7CpIYK9ezoB2GdSR"
    },
    "forum": "49",
    "id": "49",
    "pic_id": "https://drive.google.com/open?id=1sbutrPvF0UszD1DINTysrrG_riGItu2W",
    "position": "03",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "5"
  },
  {
    "content": {
      "TLDR": "Music tagging and content-based retrieval systems have traditionally been constructed using pre-defined ontologies covering a rigid set of music attributes or text queries. This paper presents MuLan: a first attempt at a new generation of acoustic models that link music audio directly to unconstrained natural language music descriptions. MuLan takes the form of a two-tower, joint audio-text embedding model trained using 44 million music recordings (370K hours) and weakly-associated, free-form text annotations. Through its compatibility with a wide range of music genres and text styles (including conventional music tags), the resulting audio-text representation subsumes existing ontologies while graduating to true zero-shot functionalities. We demonstrate the versatility of the MuLan embeddings with a range of experiments including transfer learning, zero-shot music tagging, language understanding in the music domain, and cross-modal retrieval applications.",
      "abstract": "Music tagging and content-based retrieval systems have traditionally been constructed using pre-defined ontologies covering a rigid set of music attributes or text queries. This paper presents MuLan: a first attempt at a new generation of acoustic models that link music audio directly to unconstrained natural language music descriptions. MuLan takes the form of a two-tower, joint audio-text embedding model trained using 44 million music recordings (370K hours) and weakly-associated, free-form text annotations. Through its compatibility with a wide range of music genres and text styles (including conventional music tags), the resulting audio-text representation subsumes existing ontologies while graduating to true zero-shot functionalities. We demonstrate the versatility of the MuLan embeddings with a range of experiments including transfer learning, zero-shot music tagging, language understanding in the music domain, and cross-modal retrieval applications.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1qL-do2sSYGYWyqtivxrNRELnzLrF9MoB)</b>",
      "authors": [
        "Huang, Qingqing*",
        " Jansen, Aren",
        " Lee, Joonseok",
        " Ganti, Ravi",
        " Li, Judith Yue",
        " Ellis, Daniel P W"
      ],
      "authors_and_affil": [
        "Qingqing Huang (Google Research)*",
        " Aren Jansen (Google Research)",
        " Joonseok Lee (Google Research, Seoul National University)",
        " Ravi Ganti (Google Research)",
        " Judith Yue Li (Google Research)",
        " Dan P W Ellis (Google Research)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CY0MQK0R",
      "day": "3",
      "keywords": [
        " MIR fundamentals and methodology -> web mining, and natural language processing",
        " MIR tasks -> indexing and querying",
        " MIR fundamentals and methodology -> music signal processing",
        "MIR fundamentals and methodology -> multimodality",
        "MIR fundamentals and methodology -> metadata, tags, linked data, and semantic web",
        " MIR tasks -> automatic classification"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Bengaluru",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000067.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1k29DiyZ1qC3TGAVBbv9Gox1bVCdKt30Z",
      "session": [
        "5"
      ],
      "slack_channel": "p5-04-huang",
      "title": "MuLan: A Joint Embedding of Music Audio and Natural Language",
      "video": "https://drive.google.com/uc?export=preview&id=1qL-do2sSYGYWyqtivxrNRELnzLrF9MoB"
    },
    "forum": "150",
    "id": "150",
    "pic_id": "https://drive.google.com/open?id=1eaa1toshAC9rwK91a2oDJuG1O4Fv3jvC",
    "position": "04",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "5"
  },
  {
    "content": {
      "TLDR": "Human usually composes music by organizing elements according to the musical form to express music ideas. However, for neural network-based music generation, it is difficult to do so due to the lack of labelled data on musical form. In this paper, we develop MeloForm, a system that generates melody with musical form using expert systems and neural networks. Specifically, 1) we design an expert system to generate a melody by developing musical elements from motifs to phrases then to sections with repetitions and variations according to pre-given musical form; 2) considering the generated melody is lack of musical richness, we design a Transformer based refinement model to improve the melody without changing its musical form. MeloForm enjoys the advantages of precise musical form control by expert systems and musical richness learning via neural models. Both subjective and objective experimental evaluations demonstrate that MeloForm generates melodies with precise musical form control with 97.79% accuracy, and outperforms baseline systems in terms of subjective evaluation score by 0.75, 0.50, 0.86 and 0.89 in structure, thematic, richness and overall quality, without any labelled musical form data. Besides, MeloForm can support various kinds of forms, such as verse and chorus form, rondo form, variational form, sonata form, etc.",
      "abstract": "Human usually composes music by organizing elements according to the musical form to express music ideas. However, for neural network-based music generation, it is difficult to do so due to the lack of labelled data on musical form. In this paper, we develop MeloForm, a system that generates melody with musical form using expert systems and neural networks. Specifically, 1) we design an expert system to generate a melody by developing musical elements from motifs to phrases then to sections with repetitions and variations according to pre-given musical form; 2) considering the generated melody is lack of musical richness, we design a Transformer based refinement model to improve the melody without changing its musical form. MeloForm enjoys the advantages of precise musical form control by expert systems and musical richness learning via neural models. Both subjective and objective experimental evaluations demonstrate that MeloForm generates melodies with precise musical form control with 97.79% accuracy, and outperforms baseline systems in terms of subjective evaluation score by 0.75, 0.50, 0.86 and 0.89 in structure, thematic, richness and overall quality, without any labelled musical form data. Besides, MeloForm can support various kinds of forms, such as verse and chorus form, rondo form, variational form, sonata form, etc.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1rBjo-iaak7h15gX1VfV7ikwjGX3EJW7Z)</b>",
      "authors": [
        "Lu, Peiling*",
        " Tan, Xu",
        " Yu, Botao",
        " Qin, Tao",
        " Zhao, Sheng",
        " Liu, Tie-Yan"
      ],
      "authors_and_affil": [
        "Peiling Lu (Microsoft Research Asia, Beijing, China)*",
        " Xu Tan (Microsoft Research Asia, Beijing, China)",
        " Botao Yu (Nanjing University, Nanjing, China)",
        " Tao Qin (Microsoft Research Asia, Beijing, China)",
        " Sheng Zhao (Microsoft Azure Speech, Beijing, China)",
        " Tie-Yan Liu (Microsoft Research Asia, Beijing, China)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CGCU222Z",
      "day": "3",
      "keywords": [
        " MIR tasks -> music generation",
        "Domain knowledge -> machine learning/artificial intelligence for music",
        " Musical features and properties -> structure, segmentation, and form",
        "Applications -> music composition"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Virtually",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000068.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1BmC8AZSGL18PUxACuqRa0KW0aA9o5WfI",
      "session": [
        "5"
      ],
      "slack_channel": "p5-05-lu",
      "title": "MeloForm: Generating Melody with Musical Form based on Expert Systems and Neural Networks",
      "video": "https://drive.google.com/uc?export=preview&id=1rBjo-iaak7h15gX1VfV7ikwjGX3EJW7Z"
    },
    "forum": "64",
    "id": "64",
    "pic_id": "https://drive.google.com/open?id=1TRDSd6ecnkeVasup3eVMMeGzI-uxXEND",
    "position": "05",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "5"
  },
  {
    "content": {
      "TLDR": "Nowadays, commercial music has extreme loudness and heavily compressed dynamic range compared to the past. Yet, in music source separation, these characteristics have not been thoroughly considered, resulting in the domain mismatch between the laboratory and the real world. In this paper, we confirmed that this domain mismatch negatively affect the performance of the music source separation networks. To this end, we first created the out-of-domain evaluation datasets, musdb-L and XL, by mimicking the music mastering process.\n Then, we quantitatively verify that the performance of the state-of-the-art algorithms significantly deteriorated in our datasets. Lastly, we proposed LimitAug data augmentation method to reduce the domain mismatch, which utilizes an online limiter during the training data sampling process. We confirmed that it not only alleviates the performance degradation on our out-of-domain datasets, but also results in higher performance on in-domain data.",
      "abstract": "Nowadays, commercial music has extreme loudness and heavily compressed dynamic range compared to the past. Yet, in music source separation, these characteristics have not been thoroughly considered, resulting in the domain mismatch between the laboratory and the real world. In this paper, we confirmed that this domain mismatch negatively affect the performance of the music source separation networks. To this end, we first created the out-of-domain evaluation datasets, musdb-L and XL, by mimicking the music mastering process.\n Then, we quantitatively verify that the performance of the state-of-the-art algorithms significantly deteriorated in our datasets. Lastly, we proposed LimitAug data augmentation method to reduce the domain mismatch, which utilizes an online limiter during the training data sampling process. We confirmed that it not only alleviates the performance degradation on our out-of-domain datasets, but also results in higher performance on in-domain data.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1ilduQ9Hu7bEM3-LyhIPunNf_EsImHi15)</b>",
      "authors": [
        "Jeon, Chang-Bin*",
        " Lee, Kyogu"
      ],
      "authors_and_affil": [
        "Chang-Bin Jeon (Department of Intelligence and Information, Music and Audio Research Group, Seoul National University)*",
        " Kyogu Lee (Department of Intelligence and Information, Music and Audio Research Group, Seoul National University)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CK86ASM9",
      "day": "3",
      "keywords": [
        "Evaluation, datasets, and reproducibility -> MIR tasks",
        " Evaluation, datasets, and reproducibility -> novel datasets and use cases",
        "MIR tasks -> sound source separation"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Bengaluru",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000069.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1BVzyruwuCtVOo0mBayApzFtvGt5oXBV7",
      "session": [
        "5"
      ],
      "slack_channel": "p5-06-jeon",
      "title": "Towards robust music source separation on loud commercial music",
      "video": "https://drive.google.com/uc?export=preview&id=1ilduQ9Hu7bEM3-LyhIPunNf_EsImHi15"
    },
    "forum": "88",
    "id": "88",
    "pic_id": "https://drive.google.com/open?id=127RJsbbf9Mc2VWbGLoWvfewfI4X4KN8w",
    "position": "06",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "5"
  },
  {
    "content": {
      "TLDR": "There are many benefits for a community when there is a vibrant local music scene (e.g., increased mental & physical well-being, increased economic activity) and there are many factors that contribute to an environment in which a live music scene can thrive (e.g., available performance spaces, helpful government policies). In this paper, we explore using an estimate of the live music event rate (LMER) as a rough indicator to measure the strength of a local music scene. We define LMER as the number of music shows per 100,000 people per year and then explore how this indicator is (or is not) correlated with 28 other socioeconomic indicators. To do this, we analyze a set of 308,051 music events from 2019 across 1,139 cities in the United States. Our findings reveal that factors related to transportation (e.g., high walkability), population (high density), economics (high employment rate), age (high proportion of individuals age 20-29), and education (bachelor\u2019s degree or higher) are strongly correlated with having a high number of live music events. Conversely, we did not find statistically significant evidence that other indica- tors (e.g., racial diversity) are correlated.",
      "abstract": "There are many benefits for a community when there is a vibrant local music scene (e.g., increased mental & physical well-being, increased economic activity) and there are many factors that contribute to an environment in which a live music scene can thrive (e.g., available performance spaces, helpful government policies). In this paper, we explore using an estimate of the live music event rate (LMER) as a rough indicator to measure the strength of a local music scene. We define LMER as the number of music shows per 100,000 people per year and then explore how this indicator is (or is not) correlated with 28 other socioeconomic indicators. To do this, we analyze a set of 308,051 music events from 2019 across 1,139 cities in the United States. Our findings reveal that factors related to transportation (e.g., high walkability), population (high density), economics (high employment rate), age (high proportion of individuals age 20-29), and education (bachelor\u2019s degree or higher) are strongly correlated with having a high number of live music events. Conversely, we did not find statistically significant evidence that other indica- tors (e.g., racial diversity) are correlated.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1h0-RY1e8olgt50_HS-BNt25DWVFn059w)</b>",
      "authors": [
        "Zhou, Michael*",
        " McGraw, Andrew",
        " Turnbull, Douglas R"
      ],
      "authors_and_affil": [
        "Michael Zhou (Columbia University)*",
        " Andrew McGraw (University of Richmond)",
        " Douglas R Turnbull (Ithaca College)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CGCUHYSH",
      "day": "3",
      "keywords": [
        "Evaluation, datasets, and reproducibility -> novel datasets and use cases",
        " MIR fundamentals and methodology -> web mining, and natural language processing",
        "Applications -> music and health, well-being and therapy",
        " Philosophical and ethical discussions -> legal and societal aspects of MIR"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Virtually",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000070.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1mqBWv0DB-be4qKV11vRujzJ27MNpU4vV",
      "session": [
        "5"
      ],
      "slack_channel": "p5-07-zhou",
      "title": "Towards Quantifying the Strength of Music Scenes Using Live Event Data",
      "video": "https://drive.google.com/uc?export=preview&id=1h0-RY1e8olgt50_HS-BNt25DWVFn059w"
    },
    "forum": "116",
    "id": "116",
    "pic_id": "https://drive.google.com/open?id=1UHf0cUF8zZQzNB5_sVVBxS9E9E4EQMFb",
    "position": "07",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "5"
  },
  {
    "content": {
      "TLDR": "Recent work in music structure analysis has shown the potential of deep features to highlight the underlying structure of music audio signals. Despite promising results achieved by such representations, dealing with the inherent hierarchical aspect of music structure remains a challenging problem. Because different levels of segmentation can be considered as equally valid, specifically designed representations should be optimized to improve hierarchical structure analysis. In this work, we explore unsupervised learning of such representations using a contrastive approach operating at different time-scales. We evaluate the proposed system on flat and multi-level music segmentation. By leveraging both time and the hierarchical organization of music structure, we show that the obtained deep embeddings can encode meaningful patterns and improve segmentation at various levels of granularity.",
      "abstract": "Recent work in music structure analysis has shown the potential of deep features to highlight the underlying structure of music audio signals. Despite promising results achieved by such representations, dealing with the inherent hierarchical aspect of music structure remains a challenging problem. Because different levels of segmentation can be considered as equally valid, specifically designed representations should be optimized to improve hierarchical structure analysis. In this work, we explore unsupervised learning of such representations using a contrastive approach operating at different time-scales. We evaluate the proposed system on flat and multi-level music segmentation. By leveraging both time and the hierarchical organization of music structure, we show that the obtained deep embeddings can encode meaningful patterns and improve segmentation at various levels of granularity.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1XLUMzf3uoejFSWn5jGnSPpzQ3cHKM5Jm)</b>",
      "authors": [
        "Buisson, Morgan*",
        " McFee, Brian",
        " Essid, Slim",
        " Crayencour, Crayencour, H\u00e9l\u00e8ne C."
      ],
      "authors_and_affil": [
        "Morgan Buisson (LTCI, T\u00e9l\u00e9com Paris, Institut Polytechnique de Paris, France)*",
        " Brian McFee (Music and Audio Research Laboratory, New York University, USA, Center of Data Science, New York University, USA)",
        " Slim Essid (LTCI, T\u00e9l\u00e9com Paris, Institut Polytechnique de Paris, France)",
        " Helene C Crayencour (L2S, CNRS-Univ.Paris-Sud-CentraleSup\u00e9lec, France)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CK87J411",
      "day": "3",
      "keywords": [
        "Domain knowledge -> machine learning/artificial intelligence for music",
        "Musical features and properties -> structure, segmentation, and form",
        " Musical features and properties -> representations of music"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Bengaluru",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000071.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1bAMp-dLazBX41bIuF7RGiZgAR6QSoRku",
      "session": [
        "5"
      ],
      "slack_channel": "p5-08-buisson",
      "title": "Learning Multi-Level Representations for Hierarchical Music Structure Analysis.",
      "video": "https://drive.google.com/uc?export=preview&id=1XLUMzf3uoejFSWn5jGnSPpzQ3cHKM5Jm"
    },
    "forum": "237",
    "id": "237",
    "pic_id": "https://drive.google.com/open?id=1RD0vj9G3DxjdrBAkN_NotQCczqfKn7zA",
    "position": "08",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "5"
  },
  {
    "content": {
      "TLDR": "An ideal music synthesizer should be both interactive and expressive, generating high-fidelity audio in realtime for arbitrary combinations of instruments and notes. Recent neural synthesizers have exhibited a tradeoff between domain-specific models that offer detailed control of only specific instruments, or raw waveform models that can train on any music but with minimal control and slow generation. In this work, we focus on a middle ground of neural synthesizers that can generate audio from MIDI sequences with arbitrary combinations of instruments in realtime. This enables training on a wide range of transcription datasets with a single model, which in turn offers note-level control of composition and instrumentation across a wide range of instruments. We use a simple two-stage process: MIDI to spectrograms with an encoder-decoder Transformer, then spectrograms to audio with a generative adversarial network (GAN) spectrogram inverter. We compare training the decoder as an autoregressive model and as a Denoising Diffusion Probabilistic Model (DDPM) and find that the DDPM approach is superior both qualitatively and as measured by audio reconstruction and Fr\u00e9chet distance metrics. Given the interactivity and generality of this approach, we find this to be a promising first step towards interactive and expressive neural synthesis for arbitrary combinations of instruments and notes.",
      "abstract": "An ideal music synthesizer should be both interactive and expressive, generating high-fidelity audio in realtime for arbitrary combinations of instruments and notes. Recent neural synthesizers have exhibited a tradeoff between domain-specific models that offer detailed control of only specific instruments, or raw waveform models that can train on any music but with minimal control and slow generation. In this work, we focus on a middle ground of neural synthesizers that can generate audio from MIDI sequences with arbitrary combinations of instruments in realtime. This enables training on a wide range of transcription datasets with a single model, which in turn offers note-level control of composition and instrumentation across a wide range of instruments. We use a simple two-stage process: MIDI to spectrograms with an encoder-decoder Transformer, then spectrograms to audio with a generative adversarial network (GAN) spectrogram inverter. We compare training the decoder as an autoregressive model and as a Denoising Diffusion Probabilistic Model (DDPM) and find that the DDPM approach is superior both qualitatively and as measured by audio reconstruction and Fr\u00e9chet distance metrics. Given the interactivity and generality of this approach, we find this to be a promising first step towards interactive and expressive neural synthesis for arbitrary combinations of instruments and notes.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1L0KNm15GdRf-LlBskmQap6iSrS167g0W)</b>",
      "authors": [
        "Hawthorne, Curtis*",
        " Simon, Ian",
        " Roberts, Adam",
        " Zeghidour, Neil",
        " Gardner, Joshua",
        " Manilow, Ethan",
        " Engel, Jesse"
      ],
      "authors_and_affil": [
        "Curtis Hawthorne (Google Research, Brain Team)*",
        " Ian Simon (Google Research, Brain Team)",
        " Adam Roberts (Google Research, Brain Team)",
        " Neil Zeghidour (Google Research, Brain Team)",
        " Joshua Gardner (University of Washington)",
        " Ethan Manilow (Interactive Audio Lab, Northwestern University)",
        " Jesse Engel (Google Research, Brain Team)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CKB9RCUA",
      "day": "3",
      "keywords": [
        "MIR tasks -> music synthesis and transformation",
        " MIR fundamentals and methodology -> symbolic music processing",
        "MIR fundamentals and methodology -> music signal processing"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Virtually",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000072.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1OSgBz7vOidgTnyugE8Up19l-hOKBJ12w",
      "session": [
        "5"
      ],
      "slack_channel": "p5-09-hawthorne",
      "title": "Multi-instrument Music Synthesis with Spectrogram Diffusion",
      "video": "https://drive.google.com/uc?export=preview&id=1L0KNm15GdRf-LlBskmQap6iSrS167g0W"
    },
    "forum": "246",
    "id": "246",
    "pic_id": "https://drive.google.com/open?id=18uC0iLawsMhkR4nGyZXCTI6NiFD_LD6c",
    "position": "09",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "5"
  },
  {
    "content": {
      "TLDR": "FM Synthesis is a well-known algorithm used to generate complex timbre from a compact set of design primitives. Typically featuring a MIDI interface, it is usually impractical to control it from an audio source.\n On the other hand, Differentiable Digital Signal Processing (DDSP) has enabled nuanced audio rendering by Deep Neural Networks (DNNs) that learn to control differentiable synthesis layers from arbitrary sound inputs. The training process involves a corpus of audio for supervision, and spectral reconstruction loss functions. Such functions, while being great to match spectral amplitudes, present a lack of pitch direction which can hinder the joint optimization of the parameters of FM synthesizers. In this paper, we take steps towards enabling continuous control of a well-established FM synthesis architecture from an audio input. Firstly, we discuss a set of design constraints that ease spectral optimization of a differentiable FM synthesizer via a standard reconstruction loss. Next, we present Differentiable DX7 (DDX7), a lightweight architecture for neural FM resynthesis of musical instrument sounds in terms of a compact set of parameters. We train the model on instrument samples extracted from the URMP dataset, and quantitatively demonstrate its comparable audio quality against selected benchmarks.",
      "abstract": "FM Synthesis is a well-known algorithm used to generate complex timbre from a compact set of design primitives. Typically featuring a MIDI interface, it is usually impractical to control it from an audio source.\n On the other hand, Differentiable Digital Signal Processing (DDSP) has enabled nuanced audio rendering by Deep Neural Networks (DNNs) that learn to control differentiable synthesis layers from arbitrary sound inputs. The training process involves a corpus of audio for supervision, and spectral reconstruction loss functions. Such functions, while being great to match spectral amplitudes, present a lack of pitch direction which can hinder the joint optimization of the parameters of FM synthesizers. In this paper, we take steps towards enabling continuous control of a well-established FM synthesis architecture from an audio input. Firstly, we discuss a set of design constraints that ease spectral optimization of a differentiable FM synthesizer via a standard reconstruction loss. Next, we present Differentiable DX7 (DDX7), a lightweight architecture for neural FM resynthesis of musical instrument sounds in terms of a compact set of parameters. We train the model on instrument samples extracted from the URMP dataset, and quantitatively demonstrate its comparable audio quality against selected benchmarks.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1tcuAoQbn79II6omDp02niDcXYi1HGVZv)</b>",
      "authors": [
        "Caspe, Franco*",
        " McPherson, Andrew",
        " Sandler, Mark"
      ],
      "authors_and_affil": [
        "Franco Caspe (Queen Mary University of London)*",
        " Andrew McPherson (Queen Mary University of London)",
        " Mark Sandler (Queen Mary University of London)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CCP3LHNJ",
      "day": "3",
      "keywords": [
        "Applications -> performance, and production",
        "MIR tasks -> music synthesis and transformation",
        " Musical features and properties -> representations of music",
        " Musical features and properties -> timbre, instrumentation, and singing voice"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Bengaluru",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000073.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1CQMt2dW3cGfG7CLbRxTzYkRbLvKG4Rox",
      "session": [
        "5"
      ],
      "slack_channel": "p5-10-caspe",
      "title": "DDX7: Differentiable FM Synthesis of Musical Instrument Sounds",
      "video": "https://drive.google.com/uc?export=preview&id=1tcuAoQbn79II6omDp02niDcXYi1HGVZv"
    },
    "forum": "248",
    "id": "248",
    "pic_id": "https://drive.google.com/open?id=1ttR-uhp_6fldGXAUTlH71VQHFSOlsBhr",
    "position": "10",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "5"
  },
  {
    "content": {
      "TLDR": "Tracking beats of singing voices without the presence of musical accompaniment can find many applications in music production, automatic song arrangement, and social media interaction.\n Its main challenge is the lack of strong rhythmic and harmonic patterns that are important for music rhythmic analysis in general. Even for human listeners, this can be a challenging task. As a result, existing music beat tracking systems fail to deliver satisfactory performance on singing voices. In this paper, we propose singing beat tracking as a novel task, and propose the first approach to solving this task. Our approach leverages semantic information of singing voices by employing pre-trained self-supervised WavLM and DistilHuBERT speech representations as the front-end and uses a self-attention encoder layer to predict beats. To train and test the system, we obtain separated singing voices and their beat annotations using source separation and beat tracking on complete songs, followed by manual corrections. \n Experiments on the 741 separated vocal tracks of the GTZAN dataset show that the proposed system outperforms several state-of-the-art music beat tracking methods by a large margin in terms of beat tracking accuracy. Ablation studies \n also confirm the advantages of pre-trained self-supervised speech representations over generic spectral features.",
      "abstract": "Tracking beats of singing voices without the presence of musical accompaniment can find many applications in music production, automatic song arrangement, and social media interaction.\n Its main challenge is the lack of strong rhythmic and harmonic patterns that are important for music rhythmic analysis in general. Even for human listeners, this can be a challenging task. As a result, existing music beat tracking systems fail to deliver satisfactory performance on singing voices. In this paper, we propose singing beat tracking as a novel task, and propose the first approach to solving this task. Our approach leverages semantic information of singing voices by employing pre-trained self-supervised WavLM and DistilHuBERT speech representations as the front-end and uses a self-attention encoder layer to predict beats. To train and test the system, we obtain separated singing voices and their beat annotations using source separation and beat tracking on complete songs, followed by manual corrections. \n Experiments on the 741 separated vocal tracks of the GTZAN dataset show that the proposed system outperforms several state-of-the-art music beat tracking methods by a large margin in terms of beat tracking accuracy. Ablation studies \n also confirm the advantages of pre-trained self-supervised speech representations over generic spectral features.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1iIMqgx_EjeYlM0uaaSyzJSikF4m5LIfY)</b>",
      "authors": [
        "Heydari, Mojtaba*",
        " Duan, Zhiyao"
      ],
      "authors_and_affil": [
        "Mojtaba Heydari (Department of Electrical and Computer Engineering, University of Rochester, 500 Wilson Blvd, Rochester, NY 14627, USA)*",
        " Zhiyao Duan (Department of Electrical and Computer Engineering, University of Rochester, 500 Wilson Blvd, Rochester, NY 14627, USA)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CCP3M8TY",
      "day": "3",
      "keywords": [
        "Musical features and properties -> rhythm, beat, tempo",
        "Musical features and properties -> timbre, instrumentation, and singing voice"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Virtually",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000074.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1-XUh6JmUAZckJDnV7rV3_5e-E4_CLOLe",
      "session": [
        "5"
      ],
      "slack_channel": "p5-11-heydari",
      "title": "Singing beat tracking with Self-supervised front-end and linear transformers",
      "video": "https://drive.google.com/uc?export=preview&id=1iIMqgx_EjeYlM0uaaSyzJSikF4m5LIfY"
    },
    "forum": "250",
    "id": "250",
    "pic_id": "https://drive.google.com/open?id=1ru9Ksqob5mz_y2I8D37yW1mA1iSHuUBI",
    "position": "11",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "5"
  },
  {
    "content": {
      "TLDR": "Music source separation research has made great advances in recent years, especially towards the problem of separating vocals, drums, and bass stems from mastered songs. The advances in this field can be directly attributed to the availability of large-scale multitrack research datasets for these mentioned stems. Tasks such as separating similar-sounding sources from an ensemble recording have seen limited research due to the lack of sizeable, bleed-free multitrack datasets. In this paper, we introduce a novel multitrack dataset called EnsembleSet generated using the Spitfire BBC Symphony Orchestra library using ensemble scores from RWC Classical Music Database and Mutopia. Our data generation method introduces automated articulation mapping for different playing styles based on the input MIDI/MusicXML data. The sample library also enables us to render the dataset with 20 different mix/microphone configurations allowing us to study various recording scenarios for each performance. The dataset presents 80 tracks (6+ hours) with a range of string, wind, and brass instruments arranged as chamber ensembles. We also present our benchmark on our synthesised dataset using a permutation-invariant time-domain separation model for chamber ensembles which produces generalisable results when tested on real recordings from existing datasets.",
      "abstract": "Music source separation research has made great advances in recent years, especially towards the problem of separating vocals, drums, and bass stems from mastered songs. The advances in this field can be directly attributed to the availability of large-scale multitrack research datasets for these mentioned stems. Tasks such as separating similar-sounding sources from an ensemble recording have seen limited research due to the lack of sizeable, bleed-free multitrack datasets. In this paper, we introduce a novel multitrack dataset called EnsembleSet generated using the Spitfire BBC Symphony Orchestra library using ensemble scores from RWC Classical Music Database and Mutopia. Our data generation method introduces automated articulation mapping for different playing styles based on the input MIDI/MusicXML data. The sample library also enables us to render the dataset with 20 different mix/microphone configurations allowing us to study various recording scenarios for each performance. The dataset presents 80 tracks (6+ hours) with a range of string, wind, and brass instruments arranged as chamber ensembles. We also present our benchmark on our synthesised dataset using a permutation-invariant time-domain separation model for chamber ensembles which produces generalisable results when tested on real recordings from existing datasets.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1SVuTvv-13lWyPxHnICvtH-kVhZNfmGqa)</b>",
      "authors": [
        "Sarkar, Saurjya*",
        " Benetos, Emmanouil",
        " Sandler, Mark"
      ],
      "authors_and_affil": [
        "Saurjya Sarkar (Centre for Digital Music, Queen Mary University of London, UK)*",
        " Emmanouil Benetos (Centre for Digital Music, Queen Mary University of London, UK)",
        " Mark Sandler (Centre for Digital Music, Queen Mary University of London, UK)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CMQW8TDJ",
      "day": "3",
      "keywords": [
        "Domain knowledge -> machine learning/artificial intelligence for music",
        " Evaluation, datasets, and reproducibility -> novel datasets and use cases",
        "MIR tasks -> sound source separation"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Bengaluru",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000075.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1jFALUuzfK3k0S-L5EdAnlNqc5IOM81xW",
      "session": [
        "5"
      ],
      "slack_channel": "p5-12-sarkar",
      "title": "EnsembleSet: a new high quality synthesised dataset for chamber ensemble separation",
      "video": "https://drive.google.com/uc?export=preview&id=1SVuTvv-13lWyPxHnICvtH-kVhZNfmGqa"
    },
    "forum": "277",
    "id": "277",
    "pic_id": "https://drive.google.com/open?id=1WpyTZ1dZG5NpVO46cE-1qL4Cjx7bXrG6",
    "position": "12",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "5"
  },
  {
    "content": {
      "TLDR": "This paper presents an automatic lyrics transcription (ALT) method for music recordings that leverages the framewise semitone-level sung pitches estimated in a multi-task learning framework. Compared to automatic speech recognition (ASR), ALT is challenging due to the insufficiency of training data and the variation and contamination of acoustic features caused by singing expressions and accompaniment sounds. The domain adaptation approach has thus recently been taken for updating an ASR model pre-trained from sufficient speech data. In the naive application of the end-to-end approach to ALT, the internal audio-to-lyrics alignment often fails due to the time-stretching nature of singing features. To stabilize the alignment, we make use of the semi-synchronous relationships between notes and characters. Specifically, a convolutional recurrent neural network (CRNN) is used for estimating the semitone-level pitches with note onset times while eliminating the intra- and inter-note pitch variations. This estimate helps an end-to-end ALT model based on connectionist temporal classification (CTC) learn correct audio-to-character alignment and mapping, where the ALT model is trained jointly with the pitch and onset estimation model. The experimental results show the usefulness of the pitch and onset information in ALT.",
      "abstract": "This paper presents an automatic lyrics transcription (ALT) method for music recordings that leverages the framewise semitone-level sung pitches estimated in a multi-task learning framework. Compared to automatic speech recognition (ASR), ALT is challenging due to the insufficiency of training data and the variation and contamination of acoustic features caused by singing expressions and accompaniment sounds. The domain adaptation approach has thus recently been taken for updating an ASR model pre-trained from sufficient speech data. In the naive application of the end-to-end approach to ALT, the internal audio-to-lyrics alignment often fails due to the time-stretching nature of singing features. To stabilize the alignment, we make use of the semi-synchronous relationships between notes and characters. Specifically, a convolutional recurrent neural network (CRNN) is used for estimating the semitone-level pitches with note onset times while eliminating the intra- and inter-note pitch variations. This estimate helps an end-to-end ALT model based on connectionist temporal classification (CTC) learn correct audio-to-character alignment and mapping, where the ALT model is trained jointly with the pitch and onset estimation model. The experimental results show the usefulness of the pitch and onset information in ALT.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1CyuskJl7dZ2YWd0M7lAu-Ti0X3D0ahhY)</b>",
      "authors": [
        "Deng, Tengyu*",
        " Nakamura, Eita",
        " Yoshii, Kazuyoshi"
      ],
      "authors_and_affil": [
        "Tengyu Deng (Graduate School of Informatics, Kyoto University, Japan)*",
        " Eita Nakamura (Graduate School of Informatics, Kyoto University, Japan)",
        " Kazuyoshi Yoshii (Graduate School of Informatics, Kyoto University, Japan, PRESTO, Japan Science and Technology Agency, Japan)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CGCTHZJ9",
      "day": "3",
      "keywords": [
        " MIR tasks -> music transcription and annotation",
        "MIR fundamentals and methodology -> music signal processing",
        "MIR fundamentals and methodology -> lyrics and other textual data"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Virtually",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000076.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1jZ0ilSCXRSQcTWl_GQodIO2AeBHlBn2a",
      "session": [
        "5"
      ],
      "slack_channel": "p5-13-deng",
      "title": "End-to-End Lyrics Transcription Informed by Pitch and Onset Estimation",
      "video": "https://drive.google.com/uc?export=preview&id=1CyuskJl7dZ2YWd0M7lAu-Ti0X3D0ahhY"
    },
    "forum": "6",
    "id": "6",
    "pic_id": "https://drive.google.com/open?id=1Canay8HQQGNSbK0hh6SXv1Xey8EZcph_",
    "position": "13",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "5"
  },
  {
    "content": {
      "TLDR": "As one of the most intuitive interfaces known to humans, natural language has the potential to mediate many tasks that involve human-computer interaction, especially in application-focused fields like Music Information Retrieval. In this work, we explore cross-modal learning in an attempt to bridge audio and language in the music domain. To this end, we propose MusCALL, a framework for Music Contrastive Audio-Language Learning. Our approach consists of a dual-encoder architecture that learns the alignment between pairs of music audio and descriptive sentences, producing multimodal embeddings that can be used for text-to-audio and audio-to-text retrieval out-of-the-box. Thanks to this property, MusCALL can be transferred to virtually any task that can be cast as text-based retrieval. \n Our experiments show that our method performs significantly better than the baselines at retrieving audio that matches a textual description and, conversely, text that matches an audio query. We also demonstrate that the multimodal alignment capability of our model can be successfully extended to the zero-shot transfer scenario for genre classification and auto-tagging on two public datasets.",
      "abstract": "As one of the most intuitive interfaces known to humans, natural language has the potential to mediate many tasks that involve human-computer interaction, especially in application-focused fields like Music Information Retrieval. In this work, we explore cross-modal learning in an attempt to bridge audio and language in the music domain. To this end, we propose MusCALL, a framework for Music Contrastive Audio-Language Learning. Our approach consists of a dual-encoder architecture that learns the alignment between pairs of music audio and descriptive sentences, producing multimodal embeddings that can be used for text-to-audio and audio-to-text retrieval out-of-the-box. Thanks to this property, MusCALL can be transferred to virtually any task that can be cast as text-based retrieval. \n Our experiments show that our method performs significantly better than the baselines at retrieving audio that matches a textual description and, conversely, text that matches an audio query. We also demonstrate that the multimodal alignment capability of our model can be successfully extended to the zero-shot transfer scenario for genre classification and auto-tagging on two public datasets.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1yGT7U1odyQAF9QjWG-MqcgRRbI713kCm)</b>",
      "authors": [
        "Manco, Ilaria*",
        " Benetos, Emmanouil",
        " Quinton, Elio",
        " Fazekas, George"
      ],
      "authors_and_affil": [
        "Ilaria Manco (School of EECS, Queen Mary University of London, London, U.K)*",
        " Emmanouil Benetos (School of EECS, Queen Mary University of London, London, U.K)",
        " Elio Quinton (Music & Audio Machine Learning Lab, Universal Music Group, London, U.K.)",
        " Gy\u00f6rgy Fazekas (School of EECS, Queen Mary University of London, London, U.K)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CCP3Q1GE",
      "day": "3",
      "keywords": [
        "Domain knowledge -> machine learning/artificial intelligence for music",
        "Applications -> music retrieval systems",
        " Domain knowledge -> representations of music",
        " MIR fundamentals and methodology -> web mining, and natural language processing",
        " MIR fundamentals and methodology -> multimodality"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Bengaluru",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000077.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1NV_Gdv7CIeEvzEJvrP3PLTfd4xArXf2R",
      "session": [
        "5"
      ],
      "slack_channel": "p5-14-manco",
      "title": "Contrastive Audio-Language Learning for Music",
      "video": "https://drive.google.com/uc?export=preview&id=1yGT7U1odyQAF9QjWG-MqcgRRbI713kCm"
    },
    "forum": "275",
    "id": "275",
    "pic_id": "https://drive.google.com/open?id=1hyhmeByyVCMBgKy6wY7zTVSTiCwVVDL2",
    "position": "14",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "5"
  },
  {
    "content": {
      "TLDR": "We present MusAV, a new public benchmark dataset for comparative validation of arousal and valence (AV) regression models for audio-based music emotion recognition. To gather the ground truth, we rely on relative judgments instead of absolute values to simplify the manual annotation process and improve its consistency. We build MusAV by gathering comparative annotations of arousal and valence on pairs of tracks, using track audio previews and metadata from the Spotify API. The resulting dataset contains 2,092 track previews covering 1,404 genres, with pairwise relative AV judgments by 20 annotators and various subsets of the ground truth based on different levels of annotation agreement. We demonstrate the use of the dataset in an example study evaluating nine models for AV regression that we train based on state-of-the-art audio embeddings and three existing datasets of absolute AV annotations. The results on MusAV offer a view of the performance of the models complementary to the metrics obtained during training and provide insights into the impact of the considered datasets and embeddings on the generalization abilities of the models.",
      "abstract": "We present MusAV, a new public benchmark dataset for comparative validation of arousal and valence (AV) regression models for audio-based music emotion recognition. To gather the ground truth, we rely on relative judgments instead of absolute values to simplify the manual annotation process and improve its consistency. We build MusAV by gathering comparative annotations of arousal and valence on pairs of tracks, using track audio previews and metadata from the Spotify API. The resulting dataset contains 2,092 track previews covering 1,404 genres, with pairwise relative AV judgments by 20 annotators and various subsets of the ground truth based on different levels of annotation agreement. We demonstrate the use of the dataset in an example study evaluating nine models for AV regression that we train based on state-of-the-art audio embeddings and three existing datasets of absolute AV annotations. The results on MusAV offer a view of the performance of the models complementary to the metrics obtained during training and provide insights into the impact of the considered datasets and embeddings on the generalization abilities of the models.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1B_XMXJNMPKKEyJ6_7q35A6v44YCw4GLj)</b>",
      "authors": [
        "Bogdanov, Dmitry*",
        " Lizarraga-Seijas, Xavier",
        " Alonso-Jim\u00e9nez, Pablo",
        " Serra, Xavier"
      ],
      "authors_and_affil": [
        "Dmitry Bogdanov (Music Technology Group, Universitat Pompeu Fabra, Spain)*",
        " Xavier Lizarraga-Seijas (Music Technology Group, Universitat Pompeu Fabra, Spain)",
        " Pablo Alonso-Jim\u00e9nez (Music Technology Group, Universitat Pompeu Fabra, Spain)",
        " Xavier Serra (Music Technology Group, Universitat Pompeu Fabra, Spain)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04C4QR62KZ",
      "day": "3",
      "keywords": [
        " Musical features and properties -> representations of music",
        "Musical features and properties -> musical affect, emotion and mood",
        " Evaluation, datasets, and reproducibility -> novel datasets and use cases",
        "Evaluation, datasets, and reproducibility -> annotation protocols",
        " MIR tasks -> automatic classification"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Bengaluru",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000078.pdf",
      "poster_pdf": "https://drive.google.com/open?id=11Hx1IAV5oxTVwrBnjsZotvbSRnBQDnd7",
      "session": [
        "5"
      ],
      "slack_channel": "p5-15-bogdanov",
      "title": "MusAV: A dataset of relative arousal-valence annotations for validation of audio models",
      "video": "https://drive.google.com/uc?export=preview&id=1B_XMXJNMPKKEyJ6_7q35A6v44YCw4GLj"
    },
    "forum": "286",
    "id": "286",
    "pic_id": "https://drive.google.com/open?id=1UCUzM_P69M0JjoVGW7XO6-5D6Fk7En7r",
    "position": "15",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "5"
  },
  {
    "content": {
      "TLDR": "Structure is one of the most essential aspects of music, and music structure is commonly indicated through repetition. However, the nature of repetition and structure in music is still not well understood, especially in the context of music generation, and much remains to be explored with Music Information Retrieval (MIR) techniques. Analyses of two popular music datasets (Chinese and American) illustrate important music construction principles: (1) structure exists at multiple hierarchical levels, (2) songs use repetition and limited vocabulary so that individual songs do not follow general statistics of song collections, (3) structure interacts with rhythm, melody, harmony, and predictability, and (4) over the course of a song, repetition is not random, but follows a general trend as revealed by cross-entropy. These and other findings offer challenges as well as opportunities for deep-learning music generation and suggest new formal music criteria and evaluation methods. Music from recent music generation systems is analyzed and compared to human-composed music in our datasets, often revealing striking differences from a structural perspective.",
      "abstract": "Structure is one of the most essential aspects of music, and music structure is commonly indicated through repetition. However, the nature of repetition and structure in music is still not well understood, especially in the context of music generation, and much remains to be explored with Music Information Retrieval (MIR) techniques. Analyses of two popular music datasets (Chinese and American) illustrate important music construction principles: (1) structure exists at multiple hierarchical levels, (2) songs use repetition and limited vocabulary so that individual songs do not follow general statistics of song collections, (3) structure interacts with rhythm, melody, harmony, and predictability, and (4) over the course of a song, repetition is not random, but follows a general trend as revealed by cross-entropy. These and other findings offer challenges as well as opportunities for deep-learning music generation and suggest new formal music criteria and evaluation methods. Music from recent music generation systems is analyzed and compared to human-composed music in our datasets, often revealing striking differences from a structural perspective.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1hgpQn0Gn3lO1GYdqMVFAPGLkZizMMTs5)</b>",
      "authors": [
        "Dai, Shuqi*",
        " Yu, Huiran",
        " Dannenberg, Roger B"
      ],
      "authors_and_affil": [
        "Shuqi Dai (Carnegie Mellon University)*",
        " Huiran Yu (Carnegie Mellon University)",
        " Roger B. Dannenberg (Carnegie Mellon University)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04C4QPTMUP",
      "day": "3",
      "keywords": [
        "Domain knowledge -> machine learning/artificial intelligence for music",
        " Musical features and properties",
        " Evaluation, datasets, and reproducibility -> evaluation methodology",
        "Domain knowledge -> computational music theory and musicology",
        " Musical features and properties -> structure, segmentation, and form",
        " MIR tasks -> music generation"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Virtually",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000079.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1EJMLcpDBOwBrKtONt54rNgtkUNN_M7af",
      "session": [
        "5"
      ],
      "slack_channel": "p5-16-dai",
      "title": "What is missing in deep music generation? A study of repetition and structure in popular music",
      "video": "https://drive.google.com/uc?export=preview&id=1hgpQn0Gn3lO1GYdqMVFAPGLkZizMMTs5"
    },
    "forum": "136",
    "id": "136",
    "pic_id": "https://drive.google.com/open?id=1YEa_9gth2NYF3RTiSe-oW98Btgq1n9M2",
    "position": "16",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "5"
  },
  {
    "content": {
      "TLDR": "Music emotion recognition has been a growing field of research motivated by the wealth of information that these labels express. Recognition of emotions highlights music's social and psychological functions, extending traditional applications such as style recognition or content similarity. Once musical data are intrinsically multi-modal, exploring this characteristic is usually beneficial. However, building a structure that incorporates different modalities in a unique space to represent the songs is challenging. Integrating information from related instances by learning heterogeneous graph-based representations has achieved state-of-the-art results in multiple tasks. This paper proposes structuring musical features over a heterogeneous network and learning a multi-modal representation using Graph Convolutional Networks with features extracted from audio and lyrics as inputs to handle the music emotion recognition tasks. We show that the proposed learning approach resulted in a representation with greater power to discriminate emotion labels. Moreover, our heterogeneous graph neural network classifier outperforms related works for music emotion recognition.",
      "abstract": "Music emotion recognition has been a growing field of research motivated by the wealth of information that these labels express. Recognition of emotions highlights music's social and psychological functions, extending traditional applications such as style recognition or content similarity. Once musical data are intrinsically multi-modal, exploring this characteristic is usually beneficial. However, building a structure that incorporates different modalities in a unique space to represent the songs is challenging. Integrating information from related instances by learning heterogeneous graph-based representations has achieved state-of-the-art results in multiple tasks. This paper proposes structuring musical features over a heterogeneous network and learning a multi-modal representation using Graph Convolutional Networks with features extracted from audio and lyrics as inputs to handle the music emotion recognition tasks. We show that the proposed learning approach resulted in a representation with greater power to discriminate emotion labels. Moreover, our heterogeneous graph neural network classifier outperforms related works for music emotion recognition.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=12eZbcXcwCJ2X3hQ0LSgTfsgTg0jxlAHA)</b>",
      "authors": [
        "Mendes da Silva, Angelo Cesar*",
        " Silva, Diego F",
        " Marcacini, Ricardo Marcondes"
      ],
      "authors_and_affil": [
        "Angelo Cesar Mendes da Silva (Universidade de S\u00e3o Paulo, Brazil)*",
        " Diego Furtado Silva (Universidade Federal de S\u00e3o Carlos, Brazil)",
        " Ricardo Marcondes Marcacini (Universidade de S\u00e3o Paulo, Brazil)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CKBA3ZK4",
      "day": "3",
      "keywords": [
        "Domain knowledge -> machine learning/artificial intelligence for music",
        " Musical features and properties -> musical affect, emotion and mood",
        "Musical features and properties -> representations of music",
        " MIR fundamentals and methodology -> multimodality",
        " MIR tasks -> automatic classification"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Virtually",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000080.pdf",
      "poster_pdf": "https://drive.google.com/open?id=15CVr0PQ1kJZqpx4QYunuQ_txrgBKThxD",
      "session": [
        "5"
      ],
      "slack_channel": "p5-17-silva",
      "title": "Heterogeneous Graph Neural Network for Music Emotion Recognition",
      "video": "https://drive.google.com/uc?export=preview&id=12eZbcXcwCJ2X3hQ0LSgTfsgTg0jxlAHA"
    },
    "forum": "282",
    "id": "282",
    "pic_id": "https://drive.google.com/open?id=1ZiECb_DmW_yo3dcsgp34xJKSEVWsxWay",
    "position": "17",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "5"
  },
  {
    "content": {
      "TLDR": "Version identification (VI) has seen substantial progress over the past few years. On the one hand, the introduction of the metric learning paradigm has favored the emergence of scalable yet accurate VI systems. On the other hand, using features focusing on specific aspects of musical pieces, such as melody, harmony, or lyrics, yielded interpretable and promising performances. In this work, we build upon these recent advances and propose a metric learning-based system systematically leveraging four dimensions commonly admitted to convey musical similarity between versions: melodic line, harmonic structure, rhythmic patterns, and lyrics. We describe our deliberately simple model architecture, and we show in particular that an approximated representation of the lyrics is an efficient proxy to discriminate between versions and non-versions. We then describe how these features complement each other and yield new state-of-the-art performances on two publicly available datasets. We finally suggest that a VI system using a combination of melodic, harmonic, rhythmic and lyrics features could theoretically reach the optimal performances obtainable on these datasets.",
      "abstract": "Version identification (VI) has seen substantial progress over the past few years. On the one hand, the introduction of the metric learning paradigm has favored the emergence of scalable yet accurate VI systems. On the other hand, using features focusing on specific aspects of musical pieces, such as melody, harmony, or lyrics, yielded interpretable and promising performances. In this work, we build upon these recent advances and propose a metric learning-based system systematically leveraging four dimensions commonly admitted to convey musical similarity between versions: melodic line, harmonic structure, rhythmic patterns, and lyrics. We describe our deliberately simple model architecture, and we show in particular that an approximated representation of the lyrics is an efficient proxy to discriminate between versions and non-versions. We then describe how these features complement each other and yield new state-of-the-art performances on two publicly available datasets. We finally suggest that a VI system using a combination of melodic, harmonic, rhythmic and lyrics features could theoretically reach the optimal performances obtainable on these datasets.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=19Kw2x2o6CKQbzixgXQ-uJczXDDKYMgls)</b>",
      "authors": [
        "Abrassart, Mathilde*",
        " Doras, Guillaume"
      ],
      "authors_and_affil": [
        "Mathilde Abrassart (Ircam Amplify)*",
        " Guillaume Doras (Ircam, Sorbonne Universit\u00e9, CNRS, STMS Lab)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CKB9HE6N",
      "day": "3",
      "keywords": [
        " Musical features and properties -> melody and motives",
        "Applications -> music retrieval systems",
        " Musical features and properties -> rhythm, beat, tempo",
        "MIR tasks -> fingerprinting",
        " Musical features and properties -> harmony, chords and tonality",
        " MIR tasks -> similarity metrics"
      ],
      "long_presentation": "TRUE",
      "paper_presentation": "Virtually",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000081.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1pxf_e-xXTWLnKSAi2drT_RyDzgVg49In",
      "session": [
        "6"
      ],
      "slack_channel": "p6-01-abrassart",
      "title": "And what if two musical versions don't share melody, harmony, rhythm, or lyrics ?",
      "video": "https://drive.google.com/uc?export=preview&id=19Kw2x2o6CKQbzixgXQ-uJczXDDKYMgls"
    },
    "forum": "221",
    "id": "221",
    "pic_id": "https://drive.google.com/open?id=1UkcTwl0KdkQzXc7PBnQ64CubKwDzlsof",
    "position": "01",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "6"
  },
  {
    "content": {
      "TLDR": "Notable progress in music source separation has been achieved using multi-branch networks that operate on both temporal and spectral domains. However, such networks tend to be complex and heavy-weighted. In this work, we tackle the task of singing voice extraction from polyphonic music signals in an end-to-end manner using an approach inspired by the training procedure of denoising diffusion models. We perform unconditional signal modelling to gradually convert an input mixture signal to the corresponding singing voice or accompaniment. We use fewer parameters than the state-of-the-art models while operating on the waveform domain, bypassing phase-related problems. More concisely, we train a non-causal WaveNet using a diffusion-inspired strategy improving the said network for singing voice extraction and obtaining performance comparable to the end-to-end state-of-the-art on MUSDB18. We further report results on a non-MUSDB-overlapping version of MedleyDB and the multi-track audio of the Saraga Carnatic dataset showing good generalization, and run perceptual tests of our approach. Code, models, and audio examples are made available.",
      "abstract": "Notable progress in music source separation has been achieved using multi-branch networks that operate on both temporal and spectral domains. However, such networks tend to be complex and heavy-weighted. In this work, we tackle the task of singing voice extraction from polyphonic music signals in an end-to-end manner using an approach inspired by the training procedure of denoising diffusion models. We perform unconditional signal modelling to gradually convert an input mixture signal to the corresponding singing voice or accompaniment. We use fewer parameters than the state-of-the-art models while operating on the waveform domain, bypassing phase-related problems. More concisely, we train a non-causal WaveNet using a diffusion-inspired strategy improving the said network for singing voice extraction and obtaining performance comparable to the end-to-end state-of-the-art on MUSDB18. We further report results on a non-MUSDB-overlapping version of MedleyDB and the multi-track audio of the Saraga Carnatic dataset showing good generalization, and run perceptual tests of our approach. Code, models, and audio examples are made available.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1w9UVHiHcwKdzZ8TT-5Zmac-_89Ss5LJA)</b>",
      "authors": [
        "Plaja-Roglans, Gen\u00eds*",
        " Miron, Marius",
        " Serra, Xavier"
      ],
      "authors_and_affil": [
        "Gen\u00eds Plaja-Roglans (Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain)*",
        " Marius Miron (Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain)",
        " Xavier Serra (Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CK87QV5Z",
      "day": "3",
      "keywords": [
        "Domain knowledge -> machine learning/artificial intelligence for music",
        " MIR tasks -> music synthesis and transformation",
        "MIR tasks -> sound source separation",
        " MIR fundamentals and methodology -> music signal processing"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Bengaluru",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000082.pdf",
      "poster_pdf": "https://drive.google.com/open?id=11ai_HSMAujukTAmFJtxvGAGOJlrveXw3",
      "session": [
        "6"
      ],
      "slack_channel": "p6-02-plaja-roglans",
      "title": "A diffusion-inspired training strategy for singing voice extraction in the waveform domain",
      "video": "https://drive.google.com/uc?export=preview&id=1w9UVHiHcwKdzZ8TT-5Zmac-_89Ss5LJA"
    },
    "forum": "262",
    "id": "262",
    "pic_id": "https://drive.google.com/open?id=1vSjJiUd4n7ih5a6HI03JQuXENYjppeVK",
    "position": "02",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "6"
  },
  {
    "content": {
      "TLDR": "Machine learning techniques have proved useful for classifying and analyzing audio content. However, recent methods typically rely on abstract and high-dimensional representations that are difficult to interpret. Inspired by transformation-invariant approaches developed for image and 3D data, we propose an audio identification model based on learnable spectral prototypes. Equipped with dedicated transformation networks, these prototypes can be used to cluster and classify input audio samples from large collections of sounds. Our model can be trained with or without supervision and reaches state-of-the-art results for speaker and instrument identification, while remaining easily interpretable. The code is available at: https://github.com/romainloiseau/a-model-you-can-hear",
      "abstract": "Machine learning techniques have proved useful for classifying and analyzing audio content. However, recent methods typically rely on abstract and high-dimensional representations that are difficult to interpret. Inspired by transformation-invariant approaches developed for image and 3D data, we propose an audio identification model based on learnable spectral prototypes. Equipped with dedicated transformation networks, these prototypes can be used to cluster and classify input audio samples from large collections of sounds. Our model can be trained with or without supervision and reaches state-of-the-art results for speaker and instrument identification, while remaining easily interpretable. The code is available at: https://github.com/romainloiseau/a-model-you-can-hear<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1BLk3gCxIA5v3gg7R85oFgR0VjQiORYsi)</b>",
      "authors": [
        "Loiseau, Romain*",
        " Bouvier, Baptiste",
        " Teytaut, Yann",
        " Vincent, Elliot",
        " Aubry, Mathieu",
        " Landrieu, Loic"
      ],
      "authors_and_affil": [
        "Romain Loiseau (LIGM, Ecole des Ponts, Univ Gustave Eiffel, CNRS, France, LASTIG, Univ Gustave Eiffel, IGN, ENSG)*",
        " Baptiste Bouvier (STMS Lab, UMR 9912 IRCAM, CNRS, Sorbonne University, Paris, France)",
        " Yann Teytaut (STMS Lab, UMR 9912 IRCAM, CNRS, Sorbonne University, Paris, France)",
        " Elliot Vincent (LIGM, Ecole des Ponts, Univ Gustave Eiffel, CNRS, France, INRIA and DIENS ENS-PSL, CNRS, INRIA)",
        " Mathieu Aubry (LIGM, Ecole des Ponts, Univ Gustave Eiffel, CNRS, France)",
        " Loic Landrieu (LASTIG, Univ Gustave Eiffel, IGN, ENSG)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04D92FNZFS",
      "day": "3",
      "keywords": [
        " MIR tasks -> automatic classification",
        "Domain knowledge -> machine learning/artificial intelligence for music",
        "Domain knowledge -> representations of music",
        " MIR tasks -> fingerprinting"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Virtually",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000083.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1-__8LS0fV_lMuqKd-itjhjHuZDWhXv5Z",
      "session": [
        "6"
      ],
      "slack_channel": "p6-03-loiseau",
      "title": "A Model You Can Hear: Audio Identification with Playable Prototypes",
      "video": "https://drive.google.com/uc?export=preview&id=1BLk3gCxIA5v3gg7R85oFgR0VjQiORYsi"
    },
    "forum": "23",
    "id": "23",
    "pic_id": "https://drive.google.com/open?id=1vuqO4CRkbaMJ7ns8dU86I1QjXzRkobpV",
    "position": "03",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "6"
  },
  {
    "content": {
      "TLDR": "Many previous works in recent years have explored various forms of music generation. These works have focused on generating either raw audio waveforms or symbolic music. In this work, we explore the feasibility of generating sheet music images, which is often the primary form in which musical compositions are notated for other musicians. Using the PrIMuS dataset as a testbed, we explore five different sequence-based approaches for generating lines of sheet music: generating sequences of (a) pixel columns, (b) image patches, (c) visual word tokens, (d) semantic tokens, and (e) XML-based tags. We show sample generated images, discuss the practical challenges and problems with each approach, and give our recommendation on the most promising paths to explore in the future.",
      "abstract": "Many previous works in recent years have explored various forms of music generation. These works have focused on generating either raw audio waveforms or symbolic music. In this work, we explore the feasibility of generating sheet music images, which is often the primary form in which musical compositions are notated for other musicians. Using the PrIMuS dataset as a testbed, we explore five different sequence-based approaches for generating lines of sheet music: generating sequences of (a) pixel columns, (b) image patches, (c) visual word tokens, (d) semantic tokens, and (e) XML-based tags. We show sample generated images, discuss the practical challenges and problems with each approach, and give our recommendation on the most promising paths to explore in the future.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1_Xb4reQEJYgEqL23vcAGHihgt-BtF8dg)</b>",
      "authors": [
        "Acosta, Marcos",
        " Bukey, Irmak",
        " Tsai, T J*"
      ],
      "authors_and_affil": [
        "Marcos Acosta (Harvey Mudd College)",
        " Irmak Bukey (Pomona College)",
        " TJ Tsai (Harvey Mudd College)*"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CGCUVCF7",
      "day": "3",
      "keywords": [
        "Domain knowledge -> machine learning/artificial intelligence for music",
        " Musical features and properties -> representations of music",
        "MIR tasks -> music generation",
        " Domain knowledge -> representations of music"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Virtually",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000084.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1w-yGPZwZWKnE9cfP8Xr7tyJFonVG3IBT",
      "session": [
        "6"
      ],
      "slack_channel": "p6-04-tsai",
      "title": "An Exploration of Generating Sheet Music Images",
      "video": "https://drive.google.com/uc?export=preview&id=1_Xb4reQEJYgEqL23vcAGHihgt-BtF8dg"
    },
    "forum": "155",
    "id": "155",
    "pic_id": "https://drive.google.com/open?id=1_IjN18zJOHnLkpkqAAN0tmBa-43Djhk5",
    "position": "04",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "6"
  },
  {
    "content": {
      "TLDR": "While neural network models are making significant progress in piano transcription, they are becoming more resource-consuming due to requiring larger model size and more computing power. In this paper, we attempt to apply more prior about piano to reduce model size and improve the transcription performance. The sound of a piano note contains various overtones, and the pitch of a key does not change over time. To make full use of such latent information, we propose HPPNet that using the Harmonic Dilated Convolution to capture the harmonic structures and the Frequency Grouped Recurrent Neural Network to model the pitch-invariance over time. Experimental results on the MAESTRO dataset show that our piano transcription system achieves state-of-the-art performance both in frame and note scores (frame F1 93.15%, note F1 97.18%). Moreover, the model size is much smaller than the previous state-of-the-art deep learning models.",
      "abstract": "While neural network models are making significant progress in piano transcription, they are becoming more resource-consuming due to requiring larger model size and more computing power. In this paper, we attempt to apply more prior about piano to reduce model size and improve the transcription performance. The sound of a piano note contains various overtones, and the pitch of a key does not change over time. To make full use of such latent information, we propose HPPNet that using the Harmonic Dilated Convolution to capture the harmonic structures and the Frequency Grouped Recurrent Neural Network to model the pitch-invariance over time. Experimental results on the MAESTRO dataset show that our piano transcription system achieves state-of-the-art performance both in frame and note scores (frame F1 93.15%, note F1 97.18%). Moreover, the model size is much smaller than the previous state-of-the-art deep learning models.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1I90r4HFTX8UAOfV2yAc_09WyudcfwMSr)</b>",
      "authors": [
        "Wei, Weixing*",
        " Li, Peilin",
        " Yu, Yi",
        " Li, Wei"
      ],
      "authors_and_affil": [
        "Weixing Wei (School of Computer Science and Technology, Fudan University, China)*",
        " Peilin Li (School of Computer Science and Technology, Fudan University, China)",
        " Yi Yu (Digital Content and Media Sciences Research Division, National Institute of Informatics, Japan)",
        " Wei Li (School of Computer Science and Technology, Fudan University, China, Shanghai Key Laboratory of Intelligent Information Processing, Fudan University, China)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CCP1KUMC",
      "day": "3",
      "keywords": [
        "MIR tasks -> music transcription and annotation",
        "MIR fundamentals and methodology -> music signal processing"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Virtually",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000085.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1Eq2_j_EX6vD1OWsRKF0gN7MsM0HTS_2h",
      "session": [
        "6"
      ],
      "slack_channel": "p6-05-wei",
      "title": "HPPNet: Modeling the Harmonic Structure and Pitch Invariance in Piano Transcription",
      "video": "https://drive.google.com/uc?export=preview&id=1I90r4HFTX8UAOfV2yAc_09WyudcfwMSr"
    },
    "forum": "32",
    "id": "32",
    "pic_id": "https://drive.google.com/open?id=1eYUDlPexj0t4QIfC-DOAR-dCSQmK8_Yi",
    "position": "05",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "6"
  },
  {
    "content": {
      "TLDR": "The field of Automatic Music Generation has seen significant progress thanks to the advent of Deep Learning. However, most of these results have been produced by unconditional models, which lack the ability to interact with their users, not allowing them to guide the generative process in meaningful and practical ways. Moreover, synthesizing music that remains coherent across longer timescales while still capturing the local aspects that make it sound ``realistic'' or human-like is still challenging. This is due to the large computational requirements needed to work with long sequences of data, and also to limitations imposed by the training schemes that are often employed. In this paper, we propose a\n generative model of symbolic music conditioned by data retrieved from human sentiment. The model is a Transformer-GAN trained with labels that correspond to different configurations of the valence and arousal dimensions that quantitatively represent human affective states. We try to tackle both of the problems above by employing an efficient linear version of Attention and using a Discriminator both as a tool to improve the overall quality of the generated music and its ability to follow the conditioning signals.",
      "abstract": "The field of Automatic Music Generation has seen significant progress thanks to the advent of Deep Learning. However, most of these results have been produced by unconditional models, which lack the ability to interact with their users, not allowing them to guide the generative process in meaningful and practical ways. Moreover, synthesizing music that remains coherent across longer timescales while still capturing the local aspects that make it sound ``realistic'' or human-like is still challenging. This is due to the large computational requirements needed to work with long sequences of data, and also to limitations imposed by the training schemes that are often employed. In this paper, we propose a\n generative model of symbolic music conditioned by data retrieved from human sentiment. The model is a Transformer-GAN trained with labels that correspond to different configurations of the valence and arousal dimensions that quantitatively represent human affective states. We try to tackle both of the problems above by employing an efficient linear version of Attention and using a Discriminator both as a tool to improve the overall quality of the generated music and its ability to follow the conditioning signals.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1vuuOH693beSdcybc3gwGs1T3vkQKzc2S)</b>",
      "authors": [
        "Neves, Pedro L T*",
        " Fornari, Jos\u00e9",
        " Florindo, Jo\u00e3o B"
      ],
      "authors_and_affil": [
        "Pedro L T Neves (State University of Campinas)*",
        " Jos\u00e9 E F Novo Junior (State University of Campinas)",
        " Jo\u00e3o B Florindo (State University of Campinas)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CY0LP7GR",
      "day": "3",
      "keywords": [
        "Domain knowledge -> machine learning/artificial intelligence for music",
        " MIR fundamentals and methodology -> symbolic music processing",
        "MIR tasks -> music generation",
        " Musical features and properties -> musical affect, emotion and mood"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Bengaluru",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000086.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1MWbEl2v-OICw0a3JI60Tv_o1qwus2wTs",
      "session": [
        "6"
      ],
      "slack_channel": "p6-06-neves",
      "title": "Generating music with sentiment using Transformer-GANs",
      "video": "https://drive.google.com/uc?export=preview&id=1vuuOH693beSdcybc3gwGs1T3vkQKzc2S"
    },
    "forum": "45",
    "id": "45",
    "pic_id": "https://drive.google.com/open?id=1PcUIEJovwkmsHcxwbgoYYu0ap8cY81tG",
    "position": "06",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "6"
  },
  {
    "content": {
      "TLDR": "Choral music separation refers to the task of extracting tracks of voice parts (e.g., soprano, alto, tenor, and bass) from mixed audio. The lack of datasets has impeded research on this topic as previous work has only been able to train and evaluate models on a few minutes of choral music data due to copyright issues and dataset collection difficulties. In this paper, we investigate the use of synthesized training data for the source separation task on real choral music. We make three contributions: first, we provide an automated pipeline for synthesizing choral music data from sampled instrument plugins within controllable options for instrument expressiveness. This produces an 8.2-hour-long choral music dataset from the JSB Chorales Dataset and one can easily synthesize additional data. Second, we conduct an experiment to evaluate multiple separation models on available choral music separation datasets from previous work. To the best of our knowledge, this is the first experiment to comprehensively evaluate choral music separation. Third, experiments demonstrate that the synthesized choral data is of sufficient quality to improve the model's performance on real choral music datasets. This provides additional experimental statistics and data support for the choral music separation study.",
      "abstract": "Choral music separation refers to the task of extracting tracks of voice parts (e.g., soprano, alto, tenor, and bass) from mixed audio. The lack of datasets has impeded research on this topic as previous work has only been able to train and evaluate models on a few minutes of choral music data due to copyright issues and dataset collection difficulties. In this paper, we investigate the use of synthesized training data for the source separation task on real choral music. We make three contributions: first, we provide an automated pipeline for synthesizing choral music data from sampled instrument plugins within controllable options for instrument expressiveness. This produces an 8.2-hour-long choral music dataset from the JSB Chorales Dataset and one can easily synthesize additional data. Second, we conduct an experiment to evaluate multiple separation models on available choral music separation datasets from previous work. To the best of our knowledge, this is the first experiment to comprehensively evaluate choral music separation. Third, experiments demonstrate that the synthesized choral data is of sufficient quality to improve the model's performance on real choral music datasets. This provides additional experimental statistics and data support for the choral music separation study.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1ciUlGPKpwnnfuHqjVQTW5wd60Lu0KvaS)</b>",
      "authors": [
        "Chen, Ke*",
        " Dong, Hao-Wen",
        " Luo, Yi",
        " McAuley, Julian",
        " Berg-Kirkpatrick, Taylor",
        " Puckette, Miller",
        " Dubnov, Shlomo"
      ],
      "authors_and_affil": [
        "Ke Chen (UC San Diego, USA)*",
        " Hao-Wen Dong (UC San Diego, USA)",
        " Yi Luo (Tencent AI Lab, China)",
        " Julian McAuley (UC San Diego, USA)",
        " Taylor Berg-Kirkpatrick (UC San Diego, USA)",
        " Miller Puckette (UC San Diego, USA)",
        " Shlomo Dubnov (UC San Diego, USA)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CGCU7561",
      "day": "3",
      "keywords": [
        "Evaluation, datasets, and reproducibility -> novel datasets and use cases",
        " MIR tasks -> sound source separation",
        "MIR tasks",
        " MIR fundamentals and methodology -> music signal processing"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Virtually",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000087.pdf",
      "poster_pdf": "https://drive.google.com/open?id=178fDe_sW05aT3I_3BEMm1eQNvY1w14v9",
      "session": [
        "6"
      ],
      "slack_channel": "p6-07-chen",
      "title": "Improving Choral Music Separation through Expressive Synthesized Data from Sampled Instruments",
      "video": "https://drive.google.com/uc?export=preview&id=1ciUlGPKpwnnfuHqjVQTW5wd60Lu0KvaS"
    },
    "forum": "78",
    "id": "78",
    "pic_id": "https://drive.google.com/open?id=1AIWjC2LXb3ssw-uQjiMRgwuIUBgw1uuX",
    "position": "07",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "6"
  },
  {
    "content": {
      "TLDR": "Singing Voice Synthesis (SVS) has recently garnered much attention as its quality has improved vastly with the use of artificial intelligence (AI), creating many opportunities for supporting music creators and listeners. Recently, there have been growing concerns about ethical issues related to AI development in general, and to AI-based SVS development specifically. Many questions remain unexplored about how to ethically develop and use such technology. In this paper, we investigate the perception of ethical issues related to SVS from the perspectives of two different groups: the general public and developers. We collected 3,075 user comments from YouTube videos showcasing various uses of SVS as part of a mainstream variety show. Additionally, we interviewed six researchers developing SVS technology. Through thematic analysis, we identify and discuss three different aspects related to ethical issues in SVS development, highlighting the similarities and differences between the perspectives of the general public and developers: (1) Use scenarios, (2) Attitudes towards development, and (3) Meaning of \"Creativity\", and (4) Concerns about human rights, intellectual property (IP) and legal issues.",
      "abstract": "Singing Voice Synthesis (SVS) has recently garnered much attention as its quality has improved vastly with the use of artificial intelligence (AI), creating many opportunities for supporting music creators and listeners. Recently, there have been growing concerns about ethical issues related to AI development in general, and to AI-based SVS development specifically. Many questions remain unexplored about how to ethically develop and use such technology. In this paper, we investigate the perception of ethical issues related to SVS from the perspectives of two different groups: the general public and developers. We collected 3,075 user comments from YouTube videos showcasing various uses of SVS as part of a mainstream variety show. Additionally, we interviewed six researchers developing SVS technology. Through thematic analysis, we identify and discuss three different aspects related to ethical issues in SVS development, highlighting the similarities and differences between the perspectives of the general public and developers: (1) Use scenarios, (2) Attitudes towards development, and (3) Meaning of \"Creativity\", and (4) Concerns about human rights, intellectual property (IP) and legal issues.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1_uqARNN5oTzBJCH11H30LAkpH2jwBR4G)</b>",
      "authors": [
        "Lee, Kyungyun",
        " Hitt, Gladys",
        " Terada, Emily",
        " Lee, Jin Ha*"
      ],
      "authors_and_affil": [
        "Kyungyun Lee (Gaudio Lab, Seoul, Korea)",
        " Gladys Hitt (University of Washington, Seattle, USA)",
        " Emily Terada (University of Washington, Seattle, USA)",
        " Jin Ha Lee (University of Washington, Seattle, USA)*"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CKB8QSP4",
      "day": "3",
      "keywords": [
        "Philosophical and ethical discussions -> ethical issues related to designing and implementing MIR to",
        "Human-centered MIR -> human-computer interaction"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Virtually",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000088.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1HFb7a5n-xNRCbaSpia4km_fgw-oCZx37",
      "session": [
        "6"
      ],
      "slack_channel": "p6-08-lee",
      "title": "Ethics of Singing Voice Synthesis: Perceptions of Users and Developers",
      "video": "https://drive.google.com/uc?export=preview&id=1_uqARNN5oTzBJCH11H30LAkpH2jwBR4G"
    },
    "forum": "131",
    "id": "131",
    "pic_id": "https://drive.google.com/open?id=1nUxZUs3vxTbxh7ZB4VjAXBQYcAlt7U5Z",
    "position": "08",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "6"
  },
  {
    "content": {
      "TLDR": "We propose and assess deep learning models for harmonic and tempo arrangement generation given melodies and emotional constraints. A dataset of 4000 symbolic scores and emotion labels was gathered by expanding the HTPD3 dataset with mood tags from last.fm and allmusic.com. We explore how bi-directional LSTM and Transformer encoder architectures can learn relationships between symbolic melodies, chord progressions, tempo, and expressed emotions, with and without a transfer learning strategy leveraging symbolic music data without emotion labels. Three emotion annotation summarisation methods based on the Arousal/Valence (AV) representation are compared: Emotion Average, Emotion Surface, and Emotion Category. 20 participants (average age: 30.2, 7 females and 13 males from Japan) rated how well generated accompaniments matched melodies (musical coherence) as well as perceived emotions for 75 arrangements corresponding to combinations of models and emotion summarisation methods. Musical coherence and match between target and perceived emotions were highest when melodies were encoded using a BLSTM model with transfer learning. The proposed method generates emotion-driven harmonic/tempo arrangements in a fast way, a keen advantage compared to state of the art. Applications of this work include AI-based composition assistant and live interactive music systems for entertainment such as video games.",
      "abstract": "We propose and assess deep learning models for harmonic and tempo arrangement generation given melodies and emotional constraints. A dataset of 4000 symbolic scores and emotion labels was gathered by expanding the HTPD3 dataset with mood tags from last.fm and allmusic.com. We explore how bi-directional LSTM and Transformer encoder architectures can learn relationships between symbolic melodies, chord progressions, tempo, and expressed emotions, with and without a transfer learning strategy leveraging symbolic music data without emotion labels. Three emotion annotation summarisation methods based on the Arousal/Valence (AV) representation are compared: Emotion Average, Emotion Surface, and Emotion Category. 20 participants (average age: 30.2, 7 females and 13 males from Japan) rated how well generated accompaniments matched melodies (musical coherence) as well as perceived emotions for 75 arrangements corresponding to combinations of models and emotion summarisation methods. Musical coherence and match between target and perceived emotions were highest when melodies were encoded using a BLSTM model with transfer learning. The proposed method generates emotion-driven harmonic/tempo arrangements in a fast way, a keen advantage compared to state of the art. Applications of this work include AI-based composition assistant and live interactive music systems for entertainment such as video games.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1vEyuNN9_Ol9nW987ZlxJ8qZh8BY7jCBu)</b>",
      "authors": [
        "Takahashi, Takuya*",
        " Barthet, Mathieu"
      ],
      "authors_and_affil": [
        "Takuya Takahashi (Centre for Digital Music, Queen Mary University of London)*",
        " Mathieu Barthet (Centre for Digital Music, Queen Mary University of London)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CY0M5ZBK",
      "day": "3",
      "keywords": [
        " Applications -> music composition",
        " Musical features and properties -> musical affect, emotion and mood",
        "Applications -> gaming, augmented/virtual reality",
        " Musical features and properties -> rhythm, beat, tempo",
        " Musical features and properties -> harmony, chords and tonality",
        "MIR tasks -> music generation"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Virtually",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000089.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1vIQuBKX69giJ73EStghxKlxwThSTIiam",
      "session": [
        "6"
      ],
      "slack_channel": "p6-09-takahashi",
      "title": "Emotion-driven Harmonisation And Tempo Arrangement of Melodies Using Transfer Learning",
      "video": "https://drive.google.com/uc?export=preview&id=1vEyuNN9_Ol9nW987ZlxJ8qZh8BY7jCBu"
    },
    "forum": "80",
    "id": "80",
    "pic_id": "https://drive.google.com/open?id=1th5cZjVh8_6Bb20PH70JcPgVzIrrPM2j",
    "position": "09",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "6"
  },
  {
    "content": {
      "TLDR": "Audio synchronization aims at aligning multiple recordings of the same piece of music. Traditional synchronization approaches are often based on dynamic time warping using chroma features as an input representation. Previous work has shown how one can integrate onset cues into this pipeline for improving the alignment's temporal accuracy. Furthermore, recent work based on deep neural networks has led to significant improvements for learning onset, beat, and downbeat activation functions. However, for music with soft onsets and abrupt tempo changes, these functions may be unreliable, leading to unstable results. As the main contribution of this paper, we introduce a combined approach that integrates activation functions into the synchronization pipeline. We show that this approach improves the temporal accuracy thanks to the activation cues while inheriting the robustness of the traditional synchronization approach. Conducting experiments based on string quartet recordings, we evaluate our combined approach where we transfer measure annotations from a reference recording to a target recording.",
      "abstract": "Audio synchronization aims at aligning multiple recordings of the same piece of music. Traditional synchronization approaches are often based on dynamic time warping using chroma features as an input representation. Previous work has shown how one can integrate onset cues into this pipeline for improving the alignment's temporal accuracy. Furthermore, recent work based on deep neural networks has led to significant improvements for learning onset, beat, and downbeat activation functions. However, for music with soft onsets and abrupt tempo changes, these functions may be unreliable, leading to unstable results. As the main contribution of this paper, we introduce a combined approach that integrates activation functions into the synchronization pipeline. We show that this approach improves the temporal accuracy thanks to the activation cues while inheriting the robustness of the traditional synchronization approach. Conducting experiments based on string quartet recordings, we evaluate our combined approach where we transfer measure annotations from a reference recording to a target recording.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1O7TNpNkLw1zlSaIxeuR2xH-CRpf39F9C)</b>",
      "authors": [
        "\u00d6zer, Yigitcan*",
        " I\u0161tv\u00e1nek, Matej",
        " Arifi-M\u00fcller, Vlora",
        " M\u00fcller, Meinard"
      ],
      "authors_and_affil": [
        "Yigitcan \u00d6zer (International Audio Laboratories Erlangen, Germany)*",
        " Matej I\u0161tv\u00e1nek (Brno University of Technology, Brno, Czech Republic)",
        " Vlora Arifi-M\u00fcller (International Audio Laboratories Erlangen, Germany)",
        " Meinard M\u00fcller (International Audio Laboratories Erlangen, Germany)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04D92HA7RN",
      "day": "3",
      "keywords": [
        " Musical features and properties -> rhythm, beat, tempo",
        " Musical features and properties -> representations of music",
        "MIR tasks -> alignment, synchronization, and score following",
        "Musical features and properties -> musical style and genre"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Bengaluru",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000090.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1Vdd92SN5zEyv8jSpFMuPQJbGX-PZknYt",
      "session": [
        "6"
      ],
      "slack_channel": "p6-10-\u00f6zer",
      "title": "Using Activation Functions for Improving Measure-Level Audio Synchronization",
      "video": "https://drive.google.com/uc?export=preview&id=1O7TNpNkLw1zlSaIxeuR2xH-CRpf39F9C"
    },
    "forum": "195",
    "id": "195",
    "pic_id": "https://drive.google.com/open?id=1YSwLNC-Z6C3L80mlEKbRc9HPt4QKVPGm",
    "position": "10",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "6"
  },
  {
    "content": {
      "TLDR": "The task of identifying melodic lines in a polyphonic piece is a known active research topic in the symbolic and audio domain. Its importance has attracted the interest of researchers focusing on Music Information Retrieval and musicological applications and achieving high results is a common goal in industrial applications. The distinction of a melody in a written score can be a challenging task, however improvements have been reported in recent years using deep learning methods. In this paper, we present a lightweight deep bidirectional LSTM model for identifying the most salient melodic line of a music piece using handcrafted features without requiring the input score to be separated into multiple parts. We evaluate our model to measure the effectiveness of several data augmentation techniques and to compare performance to other state-of-the-art models. We also identify the features importance and evaluate their incremental contribution on the model performance using evaluation metrics. Results on the POP909 dataset show that our model approximates or outperforms current state of the art models trained on the same dataset, based on different implemented metrics and observations.",
      "abstract": "The task of identifying melodic lines in a polyphonic piece is a known active research topic in the symbolic and audio domain. Its importance has attracted the interest of researchers focusing on Music Information Retrieval and musicological applications and achieving high results is a common goal in industrial applications. The distinction of a melody in a written score can be a challenging task, however improvements have been reported in recent years using deep learning methods. In this paper, we present a lightweight deep bidirectional LSTM model for identifying the most salient melodic line of a music piece using handcrafted features without requiring the input score to be separated into multiple parts. We evaluate our model to measure the effectiveness of several data augmentation techniques and to compare performance to other state-of-the-art models. We also identify the features importance and evaluate their incremental contribution on the model performance using evaluation metrics. Results on the POP909 dataset show that our model approximates or outperforms current state of the art models trained on the same dataset, based on different implemented metrics and observations.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/file/d/1q1HAq03MtNNx2tqb0KQXLQWHWpojYKTg)</b>",
      "authors": [
        "Kosta, Katerina*",
        " Lu, Wei Tsung",
        " Medeot, Gabriele",
        " Chanquion, Pierre"
      ],
      "authors_and_affil": [
        "Katerina Kosta (ByteDance)*",
        " Wei Tsung Lu (ByteDance)",
        " Gabriele Medeot (ByteDance)",
        " Pierre Chanquion (ByteDance)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04D92HB52L",
      "day": "3",
      "keywords": [
        "MIR fundamentals and methodology -> symbolic music processing",
        "Musical features and properties -> melody and motives",
        " MIR tasks -> automatic classification"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Virtually",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000091.pdf",
      "poster_pdf": "https://drive.google.com/file/d/1spdzWUhSXptwi4Nw1rXXTEhss8f50Cm1/view?usp=share_link",
      "session": [
        "6"
      ],
      "slack_channel": "p6-11-kosta",
      "title": "A deep learning method for melody extraction from a polyphonic symbolic music representation",
      "video": "https://drive.google.com/file/d/1q1HAq03MtNNx2tqb0KQXLQWHWpojYKTg"
    },
    "forum": "196",
    "id": "196",
    "pic_id": "https://drive.google.com/file/d/1XdHgdldt8Es5aGmjd86hluqNDrtyR2fc/view?usp=share_link",
    "position": "11",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "6"
  },
  {
    "content": {
      "TLDR": "Reproducibility of results is a central pillar of scientific work. In music information retrieval research, this is widely acknowledged and practiced by the community by re-implementing algorithms and re-validating machine learning experiments. In this paper, we argue for an increased need to also reproduce the results and findings of user studies, including qualitative work, especially since these often lay the foundations and serve as justification for choices taken in algorithmic design and optimization criteria. As an example, we attempt to reproduce the study by Kim et al. presented in the RecSys (2020) paper ''Do Channels Matter? Illuminating Interpersonal Influence on Music Recommendations.'' By repeating this study on how interpersonal relationships can affect a user's assessment of music recommendations on a new sample of n=142 participants, we can largely confirm and support the validity of the original results. At the same time, we extend the analysis and also observe differences with regards to adoption rates between different channels as well as different factors that influences the adoption rate. From this specific reproducibility study, we conclude that potential cultural differences should be accounted for more explicitly in future studies and that systems development should be more explicitly connected to its intended target audience.",
      "abstract": "Reproducibility of results is a central pillar of scientific work. In music information retrieval research, this is widely acknowledged and practiced by the community by re-implementing algorithms and re-validating machine learning experiments. In this paper, we argue for an increased need to also reproduce the results and findings of user studies, including qualitative work, especially since these often lay the foundations and serve as justification for choices taken in algorithmic design and optimization criteria. As an example, we attempt to reproduce the study by Kim et al. presented in the RecSys (2020) paper ''Do Channels Matter? Illuminating Interpersonal Influence on Music Recommendations.'' By repeating this study on how interpersonal relationships can affect a user's assessment of music recommendations on a new sample of n=142 participants, we can largely confirm and support the validity of the original results. At the same time, we extend the analysis and also observe differences with regards to adoption rates between different channels as well as different factors that influences the adoption rate. From this specific reproducibility study, we conclude that potential cultural differences should be accounted for more explicitly in future studies and that systems development should be more explicitly connected to its intended target audience.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1wi6KSaA3kqqAr3917K4KWxxV-XJiketH)</b>",
      "authors": [
        "Knees, Peter*",
        " Ferwerda, Bruce",
        " Rauber, Andreas",
        " Strumbelj, Sebastian",
        " Resch, Annabel",
        " Tomandl, Laurenz",
        " Bauer, Valentin",
        " Tang, Fung Yee",
        " Bobinac, Josip",
        " Ceranic, Amila",
        " Dizdar, Riad"
      ],
      "authors_and_affil": [
        "Peter Knees (Faculty of Informatics, TU Wien, Austria, School of Music, Georgia Institute of Technology, USA)*",
        " Bruce Ferwerda (Department of Computer Science and Informatics, J\u00f6nk\u00f6ping University, Sweden)",
        " Andreas Rauber (Faculty of Informatics, TU Wien, Austria)",
        " Sebastian Strumbelj (Faculty of Informatics, TU Wien, Austria)",
        " Annabel Resch (Faculty of Informatics, TU Wien, Austria)",
        " Laurenz Tomandl (Faculty of Informatics, TU Wien, Austria)",
        " Valentin Bauer (Faculty of Informatics, TU Wien, Austria)",
        " Fung Yee Tang (Faculty of Informatics, TU Wien, Austria)",
        " Josip Bobinac (Faculty of Informatics, TU Wien, Austria)",
        " Amila Ceranic (Faculty of Informatics, TU Wien, Austria)",
        " Riad Dizdar (Faculty of Informatics, TU Wien, Austria)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04C4QQFBCP",
      "day": "3",
      "keywords": [
        "Human-centered MIR",
        "Evaluation, datasets, and reproducibility -> reproducibility"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Bengaluru",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000092.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1MdamLjENVz249t-oqgq47-TCkuSaaMT3",
      "session": [
        "6"
      ],
      "slack_channel": "p6-12-knees",
      "title": "A Reproducibility Study on User-centric MIR Research and Why it is Important",
      "video": "https://drive.google.com/uc?export=preview&id=1wi6KSaA3kqqAr3917K4KWxxV-XJiketH"
    },
    "forum": "209",
    "id": "209",
    "pic_id": "https://drive.google.com/open?id=1TsjCIMzXbuDW0QgA9MvSxMp6WvkJUFWh",
    "position": "12",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "6"
  },
  {
    "content": {
      "TLDR": "Despite phenomenal progress in recent years, state-of-the-art music separation systems produce source estimates with significant perceptual shortcomings, such as adding extraneous noise or removing harmonics. We propose a post-processing model (the Make it Sound Good (MSG) post-processor) to enhance the output of music source separation systems. We apply our post-processing model to state-of-the-art waveform-based and spectrogram-based music source separators, including a separator unseen by MSG during training. Our analysis of the errors produced by source separators shows that waveform models tend to introduce more high-frequency noise, while spectrogram models tend to lose transients and high frequency content. We introduce objective measures to quantify both kinds of errors and show MSG improves the source reconstruction of both kinds of errors. Crowdsourced subjective evaluations demonstrate that human listeners prefer source estimates of bass and drums that have been post-processed by MSG.",
      "abstract": "Despite phenomenal progress in recent years, state-of-the-art music separation systems produce source estimates with significant perceptual shortcomings, such as adding extraneous noise or removing harmonics. We propose a post-processing model (the Make it Sound Good (MSG) post-processor) to enhance the output of music source separation systems. We apply our post-processing model to state-of-the-art waveform-based and spectrogram-based music source separators, including a separator unseen by MSG during training. Our analysis of the errors produced by source separators shows that waveform models tend to introduce more high-frequency noise, while spectrogram models tend to lose transients and high frequency content. We introduce objective measures to quantify both kinds of errors and show MSG improves the source reconstruction of both kinds of errors. Crowdsourced subjective evaluations demonstrate that human listeners prefer source estimates of bass and drums that have been post-processed by MSG.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1ePaU2wBytZx_pqJb8uouxlJS9BRk9LLO)</b>",
      "authors": [
        "Schaffer, Noah*",
        " Cogan, Boaz",
        " Manilow, Ethan",
        " Morrison, Max",
        " Seetharaman, Prem",
        " Pardo, Bryan"
      ],
      "authors_and_affil": [
        "Noah Schaffer (Interactive Audio Lab, Northwestern University, Evanston IL, USA)*",
        " Boaz Cogan (Interactive Audio Lab, Northwestern University, Evanston IL, USA)",
        " Ethan Manilow (Interactive Audio Lab, Northwestern University, Evanston IL, USA)",
        " Max Morrison (Interactive Audio Lab, Northwestern University, Evanston IL, USA)",
        " Prem Seetharaman (Descript, Inc)",
        " Bryan Pardo (Interactive Audio Lab, Northwestern University, Evanston IL, USA)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CKB9JE2E",
      "day": "3",
      "keywords": [
        " Evaluation, datasets, and reproducibility -> MIR tasks",
        "MIR tasks -> sound source separation",
        "Evaluation, datasets, and reproducibility -> evaluation methodology",
        " MIR tasks -> music synthesis and transformation",
        " MIR tasks -> music generation"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Virtually",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000093.pdf",
      "poster_pdf": "https://drive.google.com/open?id=10uCuqwwsDUCyYSMBhXU_VJlzouwMmuIn",
      "session": [
        "6"
      ],
      "slack_channel": "p6-13-schaffer",
      "title": "Music Separation Enhancement with Generative Modeling",
      "video": "https://drive.google.com/uc?export=preview&id=1ePaU2wBytZx_pqJb8uouxlJS9BRk9LLO"
    },
    "forum": "225",
    "id": "225",
    "pic_id": "https://drive.google.com/open?id=19Weiy20EdE8xvO9KfWOHvJ-fB5VlQzxw",
    "position": "13",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "6"
  },
  {
    "content": {
      "TLDR": "Modern digital music production typically involves combining numerous acoustic elements to compile a piece of music. Important types of such elements are drum samples, which determine the characteristics of the percussive components of the piece. Artists must use their aesthetic judgement to assess whether a given drum sample fits the current musical context. However, selecting drum samples from a potentially large library is tedious and may interrupt the creative flow. In this work, we explore the automatic drum sample retrieval based on aesthetic principles learned from data. As a result, artists can rank the samples in their library by fit to some musical context at different stages of the production process (i.e., by fit to incomplete song mixtures). To this end, we use contrastive learning to maximize the score of drum samples originating from the same song as the mixture. We conduct a listening test to determine whether the human ratings match the automatic scoring function. We also perform objective quantitative analyses to evaluate the efficacy of our approach.",
      "abstract": "Modern digital music production typically involves combining numerous acoustic elements to compile a piece of music. Important types of such elements are drum samples, which determine the characteristics of the percussive components of the piece. Artists must use their aesthetic judgement to assess whether a given drum sample fits the current musical context. However, selecting drum samples from a potentially large library is tedious and may interrupt the creative flow. In this work, we explore the automatic drum sample retrieval based on aesthetic principles learned from data. As a result, artists can rank the samples in their library by fit to some musical context at different stages of the production process (i.e., by fit to incomplete song mixtures). To this end, we use contrastive learning to maximize the score of drum samples originating from the same song as the mixture. We conduct a listening test to determine whether the human ratings match the automatic scoring function. We also perform objective quantitative analyses to evaluate the efficacy of our approach.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1v8NaHzFdP4gTEFX6ElPx52-Byh-x87BW)</b>",
      "authors": [
        "Lattner, Stefan*"
      ],
      "authors_and_affil": [
        "Stefan Lattner (Sony Computer Science Laboratories, Paris, France)*"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04C4QQG07R",
      "day": "3",
      "keywords": [
        " Musical features and properties -> timbre, instrumentation, and singing voice",
        "MIR tasks -> indexing and querying",
        "Applications -> performance, and production",
        " MIR fundamentals and methodology -> music signal processing",
        " Domain knowledge -> machine learning/artificial intelligence for music",
        " MIR tasks -> similarity metrics"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Bengaluru",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000094.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1Ljv0GV74Jpg-ELBpO--bIfc4py3qCh0E",
      "session": [
        "6"
      ],
      "slack_channel": "p6-14-lattner",
      "title": "SampleMatch: Drum Sample Retrieval by Musical Context",
      "video": "https://drive.google.com/uc?export=preview&id=1v8NaHzFdP4gTEFX6ElPx52-Byh-x87BW"
    },
    "forum": "211",
    "id": "211",
    "pic_id": "https://drive.google.com/open?id=1L7h56UFGymGdMdZd6tHjL_0n6aRO3ltF",
    "position": "14",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "6"
  },
  {
    "content": {
      "TLDR": "The outputs of Optical Music Recognition (OMR) systems require time-consuming human correction. Given that most of the errors induced by OMR processes appear \"non-musical\" to humans, we propose that the time to correct errors may be reduced by marking all symbols on a score that are musically unlikely, allowing the human to focus their attention accordingly. Using a dataset of Romantic string quartets, we train a variant of the Transformer network architecture on the task of classifying each symbol of an optically-recognized musical piece in symbolic format as correct or erroneous, based on whether a manual correction of the piece would require an insertion, deletion, or replacement of a symbol at that location. Since we have a limited amount of data with real OMR errors, we employ extensive data augmentation to add errors into training data in a way that mimics how OMR would modify the score. Our best-performing models achieve 99% recall and 50% precision on this error-detection task.",
      "abstract": "The outputs of Optical Music Recognition (OMR) systems require time-consuming human correction. Given that most of the errors induced by OMR processes appear \"non-musical\" to humans, we propose that the time to correct errors may be reduced by marking all symbols on a score that are musically unlikely, allowing the human to focus their attention accordingly. Using a dataset of Romantic string quartets, we train a variant of the Transformer network architecture on the task of classifying each symbol of an optically-recognized musical piece in symbolic format as correct or erroneous, based on whether a manual correction of the piece would require an insertion, deletion, or replacement of a symbol at that location. Since we have a limited amount of data with real OMR errors, we employ extensive data augmentation to add errors into training data in a way that mimics how OMR would modify the score. Our best-performing models achieve 99% recall and 50% precision on this error-detection task.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1iOUbYiJZQgZeDPZfF72fr_Lhykj7WhNQ)</b>",
      "authors": [
        "de Reuse, Timothy*",
        " Fujinaga, Ichiro"
      ],
      "authors_and_affil": [
        "Timothy de Reuse (Centre for Interdisciplinary Research in Music Media and Technology, McGill University)*",
        " Ichiro Fujinaga (Centre for Interdisciplinary Research in Music Media and Technology, McGill University)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CMQV3Y76",
      "day": "3",
      "keywords": [
        " MIR fundamentals and methodology -> symbolic music processing",
        "MIR tasks -> optical music recognition",
        "Applications -> music retrieval systems",
        " Domain knowledge -> representations of music",
        " Domain knowledge -> machine learning/artificial intelligence for music"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Bengaluru",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000095.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1BHbdECdT3ATAqOPD9BkQE6-8YJcc8aLu",
      "session": [
        "6"
      ],
      "slack_channel": "p6-15-reuse",
      "title": "A Transformer-Based \"Spellchecker\" for Detecting Errors in OMR Output",
      "video": "https://drive.google.com/uc?export=preview&id=1iOUbYiJZQgZeDPZfF72fr_Lhykj7WhNQ"
    },
    "forum": "134",
    "id": "134",
    "pic_id": "https://drive.google.com/open?id=1QFjVEhTbOEl5Y4T9OOw4-X4R1ikWhZ6O",
    "position": "15",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "6"
  },
  {
    "content": {
      "TLDR": "This study explores the association between music preferences and moral values by applying text analysis techniques to lyrics. Harvesting data from a Facebook-hosted application, we align psychometric scores of 1,386 users to lyrics from the top 5 songs of their preferred music artists as emerged from Facebook Page Likes. We extract a set of lyrical features related to each song\u2019s overarching narrative, moral valence, sentiment, and emotion. A machine learning framework was designed to exploit regression ap- proaches and evaluate the predictive power of lyrical features for inferring moral values. Results suggest that lyrics from top songs of artists people like inform their morality. Virtues of hierarchy and tradition achieve higher prediction scores (.20 \u2264 r \u2264 .30) than values of empathy and equality (.08 \u2264 r \u2264 .11), while basic demographic variables only account for a small part in the models\u2019 explainability. This shows the importance of music listening behaviours, as assessed via lyrical preferences, alone in capturing moral values. We discuss the technological and musicological implications and possible future improvements.",
      "abstract": "This study explores the association between music preferences and moral values by applying text analysis techniques to lyrics. Harvesting data from a Facebook-hosted application, we align psychometric scores of 1,386 users to lyrics from the top 5 songs of their preferred music artists as emerged from Facebook Page Likes. We extract a set of lyrical features related to each song\u2019s overarching narrative, moral valence, sentiment, and emotion. A machine learning framework was designed to exploit regression ap- proaches and evaluate the predictive power of lyrical features for inferring moral values. Results suggest that lyrics from top songs of artists people like inform their morality. Virtues of hierarchy and tradition achieve higher prediction scores (.20 \u2264 r \u2264 .30) than values of empathy and equality (.08 \u2264 r \u2264 .11), while basic demographic variables only account for a small part in the models\u2019 explainability. This shows the importance of music listening behaviours, as assessed via lyrical preferences, alone in capturing moral values. We discuss the technological and musicological implications and possible future improvements.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1MMI5RbZdgjhQOavn7Slxzysb2RQ_Tza1)</b>",
      "authors": [
        "Preniqi, Vjosa*",
        " Kalimeri, Kyriaki",
        " Saitis, Charalampos"
      ],
      "authors_and_affil": [
        "Vjosa Preniqi (Centre for Digital Music, Queen Mary University of London, London UK)*",
        " Kyriaki Kalimeri (ISI Foundation, Turin, Italy)",
        " Charalampos Saitis (Centre for Digital Music, Queen Mary University of London, London UK)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CMQWA9M2",
      "day": "3",
      "keywords": [
        "Human-centered MIR",
        " Human-centered MIR -> personalization",
        "Applications -> music recommendation and playlist generation",
        " Human-centered MIR -> user modeling",
        " Human-centered MIR -> user behavior analysis and mining",
        " MIR fundamentals and methodology -> lyrics and other textual data"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Bengaluru",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000096.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1anqS2_P7AJ8Ft9JvpfWblzDDsGNmeF4T",
      "session": [
        "6"
      ],
      "slack_channel": "p6-16-preniqi",
      "title": "\"More than words\": Linking Music Preferences and Moral Values through Lyrics",
      "video": "https://drive.google.com/uc?export=preview&id=1MMI5RbZdgjhQOavn7Slxzysb2RQ_Tza1"
    },
    "forum": "281",
    "id": "281",
    "pic_id": "https://drive.google.com/open?id=1IjadVnjYR-DHiQV8XFQ7aAAJOCNFHwjS",
    "position": "16",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "6"
  },
  {
    "content": {
      "TLDR": "Recent advances in deep learning not only facilitate the implementation of zero-shot singing voice synthesis (SVS) and singing voice conversion (SVC) tasks but also provide the opportunity to unify these two tasks into one generalized model. In this paper, we propose such a model that generate the singing voice of any target singer from any source singing content in either text or audio format. The model incorporates self-supervised joint training of the phonetic encoder and the acoustic encoder, with an audio-to-phoneme alignment process in each training step, such that these encoders map the audio and text data respectively into a shared, temporally aligned, and singer agnostic latent space. The target singer\u2019s latent representations encoded at different granularity levels are all trained to match the source latent representations sequentially with the attention mechanisms in the decoding stage. This enables the model to generate unseen target singer\u2019s voice with fine-grained resolution from either text or audio sources. Both objective and subjective experiments confirmed that the proposed model is competitive with the state-of-the-art SVC and SVS methods.",
      "abstract": "Recent advances in deep learning not only facilitate the implementation of zero-shot singing voice synthesis (SVS) and singing voice conversion (SVC) tasks but also provide the opportunity to unify these two tasks into one generalized model. In this paper, we propose such a model that generate the singing voice of any target singer from any source singing content in either text or audio format. The model incorporates self-supervised joint training of the phonetic encoder and the acoustic encoder, with an audio-to-phoneme alignment process in each training step, such that these encoders map the audio and text data respectively into a shared, temporally aligned, and singer agnostic latent space. The target singer\u2019s latent representations encoded at different granularity levels are all trained to match the source latent representations sequentially with the attention mechanisms in the decoding stage. This enables the model to generate unseen target singer\u2019s voice with fine-grained resolution from either text or audio sources. Both objective and subjective experiments confirmed that the proposed model is competitive with the state-of-the-art SVC and SVS methods.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1sDYV4eSrxppXdXHjsHriKtQ7hSQTcNcY)</b>",
      "authors": [
        "Wu, Jui-Te",
        " Wang, Jun-You",
        " Jang, Jyh-Shing Roger",
        " Su, Li*"
      ],
      "authors_and_affil": [
        "Jui-Te Wu (NTU-AS Data Science Degree Program, National Taiwan University, Taiwan)",
        " Jun-You Wang (Department of Computer Science and Information Engineering, National Taiwan University, Taiwan)",
        " Jyh-Shing Roger Jang (NTU-AS Data Science Degree Program, National Taiwan University, Taiwan, Department of Computer Science and Information Engineering, National Taiwan University, Taiwan)",
        " Li Su (NTU-AS Data Science Degree Program, National Taiwan University, Taiwan, Institute of Information Science, Academia Sinica, Taiwan)*"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CGCV0NSZ",
      "day": "4",
      "keywords": [
        "MIR tasks -> music synthesis and transformation",
        " Musical features and properties -> timbre, instrumentation, and singing voice",
        "MIR tasks -> music generation"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Virtually",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000097.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1BL5gPSod8aX-uNeCwpZfPT92T6_EfxGp",
      "session": [
        "7"
      ],
      "slack_channel": "p7-01-su",
      "title": "A unified model for zero-shot singing voice conversion and synthesis",
      "video": "https://drive.google.com/uc?export=preview&id=1sDYV4eSrxppXdXHjsHriKtQ7hSQTcNcY"
    },
    "forum": "156",
    "id": "156",
    "pic_id": "https://drive.google.com/open?id=1aQqBJoPXKXWcyTLnThakkirPRCfUIW6O",
    "position": "01",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "7"
  },
  {
    "content": {
      "TLDR": "Deep generative neural networks have been successful in tasks such as composing novel music and rendering expressive performance. Controllability is essential for building creative tools from such models. Recent work in this area has focused on disentangled latent space representations, but this is only part of the solution. Efficient control of semantic attributes must handle non-linearities and holes that occur in latent spaces, whilst minimising unwanted changes to other attributes. This paper introduces SeNT-Gen, a neural traversal algorithm that uses a secondary neural network to model the complex relationships between latent codes and musical attributes. This enables precise editing of semantic attributes that adapts to context. We demonstrate the method using the dMelodies dataset, and show strong performance for several VAE models.",
      "abstract": "Deep generative neural networks have been successful in tasks such as composing novel music and rendering expressive performance. Controllability is essential for building creative tools from such models. Recent work in this area has focused on disentangled latent space representations, but this is only part of the solution. Efficient control of semantic attributes must handle non-linearities and holes that occur in latent spaces, whilst minimising unwanted changes to other attributes. This paper introduces SeNT-Gen, a neural traversal algorithm that uses a secondary neural network to model the complex relationships between latent codes and musical attributes. This enables precise editing of semantic attributes that adapts to context. We demonstrate the method using the dMelodies dataset, and show strong performance for several VAE models.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1g0hfLPDSrY5tHMNUzKEJMMKjWwwlYTvo)</b>",
      "authors": [
        "Greenhill, Stewart*",
        " Abdolshah, Majid",
        " Le, Vuong",
        " Gupta, Sunil",
        " Venkatesh, Svetha"
      ],
      "authors_and_affil": [
        "Stewart Greenhill (Applied Artificial Intelligence Institute, Deakin University, Australia)*",
        " Majid Abdolshah (Applied Artificial Intelligence Institute, Deakin University, Australia)",
        " Vuong Le (Applied Artificial Intelligence Institute, Deakin University, Australia)",
        " Sunil Gupta (Applied Artificial Intelligence Institute, Deakin University, Australia)",
        " Svetha Venkatesh (Applied Artificial Intelligence Institute, Deakin University, Australia)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CY0N0YL9",
      "day": "4",
      "keywords": [
        " MIR tasks -> music generation",
        "Domain knowledge -> machine learning/artificial intelligence for music",
        " MIR fundamentals and methodology -> symbolic music processing",
        "Human-centered MIR -> human-computer interaction"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Virtually",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000098.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1VE7uZf3V3GZZ-noD7BPZwcjn1AZuO7bt",
      "session": [
        "7"
      ],
      "slack_channel": "p7-02-greenhill",
      "title": "Semantic Control of Generative Musical Attributes",
      "video": "https://drive.google.com/uc?export=preview&id=1g0hfLPDSrY5tHMNUzKEJMMKjWwwlYTvo"
    },
    "forum": "168",
    "id": "168",
    "pic_id": "https://drive.google.com/open?id=1i7t5YjaI3T3odMJxnVerf1qYCD8AWxpN",
    "position": "02",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "7"
  },
  {
    "content": {
      "TLDR": "This paper revisits the idea of music representation learning supervised by editorial metadata, contributing to the state of the art in two ways. First, we exploit the public editorial metadata available on Discogs, an extensive community-maintained music database containing information about artists, releases, and record labels. Second, we use a contrastive learning setup based on COLA, different from previous systems based on triplet loss. We train models targeting several associations derived from the metadata and experiment with stacked combinations of learned representations, evaluating them on standard music classification tasks. Additionally, we consider learning all the associations jointly in a multi-task setup. We show that it is possible to improve the performance of current self-supervised models by using inexpensive metadata commonly available in music collections, producing representations comparable to those learned on classification setups. We find that the resulting representations based on editorial metadata outperform a system trained with music style tags available in the same large-scale dataset, which motivates further research using this type of supervision. Additionally, we give insights on how to preprocess Discogs metadata to build training objectives and provide public pre-trained models.",
      "abstract": "This paper revisits the idea of music representation learning supervised by editorial metadata, contributing to the state of the art in two ways. First, we exploit the public editorial metadata available on Discogs, an extensive community-maintained music database containing information about artists, releases, and record labels. Second, we use a contrastive learning setup based on COLA, different from previous systems based on triplet loss. We train models targeting several associations derived from the metadata and experiment with stacked combinations of learned representations, evaluating them on standard music classification tasks. Additionally, we consider learning all the associations jointly in a multi-task setup. We show that it is possible to improve the performance of current self-supervised models by using inexpensive metadata commonly available in music collections, producing representations comparable to those learned on classification setups. We find that the resulting representations based on editorial metadata outperform a system trained with music style tags available in the same large-scale dataset, which motivates further research using this type of supervision. Additionally, we give insights on how to preprocess Discogs metadata to build training objectives and provide public pre-trained models.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1nEt3Z7jRysFimuJRPF0NZmeMY6qlXXWs)</b>",
      "authors": [
        "Alonso-Jim\u00e9nez, Pablo*",
        " Serra, Xavier",
        " Bogdanov, Dmitry"
      ],
      "authors_and_affil": [
        "Pablo Alonso-Jim\u00e9nez (Music Technology Group, Universitat Pompeu Fabra)*",
        " Xavier Serra (Music Technology Group, Universitat Pompeu Fabra)",
        " Dmitry Bogdanov (Music Technology Group, Universitat Pompeu Fabra)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CGCVKU69",
      "day": "4",
      "keywords": [
        " Musical features and properties -> musical affect, emotion and mood",
        " MIR tasks -> similarity metrics",
        " Musical features and properties -> musical style and genre",
        "Musical features and properties -> representations of music",
        "MIR fundamentals and methodology -> metadata, tags, linked data, and semantic web",
        " MIR tasks -> automatic classification"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Bengaluru",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000099.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1Q-OGckt9Dzd2R9cSzved_8uRG0xA0lZ3",
      "session": [
        "7"
      ],
      "slack_channel": "p7-03-alonso-jim\u00e9nez",
      "title": "Music Representation Learning Based on Editorial Metadata from Discogs",
      "video": "https://drive.google.com/uc?export=preview&id=1nEt3Z7jRysFimuJRPF0NZmeMY6qlXXWs"
    },
    "forum": "243",
    "id": "243",
    "pic_id": "https://drive.google.com/open?id=12uQCBAM3xQi9miVwq6GsAa2bossFb2cK",
    "position": "03",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "7"
  },
  {
    "content": {
      "TLDR": "This paper proposes a novel Transformer-based model for music score infilling, to generate a music passage that fills in the gap between given past and future contexts. While existing infilling approaches can generate a passage that connects smoothly locally with the given contexts, they do not take into account the musical form or structure of the music and may therefore generate overly smooth results. To address this issue, we propose a structure-aware conditioning approach that employs a novel attention-selecting module to supply user-provided structure-related information to the Transformer for infilling. With both objective and subjective evaluations, we show that the proposed model can harness the structural information effectively and generate melodies in the style of pop of higher quality than the two existing structure-agnostic infilling models.",
      "abstract": "This paper proposes a novel Transformer-based model for music score infilling, to generate a music passage that fills in the gap between given past and future contexts. While existing infilling approaches can generate a passage that connects smoothly locally with the given contexts, they do not take into account the musical form or structure of the music and may therefore generate overly smooth results. To address this issue, we propose a structure-aware conditioning approach that employs a novel attention-selecting module to supply user-provided structure-related information to the Transformer for infilling. With both objective and subjective evaluations, we show that the proposed model can harness the structural information effectively and generate melodies in the style of pop of higher quality than the two existing structure-agnostic infilling models.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1zdgG8FxH4w_tnBdKcxbIU2EOC2XqdRKR)</b>",
      "authors": [
        "Tan, Chih-Pin*",
        " Su, Alvin W Y",
        " Yang, Yi-Hsuan"
      ],
      "authors_and_affil": [
        "Chih-Pin Tan (National Cheng Kung University, Academia Sinica)*",
        " Alvin W Y Su (National Cheng Kung University)",
        " Yi-Hsuan Yang (Academia Sinica, Taiwan AI Labs)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CGCTVCP7",
      "day": "4",
      "keywords": [
        "Domain knowledge -> machine learning/artificial intelligence for music",
        " MIR fundamentals and methodology -> symbolic music processing",
        " Musical features and properties -> structure, segmentation, and form",
        " MIR tasks -> music generation",
        "Applications -> music composition"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Virtually",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000100.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1QirWM-r5jIpZG40qeOcsAuOZsgtokM9S",
      "session": [
        "7"
      ],
      "slack_channel": "p7-04-tan",
      "title": "Melody Infilling with User-Provided Structural Context",
      "video": "https://drive.google.com/uc?export=preview&id=1zdgG8FxH4w_tnBdKcxbIU2EOC2XqdRKR"
    },
    "forum": "59",
    "id": "59",
    "pic_id": "https://drive.google.com/open?id=1917A7wfbJZPRKUrzRzKE3yn-yoxMhKRm",
    "position": "04",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "7"
  },
  {
    "content": {
      "TLDR": "Melody tracks are worthy of special attention in the field of symbolic music information retrieval (MIR) because they contribute more towards music perception than many other musical components. However, many existing symbolic MIR systems neglect melody track identification (MTI) and are thus less effective. Existing MTI methods are also not robust and perform poorly on MIDI files representing music of unusual genres, arrangements, or formats. To address this problem, we propose a CNN-Transformer-based MTI model designed to robustly identify a single melody track for a given MIDI file. As this process can take a sizable amount of time for long songs, we also use a sparse Transformer to speed up attention computation. Our experiments show that our proposed model outperforms state-of-the-art (SOTA) algorithms in accuracy and can also benefit downstream MIR tasks.",
      "abstract": "Melody tracks are worthy of special attention in the field of symbolic music information retrieval (MIR) because they contribute more towards music perception than many other musical components. However, many existing symbolic MIR systems neglect melody track identification (MTI) and are thus less effective. Existing MTI methods are also not robust and perform poorly on MIDI files representing music of unusual genres, arrangements, or formats. To address this problem, we propose a CNN-Transformer-based MTI model designed to robustly identify a single melody track for a given MIDI file. As this process can take a sizable amount of time for long songs, we also use a sparse Transformer to speed up attention computation. Our experiments show that our proposed model outperforms state-of-the-art (SOTA) algorithms in accuracy and can also benefit downstream MIR tasks.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1_FbTmND6no8vptyIFsC9rjI6deCjxDTp)</b>",
      "authors": [
        "Ma, Xichu",
        " Liu, Xiao",
        " Zhang, Bowen",
        " Wang, Ye*"
      ],
      "authors_and_affil": [
        "Xichu Ma (National University of Singapore)",
        " Xiao Liu (National University of Singapore)",
        " Bowen Zhang (National University of Singapore)",
        " Ye Wang (National University of Singapore)*"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CGCTJME1",
      "day": "4",
      "keywords": [
        " Musical features and properties -> melody and motives",
        " Evaluation, datasets, and reproducibility -> novel datasets and use cases",
        " MIR tasks -> music transcription and annotation",
        "MIR fundamentals and methodology -> symbolic music processing",
        "Domain knowledge -> cognitive MIR",
        " Domain knowledge -> machine learning/artificial intelligence for music"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Bengaluru",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000101.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1k_xWVpieuYbnYrqcKtBMJnsyr4DcU7qr",
      "session": [
        "7"
      ],
      "slack_channel": "p7-05-wang",
      "title": "Robust Melody Track Identification in Symbolic Music",
      "video": "https://drive.google.com/uc?export=preview&id=1_FbTmND6no8vptyIFsC9rjI6deCjxDTp"
    },
    "forum": "7",
    "id": "7",
    "pic_id": "https://drive.google.com/open?id=1ruzDpJPC5nQGbRYWyvqYmQQa9Kz4r-X8",
    "position": "05",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "7"
  },
  {
    "content": {
      "TLDR": "Evolutionary studies have become a dominant thread in the analysis of large audio collections. Such corpora usually consist of musical pieces by various composers or bands and the studies usually focus on identifying general historical trends in harmonic content or music production techniques. In this paper we present a comparable study that examines the music of a single band whose publicly available live recordings span three decades. We first discuss the opportunities and challenges faced when working with single-artist and live-music datasets and introduce solutions for audio feature validation and outlier detection. We then investigate how individual songs vary over time and identify general performance trends using a new approach based on relative feature values, which improves accuracy for features with a large variance. Finally, we validate our findings by juxtaposing them with descriptions posted in online forums by experienced listeners of the band's large following.",
      "abstract": "Evolutionary studies have become a dominant thread in the analysis of large audio collections. Such corpora usually consist of musical pieces by various composers or bands and the studies usually focus on identifying general historical trends in harmonic content or music production techniques. In this paper we present a comparable study that examines the music of a single band whose publicly available live recordings span three decades. We first discuss the opportunities and challenges faced when working with single-artist and live-music datasets and introduce solutions for audio feature validation and outlier detection. We then investigate how individual songs vary over time and identify general performance trends using a new approach based on relative feature values, which improves accuracy for features with a large variance. Finally, we validate our findings by juxtaposing them with descriptions posted in online forums by experienced listeners of the band's large following.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1uAL5bgP8TUTwTyMguU2bOLfHMUR7G_hI)</b>",
      "authors": [
        "Thalmann, Florian*",
        " Nakamura, Eita",
        " Yoshii, Kazuyoshi"
      ],
      "authors_and_affil": [
        "Florian Thalmann (Graduate School of Informatics, Kyoto University, Japan)*",
        " Eita Nakamura (Graduate School of Informatics, Kyoto University, Japan)",
        " Kazuyoshi Yoshii (Graduate School of Informatics, Kyoto University, Japan)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CMQU1H8U",
      "day": "4",
      "keywords": [
        " Evaluation, datasets, and reproducibility -> MIR tasks",
        "Applications -> digital libraries and archives",
        " Evaluation, datasets, and reproducibility -> novel datasets and use cases",
        " Applications -> performance, and production",
        "Domain knowledge -> computational music theory and musicology"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Virtually",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000102.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1DhB8xAaKrGiFcr2MtjLFHg34QWcnOAuu",
      "session": [
        "7"
      ],
      "slack_channel": "p7-06-thalmann",
      "title": "Tracking the Evolution of a Band's Live Performances over Decades",
      "video": "https://drive.google.com/uc?export=preview&id=1uAL5bgP8TUTwTyMguU2bOLfHMUR7G_hI"
    },
    "forum": "12",
    "id": "12",
    "pic_id": "https://drive.google.com/open?id=1r_zMQ2KOCtuHusNoEpiRSdO8ddrcLTTE",
    "position": "06",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "7"
  },
  {
    "content": {
      "TLDR": "Recent years have seen considerable advances in audio synthesis with deep generative models. However, the state-of-the-art is very difficult to quantify; different studies often use different evaluation methodologies and different metrics when reporting results, making a direct comparison to other systems difficult if not impossible. Furthermore, the perceptual relevance and meaning of the reported metrics in most cases unknown, prohibiting any conclusive insights with respect to practical usability and audio quality. This paper presents a study that investigates state-of-the-art approaches side-by-side with (i) a set of previously proposed objective metrics for audio reconstruction, and with (ii) a listening study. The results indicate that currently used objective metrics are insufficient to describe the perceptual quality of current systems.",
      "abstract": "Recent years have seen considerable advances in audio synthesis with deep generative models. However, the state-of-the-art is very difficult to quantify; different studies often use different evaluation methodologies and different metrics when reporting results, making a direct comparison to other systems difficult if not impossible. Furthermore, the perceptual relevance and meaning of the reported metrics in most cases unknown, prohibiting any conclusive insights with respect to practical usability and audio quality. This paper presents a study that investigates state-of-the-art approaches side-by-side with (i) a set of previously proposed objective metrics for audio reconstruction, and with (ii) a listening study. The results indicate that currently used objective metrics are insufficient to describe the perceptual quality of current systems.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1joOvb5Bd7qxAYR2GlteXaKU9pyuXr8KE)</b>",
      "authors": [
        "Vinay, Ashvala*",
        " Lerch, Alexander"
      ],
      "authors_and_affil": [
        "Ashvala Vinay (Center for Music Technology, Georgia Institute of Technology)*",
        " Alexander Lerch (Center for Music Technology, Georgia Institue of Technology)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CKBAAB42",
      "day": "4",
      "keywords": [
        " MIR tasks -> music synthesis and transformation",
        "Evaluation, datasets, and reproducibility -> evaluation methodology",
        "Evaluation, datasets, and reproducibility"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Bengaluru",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000103.pdf",
      "poster_pdf": "https://drive.google.com/open?id=13DiY5H3CC_5P4QrXjT-u-zNktZsntn1m",
      "session": [
        "7"
      ],
      "slack_channel": "p7-07-vinay",
      "title": "Evaluating Generative Audio Systems and Their Metrics",
      "video": "https://drive.google.com/uc?export=preview&id=1joOvb5Bd7qxAYR2GlteXaKU9pyuXr8KE"
    },
    "forum": "308",
    "id": "308",
    "pic_id": "https://drive.google.com/open?id=1To34pJg900ZjGQis0n6C4a2E7amV3eP7",
    "position": "07",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "7"
  },
  {
    "content": {
      "TLDR": "Labeling and maintaining a commercial sound effects library is a time-consuming task exacerbated by databases that continually grow in size and undergo taxonomy updates. Moreover, sound search and taxonomy creation are complicated by non-uniform metadata, an unrelenting problem even with the introduction of a new industry standard, the Universal Category System. To address these problems and overcome dataset-dependent limitations that inhibit the successful training of deep learning models, we pursue representation learning to train generalized embeddings that can be used for a wide variety of sound effects libraries and are a taxonomy-agnostic representation of sound. We show that a task-specific but dataset-independent representation can successfully address data issues such as class imbalance, inconsistent class labels, and insufficient dataset size, outperforming established representations such as OpenL3. Detailed experimental results show the impact of metric learning approaches and different cross-dataset training methods on representational effectiveness.",
      "abstract": "Labeling and maintaining a commercial sound effects library is a time-consuming task exacerbated by databases that continually grow in size and undergo taxonomy updates. Moreover, sound search and taxonomy creation are complicated by non-uniform metadata, an unrelenting problem even with the introduction of a new industry standard, the Universal Category System. To address these problems and overcome dataset-dependent limitations that inhibit the successful training of deep learning models, we pursue representation learning to train generalized embeddings that can be used for a wide variety of sound effects libraries and are a taxonomy-agnostic representation of sound. We show that a task-specific but dataset-independent representation can successfully address data issues such as class imbalance, inconsistent class labels, and insufficient dataset size, outperforming established representations such as OpenL3. Detailed experimental results show the impact of metric learning approaches and different cross-dataset training methods on representational effectiveness.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1EJsKWAm3a3JCiTGhYx1XUokQwjSvViE6)</b>",
      "authors": [
        "Ma, Alison B*",
        " Lerch, Alexander"
      ],
      "authors_and_affil": [
        "Alison B Ma (Music Informatics Group, Georgia Institute of Technology)*",
        " Alexander Lerch (Music Informatics Group, Georgia Institute of Technology)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CGCTUN13",
      "day": "4",
      "keywords": [
        "MIR tasks -> automatic classification",
        "Applications -> music retrieval systems",
        " Domain knowledge -> representations of music",
        " MIR tasks -> indexing and querying",
        " Domain knowledge -> machine learning/artificial intelligence for music",
        " Musical features and properties -> representations of music"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Virtually",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000104.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1L7ZphOctH5PbeJFUlwRlcIxD_by_ELHc",
      "session": [
        "7"
      ],
      "slack_channel": "p7-08-ma",
      "title": "Representation Learning for the Automatic Indexing of Sound Effects Libraries",
      "video": "https://drive.google.com/uc?export=preview&id=1EJsKWAm3a3JCiTGhYx1XUokQwjSvViE6"
    },
    "forum": "53",
    "id": "53",
    "pic_id": "https://drive.google.com/open?id=1SySGsZHnjCCG5fM87XeVEVxC8e24MdFO",
    "position": "08",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "7"
  },
  {
    "content": {
      "TLDR": "Current approaches for explaining deep learning systems applied to musical data provide results in a low-level feature space, e.g., by highlighting potentially relevant time-frequency bins in a spectrogram or time-pitch bins in a piano roll. This can be difficult to understand, particularly for musicologists without technical knowledge. To address this issue, we focus on more human-friendly explanations based on high-level musical concepts. Our research targets trained systems (post-hoc explanations) and explores two approaches: a supervised one, where the user can define a musical concept and test if it is relevant to the system; and an unsupervised one, where musical excerpts containing relevant concepts are automatically selected and given to the user for interpretation. We demonstrate both techniques on an existing symbolic composer classification system, showcase their potential, and highlight their intrinsic limitations.",
      "abstract": "Current approaches for explaining deep learning systems applied to musical data provide results in a low-level feature space, e.g., by highlighting potentially relevant time-frequency bins in a spectrogram or time-pitch bins in a piano roll. This can be difficult to understand, particularly for musicologists without technical knowledge. To address this issue, we focus on more human-friendly explanations based on high-level musical concepts. Our research targets trained systems (post-hoc explanations) and explores two approaches: a supervised one, where the user can define a musical concept and test if it is relevant to the system; and an unsupervised one, where musical excerpts containing relevant concepts are automatically selected and given to the user for interpretation. We demonstrate both techniques on an existing symbolic composer classification system, showcase their potential, and highlight their intrinsic limitations.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1JQR--TXENiS7hXgS169ktY1NeSnckSiY)</b>",
      "authors": [
        "Foscarin, Francesco",
        " Hoedt, Katharina*",
        " Praher, Verena",
        " Flexer, Arthur",
        " Widmer, Gerhard"
      ],
      "authors_and_affil": [
        "Francesco Foscarin (Institute of Computational Perception, Johannes Kepler University Linz, Austria)",
        " Katharina Hoedt (Institute of Computational Perception, Johannes Kepler University Linz, Austria)*",
        " Verena Praher (Institute of Computational Perception, Johannes Kepler University Linz, Austria)",
        " Arthur Flexer (Institute of Computational Perception, Johannes Kepler University Linz, Austria)",
        " Gerhard Widmer (Institute of Computational Perception, Johannes Kepler University Linz, Austria, LIT AI Lab, Linz Institute of Technology, Austria)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CKB87ESW",
      "day": "4",
      "keywords": [
        "Domain knowledge -> machine learning/artificial intelligence for music",
        " MIR fundamentals and methodology -> symbolic music processing",
        " Musical features and properties -> musical style and genre",
        "Domain knowledge -> computational music theory and musicology",
        " MIR tasks -> automatic classification"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Bengaluru",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000105.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1t-4k-R3PCNSYQaxR1-AGLuss9M7nPnJH",
      "session": [
        "7"
      ],
      "slack_channel": "p7-09-hoedt",
      "title": "Concept-Based Techniques for \"Musicologist-Friendly\" Explanations in Deep Music Classifiers",
      "video": "https://drive.google.com/uc?export=preview&id=1JQR--TXENiS7hXgS169ktY1NeSnckSiY"
    },
    "forum": "67",
    "id": "67",
    "pic_id": "https://drive.google.com/open?id=1Zl2_LH5Qda6dPIte_zJW6_I2XJFx6fMh",
    "position": "09",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "7"
  },
  {
    "content": {
      "TLDR": "The aim of lyrics-based genre recognition is to automatically determine the genre of a given song based on its lyrics. Previous approaches for this task have commonly used textual features extracted from the entirety of a song's lyrics, neglecting the inherent structure of lyrics consisting of, for instance, verses and choruses. Therefore, we pose the hypothesis that features extracted from different parts of the lyrics can have significantly different predictive power. To test this hypothesis, we perform a series of experiments to determine whether models trained on features taken from verses and choruses perform differently for genre recognition. Our experiments indeed confirm our hypothesis, showing that generally, using features extracted from verses leads to higher performance than features extracted from choruses. Digging deeper, we found that this is especially true for pop and rap songs. Rock songs show the opposite effect, with features extracted from choruses performing better than those taken from verses.",
      "abstract": "The aim of lyrics-based genre recognition is to automatically determine the genre of a given song based on its lyrics. Previous approaches for this task have commonly used textual features extracted from the entirety of a song's lyrics, neglecting the inherent structure of lyrics consisting of, for instance, verses and choruses. Therefore, we pose the hypothesis that features extracted from different parts of the lyrics can have significantly different predictive power. To test this hypothesis, we perform a series of experiments to determine whether models trained on features taken from verses and choruses perform differently for genre recognition. Our experiments indeed confirm our hypothesis, showing that generally, using features extracted from verses leads to higher performance than features extracted from choruses. Digging deeper, we found that this is especially true for pop and rap songs. Rock songs show the opposite effect, with features extracted from choruses performing better than those taken from verses.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=185Rq0MoJFw_uwaQbNknlj8kHgEqnQaa3)</b>",
      "authors": [
        "Mayerl, Maximilian*",
        " Brandl, Stefan",
        " Specht, G\u00fcnther",
        " Schedl, Markus",
        " Zangerle, Eva"
      ],
      "authors_and_affil": [
        "Maximilian Mayerl (Department of Computer Science, Leopold-Franzens-Universit\u00e4t Innsbruck, Austria)*",
        " Stefan Brandl (Institute of Computational Perception, Johannes Kepler Universit\u00e4t Linz, Austria, Human-centered AI Group, AI Lab, Linz Institute of Technology, Austria)",
        " G\u00fcnther Specht (Department of Computer Science, Leopold-Franzens-Universit\u00e4t Innsbruck, Austria)",
        " Markus Schedl (Institute of Computational Perception, Johannes Kepler Universit\u00e4t Linz, Austria, Human-centered AI Group, AI Lab, Linz Institute of Technology, Austria)",
        " Eva Zangerle (Department of Computer Science, Leopold-Franzens-Universit\u00e4t Innsbruck, Austria)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CCP29X6J",
      "day": "4",
      "keywords": [
        "Domain knowledge -> machine learning/artificial intelligence for music",
        "MIR tasks -> automatic classification",
        " Musical features and properties -> musical style and genre",
        " Musical features and properties -> structure, segmentation, and form",
        " MIR fundamentals and methodology -> lyrics and other textual data"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Virtually",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000106.pdf",
      "poster_pdf": "https://drive.google.com/open?id=131Jy3tK0QjPWhBP6ujYRX6UeCFx2gSZ-",
      "session": [
        "7"
      ],
      "slack_channel": "p7-10-mayerl",
      "title": "Verse versus Chorus: Structure-aware Feature Extraction for Lyrics-based Genre Recognition",
      "video": "https://drive.google.com/uc?export=preview&id=185Rq0MoJFw_uwaQbNknlj8kHgEqnQaa3"
    },
    "forum": "92",
    "id": "92",
    "pic_id": "https://drive.google.com/open?id=1q_GOzN6TfcbPvrh-izQyB-x6osnQekOB",
    "position": "10",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "7"
  },
  {
    "content": {
      "TLDR": "Automatic speech recognition (ASR) has progressed significantly in recent years due to the emergence of large-scale datasets and the self-supervised learning (SSL) paradigm. However, as its counterpart problem in the singing domain, the development of automatic lyric transcription (ALT) suffers from limited data and degraded intelligibility of sung lyrics. To fill in the performance gap between ALT and ASR, we attempt to exploit the similarities between speech and singing. In this work, we propose a transfer-learning-based ALT solution that takes advantage of these similarities by adapting wav2vec 2.0, an SSL ASR model, to the singing domain. We maximize the effectiveness of transfer learning by exploring the influence of different transfer starting points. We further enhance the performance by extending the original CTC model to a hybrid CTC/attention model. Our method surpasses previous approaches by a large margin on various ALT benchmark datasets. Further experiments show that, with even a tiny proportion of training data, our method still achieves competitive performance.",
      "abstract": "Automatic speech recognition (ASR) has progressed significantly in recent years due to the emergence of large-scale datasets and the self-supervised learning (SSL) paradigm. However, as its counterpart problem in the singing domain, the development of automatic lyric transcription (ALT) suffers from limited data and degraded intelligibility of sung lyrics. To fill in the performance gap between ALT and ASR, we attempt to exploit the similarities between speech and singing. In this work, we propose a transfer-learning-based ALT solution that takes advantage of these similarities by adapting wav2vec 2.0, an SSL ASR model, to the singing domain. We maximize the effectiveness of transfer learning by exploring the influence of different transfer starting points. We further enhance the performance by extending the original CTC model to a hybrid CTC/attention model. Our method surpasses previous approaches by a large margin on various ALT benchmark datasets. Further experiments show that, with even a tiny proportion of training data, our method still achieves competitive performance.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1J02o1LVVDJPfS9xwjtyrktHLDnvdyqWx)</b>",
      "authors": [
        "Ou, Longshen*",
        " Gu, Xiangming",
        " Wang, Ye"
      ],
      "authors_and_affil": [
        "Longshen Ou (School of Computing, National University of Singapore)*",
        " Xiangming Gu (School of Computing, National University of Singapore)",
        " Ye Wang (School of Computing, National University of Singapore)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CY0MB30R",
      "day": "4",
      "keywords": [
        " Musical features and properties -> timbre, instrumentation, and singing voice",
        "Applications -> music retrieval systems",
        " MIR tasks -> music transcription and annotation",
        " MIR fundamentals and methodology -> web mining, and natural language processing",
        "MIR fundamentals and methodology -> lyrics and other textual data",
        " Domain knowledge -> machine learning/artificial intelligence for music"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Bengaluru",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000107.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1tDh_pupJdv1rGg5ImIf7nWHJs4sh4FIA",
      "session": [
        "7"
      ],
      "slack_channel": "p7-11-ou",
      "title": "Transfer Learning of wav2vec 2.0 for Automatic Lyric Transcription",
      "video": "https://drive.google.com/uc?export=preview&id=1J02o1LVVDJPfS9xwjtyrktHLDnvdyqWx"
    },
    "forum": "94",
    "id": "94",
    "pic_id": "https://drive.google.com/open?id=1aJwe52UdKRS_Dc0pn_uVIAwazJX14ErS",
    "position": "11",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "7"
  },
  {
    "content": {
      "TLDR": "Automated computational analysis schemes for Western classical music analysis based on form and hierarchical structure have not received much attention in the literature so far. One reason, of course, is the paucity of labeled datasets \u2014 which, if available, could be used to train machine learning approaches. Dataset curation cannot be crowdsourced; one needs trained musicians to devote sizable effort to carry out such annotations. Further, such an analysis is not simple for beginners; obtaining labeled data that can capture the nuances of a musician's reasoning acquired over years of practice is fraught with challenges. To this end, we provide a system for computational analysis of classical music, both for machine learning and music researchers. First, we introduce a labeled dataset containing 200 classical music pieces annotated by form and phrases. Then, by leveraging this dataset, we show that deep learning-based methods can be used to learn Form Classification as well as Phrase Analysis and Classification, for which few (if any) results have been reported yet. Taken together, we provide the community with a unique dataset as well as a toolkit needed to analyze classical music structure, which can be used or extended to drive applications in both commercial and educational settings.",
      "abstract": "Automated computational analysis schemes for Western classical music analysis based on form and hierarchical structure have not received much attention in the literature so far. One reason, of course, is the paucity of labeled datasets \u2014 which, if available, could be used to train machine learning approaches. Dataset curation cannot be crowdsourced; one needs trained musicians to devote sizable effort to carry out such annotations. Further, such an analysis is not simple for beginners; obtaining labeled data that can capture the nuances of a musician's reasoning acquired over years of practice is fraught with challenges. To this end, we provide a system for computational analysis of classical music, both for machine learning and music researchers. First, we introduce a labeled dataset containing 200 classical music pieces annotated by form and phrases. Then, by leveraging this dataset, we show that deep learning-based methods can be used to learn Form Classification as well as Phrase Analysis and Classification, for which few (if any) results have been reported yet. Taken together, we provide the community with a unique dataset as well as a toolkit needed to analyze classical music structure, which can be used or extended to drive applications in both commercial and educational settings.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=18EScDB4ill2V1GnNDaIY7Q-A0WNeTfX4)</b>",
      "authors": [
        "Szelogowski, Daniel*",
        " Mukherjee, Lopamudra",
        " Whitcomb, Benjamin"
      ],
      "authors_and_affil": [
        "Daniel Szelogowski (University of Wisconsin - Whitewater)*",
        " Lopamudra Mukherjee (University of Wisconsin - Whitewater)",
        " Benjamin Whitcomb (University of Wisconsin - Whitewater)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04C4QQ3295",
      "day": "4",
      "keywords": [
        "Domain knowledge -> machine learning/artificial intelligence for music",
        " Evaluation, datasets, and reproducibility -> novel datasets and use cases",
        " Musical features and properties -> structure, segmentation, and form",
        " Domain knowledge -> computational music theory and musicology",
        "Applications -> music training and education",
        " MIR tasks -> automatic classification"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Virtually",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000108.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1QkUF-gWX4l1BCbu-JWLE9b10Yy_bjY-3",
      "session": [
        "7"
      ],
      "slack_channel": "p7-12-szelogowski",
      "title": "A Novel Dataset and Deep Learning Benchmark for Classical Music Form Recognition and Analysis",
      "video": "https://drive.google.com/uc?export=preview&id=18EScDB4ill2V1GnNDaIY7Q-A0WNeTfX4"
    },
    "forum": "152",
    "id": "152",
    "pic_id": "https://drive.google.com/open?id=1lmMrdSbxvlxc-tqR6j3qDgo3UBYNca-0",
    "position": "12",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "7"
  },
  {
    "content": {
      "TLDR": "Audio Fingerprinting (AFP) is a well-studied problem in music information retrieval for various use-cases e.g. content-based copy detection, DJ-set monitoring, and music excerpt identification. However, AFP for continuous broadcast monitoring (e.g. for TV & Radio), where music is often in the background, has not received much attention despite its importance to the music industry. In this paper (1) we present BAF, the first public dataset for music monitoring in broadcast. It contains 74 hours of production music from Epidemic Sound and 57 hours of TV audio recordings. Furthermore, BAF provides cross-annotations with exact matching timestamps between Epidemic tracks and TV recordings. Approximately, 80% of the total annotated time is background music. (2) We benchmark BAF with public state-of-the-art AFP systems, together with our proposed baseline PeakFP: a simple, non-scalable AFP algorithm based on spectral peak matching. In this benchmark, none of the algorithms obtain a F1-score above 47%, pointing out that further research is needed to reach the AFP performance levels in other studied use cases. The dataset, baseline, and benchmark framework are open and available for research.",
      "abstract": "Audio Fingerprinting (AFP) is a well-studied problem in music information retrieval for various use-cases e.g. content-based copy detection, DJ-set monitoring, and music excerpt identification. However, AFP for continuous broadcast monitoring (e.g. for TV & Radio), where music is often in the background, has not received much attention despite its importance to the music industry. In this paper (1) we present BAF, the first public dataset for music monitoring in broadcast. It contains 74 hours of production music from Epidemic Sound and 57 hours of TV audio recordings. Furthermore, BAF provides cross-annotations with exact matching timestamps between Epidemic tracks and TV recordings. Approximately, 80% of the total annotated time is background music. (2) We benchmark BAF with public state-of-the-art AFP systems, together with our proposed baseline PeakFP: a simple, non-scalable AFP algorithm based on spectral peak matching. In this benchmark, none of the algorithms obtain a F1-score above 47%, pointing out that further research is needed to reach the AFP performance levels in other studied use cases. The dataset, baseline, and benchmark framework are open and available for research.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=15uRhIXqDebPcLCC3bRqZovkL_kXCoJqn)</b>",
      "authors": [
        "Cort\u00e8s, Guillem*",
        " Ciurana, Alex",
        " Molina, Emilio",
        " Miron, Marius",
        " Meyers, Owen",
        " Six, Joren",
        " Serra, Xavier"
      ],
      "authors_and_affil": [
        "Guillem Cort\u00e8s (BMAT Licensing S.L., Barcelona, MTG, Universitat Pompeu Fabra, Barcelona)*",
        " Alex Ciurana (BMAT Licensing S.L., Barcelona)",
        " Emilio Molina (BMAT Licensing S.L., Barcelona)",
        " Marius Miron (MTG, Universitat Pompeu Fabra, Barcelona)",
        " Owen Meyers (Epidemic Sound, Stockholm)",
        " Joren Six (IPEM, Ghent University, Ghent)",
        " Xavier Serra (MTG, Universitat Pompeu Fabra, Barcelona)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CK87EZJP",
      "day": "4",
      "keywords": [
        " MIR tasks -> fingerprinting",
        " Evaluation, datasets, and reproducibility -> reproducibility",
        "Applications -> music retrieval systems",
        " Evaluation, datasets, and reproducibility -> annotation protocols",
        " MIR tasks -> indexing and querying",
        "Evaluation, datasets, and reproducibility -> novel datasets and use cases"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Bengaluru",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000109.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1f0-EYH2ANBqVEvrBTI30GlU3-Of04UHr",
      "session": [
        "7"
      ],
      "slack_channel": "p7-13-cort\u00e8s",
      "title": "BAF: An audio fingerprinting dataset for broadcast monitoring",
      "video": "https://drive.google.com/uc?export=preview&id=15uRhIXqDebPcLCC3bRqZovkL_kXCoJqn"
    },
    "forum": "228",
    "id": "228",
    "pic_id": "https://drive.google.com/open?id=1svXtCf014Mohn9EVK_fx1Cu0cgKTAMmm",
    "position": "13",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "7"
  },
  {
    "content": {
      "TLDR": "Cadences are complex structures that have been driving music from the beginning of contrapuntal polyphony until today. Detecting such structures is vital for numerous MIR tasks such as musicological analysis, key detection, music segmentation, and others. However, automatic cadence detection remains a challenging task mainly because it involves a combination of high-level musical elements like harmony, voice leading, and rhythm. In this work, we present a graph representation of symbolic scores as an intermediate means to solve the cadence detection task. We approach cadence detection as an imbalanced node classification problem using a Graph Convolutional Network. We obtain results that are at least on par with the state of the art, and we present a model capable of making predictions at multiple levels of granularity, from individual notes to beats, thanks to the fine-grained, note-by-note representation. Moreover, our experiments suggest that graph convolution is able to learn non-local features that assist in cadence detection, freeing us from the need of having to devise specialized features that encode non-local context. We argue that this general approach to modeling musical scores and classification tasks has a number of potential advantages, beyond the specific recognition task presented here.",
      "abstract": "Cadences are complex structures that have been driving music from the beginning of contrapuntal polyphony until today. Detecting such structures is vital for numerous MIR tasks such as musicological analysis, key detection, music segmentation, and others. However, automatic cadence detection remains a challenging task mainly because it involves a combination of high-level musical elements like harmony, voice leading, and rhythm. In this work, we present a graph representation of symbolic scores as an intermediate means to solve the cadence detection task. We approach cadence detection as an imbalanced node classification problem using a Graph Convolutional Network. We obtain results that are at least on par with the state of the art, and we present a model capable of making predictions at multiple levels of granularity, from individual notes to beats, thanks to the fine-grained, note-by-note representation. Moreover, our experiments suggest that graph convolution is able to learn non-local features that assist in cadence detection, freeing us from the need of having to devise specialized features that encode non-local context. We argue that this general approach to modeling musical scores and classification tasks has a number of potential advantages, beyond the specific recognition task presented here.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1s0aQoGik8UHNbfbYkMA22QsRFL_VRc83)</b>",
      "authors": [
        "Karystinaios, Emmanouil*",
        " Widmer, Gerhard"
      ],
      "authors_and_affil": [
        "Emmanouil Karystinaios (Institute of Computational Perception, Johannes Kepler University Linz, Austria)*",
        " Gerhard Widmer (Institute of Computational Perception, Johannes Kepler University Linz, Austria, LIT AI Lab, Linz Institute of Technology, Austria)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CK8741KM",
      "day": "4",
      "keywords": [
        "Musical features and properties -> harmony, chords and tonality",
        "MIR fundamentals and methodology -> symbolic music processing",
        " Musical features and properties -> structure, segmentation, and form",
        " Musical features and properties -> representations of music"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Bengaluru",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000110.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1dNTgTRl7adkUEQ0Q0oLxwNtpRIEwegpE",
      "session": [
        "7"
      ],
      "slack_channel": "p7-14-karystinaios",
      "title": "Cadence Detection in Symbolic Classical Music using Graph Neural Networks.",
      "video": "https://drive.google.com/uc?export=preview&id=1s0aQoGik8UHNbfbYkMA22QsRFL_VRc83"
    },
    "forum": "173",
    "id": "173",
    "pic_id": "https://drive.google.com/open?id=1hyu6MSPAUDVVMy2O1nCxT-jiSRzcruUP",
    "position": "14",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "7"
  },
  {
    "content": {
      "TLDR": "The variational auto-encoder has become a leading framework for symbolic music generation, and a popular research direction is to study how to effectively control the generation process. A straightforward way is to control a model using different conditions during inference. However, in music practice, conditions are usually sequential (rather than simple categorical labels), involving rich information that overlaps with the learned representation. Consequently, the decoder gets confused about whether to \"listen to\" the latent representation or the condition, and sometimes just ignores the condition. To solve this problem, we leverage domain adversarial training to disentangle the representation from condition cues for better control. Specifically, we propose a condition corruption objective that uses the representation to denoise a corrupted condition. Minimized by a discriminator and maximized by the VAE encoder, this objective adversarially induces a condition-invariant representation. In this paper, we focus on the task of melody harmonization to illustrate our idea, while our methodology can be generalized to other controllable generative tasks. Demos and experiments show that our methodology facilitates not only condition-invariant representation learning but also higher-quality controllability compared to baselines.",
      "abstract": "The variational auto-encoder has become a leading framework for symbolic music generation, and a popular research direction is to study how to effectively control the generation process. A straightforward way is to control a model using different conditions during inference. However, in music practice, conditions are usually sequential (rather than simple categorical labels), involving rich information that overlaps with the learned representation. Consequently, the decoder gets confused about whether to \"listen to\" the latent representation or the condition, and sometimes just ignores the condition. To solve this problem, we leverage domain adversarial training to disentangle the representation from condition cues for better control. Specifically, we propose a condition corruption objective that uses the representation to denoise a corrupted condition. Minimized by a discriminator and maximized by the VAE encoder, this objective adversarially induces a condition-invariant representation. In this paper, we focus on the task of melody harmonization to illustrate our idea, while our methodology can be generalized to other controllable generative tasks. Demos and experiments show that our methodology facilitates not only condition-invariant representation learning but also higher-quality controllability compared to baselines.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=15PMfRRlWmV8zeUGbZzxfPbC5dNUui0ze)</b>",
      "authors": [
        "Zhao, Jingwei*",
        " Xia, Gus",
        " Wang, Ye"
      ],
      "authors_and_affil": [
        "Jingwei Zhao (Institute of Data Science, NUS, Integrative Sciences and Engineering Programme, NUS Graduate School)*",
        " Gus Xia (Music X Lab, NYU Shanghai, MBZUAI)",
        " Ye Wang (School of Computing, NUS, Institute of Data Science, NUS, Integrative Sciences and Engineering Programme, NUS Graduate School)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CMQUGT6Y",
      "day": "4",
      "keywords": [
        " MIR fundamentals and methodology -> symbolic music processing",
        "MIR tasks",
        " MIR tasks -> music synthesis and transformation",
        " MIR tasks -> music generation",
        " Domain knowledge -> machine learning/artificial intelligence for music",
        "Applications -> music composition"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Bengaluru",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000111.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1h29abkAL9gGQ6lU3d7MWZQME7Qv5TsJS",
      "session": [
        "7"
      ],
      "slack_channel": "p7-15-zhao",
      "title": "Domain Adversarial Training on Conditional Variational Auto-Encoder for Controllable Music Generation",
      "video": "https://drive.google.com/uc?export=preview&id=15PMfRRlWmV8zeUGbZzxfPbC5dNUui0ze"
    },
    "forum": "73",
    "id": "73",
    "pic_id": "https://drive.google.com/open?id=1oU3Ai680ktJf5S80ZMWoA-e0HmMieXsg",
    "position": "15",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "7"
  },
  {
    "content": {
      "TLDR": "The relationship between perceptual loudness and physical attributes of sound is an important subject in both computer music and psychoacoustics. Early studies of \u201cequal-loudness contour\u201d can trace back to the 1920s and the measured loudness with respect to intensity and frequency has been revised many times since then. However, most studies merely focus on synthesized sound, and the induced theories on natural tones with complex timbre has rarely been justified. To this end, we investigate both theory and applications of natural-tone loudness perception in this paper via modeling piano tone. The theory part contains: 1) an accurate measurement of piano-tone equal-loudness contour of pitches, and 2) a machine-learning model capable of inferring loudness purely based on spectral features trained on human subject measurements. As for the application, we apply our theory to piano control transfer, in which we adjust the MIDI velocities on two different player pianos (in different acoustic environments) to achieve the same perceptual effect. Experiments show that both of our theoretical loudness modeling and the corresponding performance control transfer algorithm significantly outperform their baselines.",
      "abstract": "The relationship between perceptual loudness and physical attributes of sound is an important subject in both computer music and psychoacoustics. Early studies of \u201cequal-loudness contour\u201d can trace back to the 1920s and the measured loudness with respect to intensity and frequency has been revised many times since then. However, most studies merely focus on synthesized sound, and the induced theories on natural tones with complex timbre has rarely been justified. To this end, we investigate both theory and applications of natural-tone loudness perception in this paper via modeling piano tone. The theory part contains: 1) an accurate measurement of piano-tone equal-loudness contour of pitches, and 2) a machine-learning model capable of inferring loudness purely based on spectral features trained on human subject measurements. As for the application, we apply our theory to piano control transfer, in which we adjust the MIDI velocities on two different player pianos (in different acoustic environments) to achieve the same perceptual effect. Experiments show that both of our theoretical loudness modeling and the corresponding performance control transfer algorithm significantly outperform their baselines.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1wrtsC8vVVCiZ0EWm4dmK8-qGRKx4MN6H)</b>",
      "authors": [
        "Qu, Yang*",
        " Qin, Yutian",
        " Chao, Lecheng",
        " Qian, Hangkai",
        " Wang, Ziyu",
        " Xia, Gus"
      ],
      "authors_and_affil": [
        "Yang Qu (Music X Lab, NYU Shanghai, City University of Hong Kong)*",
        " Yutian Qin (Music X Lab, NYU Shanghai, New York University)",
        " Lecheng Chao (Music X Lab, NYU Shanghai)",
        " Hangkai Qian (Music X Lab, NYU Shanghai)",
        " Ziyu Wang (Music X Lab, NYU Shanghai, Mohamed Bin Zayed University of Artificial Intelligence)",
        " Gus Xia (Music X Lab, NYU Shanghai, Mohamed Bin Zayed University of Artificial Intelligence)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CKB9E4F4",
      "day": "4",
      "keywords": [
        " Musical features and properties -> timbre, instrumentation, and singing voice",
        "Applications -> performance, and production",
        " Domain knowledge -> cognitive MIR",
        " MIR fundamentals and methodology -> music signal processing",
        "Domain knowledge -> music acoustics",
        " Domain knowledge -> machine learning/artificial intelligence for music"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Virtually",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000112.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1urtGL6vD4F9q53fyjM31PZi1CCb_R9KO",
      "session": [
        "7"
      ],
      "slack_channel": "p7-16-qu",
      "title": "Modeling perceptual loudness of piano tone: theory and applications",
      "video": "https://drive.google.com/uc?export=preview&id=1wrtsC8vVVCiZ0EWm4dmK8-qGRKx4MN6H"
    },
    "forum": "206",
    "id": "206",
    "pic_id": "https://drive.google.com/open?id=1hLUl4F8WoLbLtUodBp3GPjEVx5a_3mWe",
    "position": "16",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "7"
  },
  {
    "content": {
      "TLDR": "Automatic music tagging systems have once more gained relevance over the last years, not least through their use in applications such as music recommender systems.\n State-of-the-art systems are based on a variant of convolutional neural networks (CNNs) and use some type of time-frequency audio representation as input, in a fitting combination to predict semantic tags available through expert or crowd-based annotation. \n In this work we systematically compare five widely used audio input representations (STFT, CQT, Mel spectrograms, MFCCs, and raw audio waveform) using five established convolutional neural network architectures (MusicCNN, VGG16, ResNet, a Squeeze and Excitation Network (SeNet), as well as a newly proposed MusicCNN variant using dilated convolutions) for the task of music tag prediction.\n Performance of all factor combinations are measured on two distinct tagging datasets, namely MagnaTagATune and MTG Jamendo.\n A two-way ANOVA shows that both input representation and model architecture significantly impact the classification results. Despite differently sized input representations and practical impact on model training, we find that using STFT as input representations provides the best results overall and on specific tag categories (genre, instrument, mood), while other representations show less consistent behavior in these regards.\n Furthermore, the proposed dilated convolutional architecture shows significant performance improvements for all input representations except raw waveform.",
      "abstract": "Automatic music tagging systems have once more gained relevance over the last years, not least through their use in applications such as music recommender systems.\n State-of-the-art systems are based on a variant of convolutional neural networks (CNNs) and use some type of time-frequency audio representation as input, in a fitting combination to predict semantic tags available through expert or crowd-based annotation. \n In this work we systematically compare five widely used audio input representations (STFT, CQT, Mel spectrograms, MFCCs, and raw audio waveform) using five established convolutional neural network architectures (MusicCNN, VGG16, ResNet, a Squeeze and Excitation Network (SeNet), as well as a newly proposed MusicCNN variant using dilated convolutions) for the task of music tag prediction.\n Performance of all factor combinations are measured on two distinct tagging datasets, namely MagnaTagATune and MTG Jamendo.\n A two-way ANOVA shows that both input representation and model architecture significantly impact the classification results. Despite differently sized input representations and practical impact on model training, we find that using STFT as input representations provides the best results overall and on specific tag categories (genre, instrument, mood), while other representations show less consistent behavior in these regards.\n Furthermore, the proposed dilated convolutional architecture shows significant performance improvements for all input representations except raw waveform.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/open?id=1d14BKzh77h8vwRgw7KP9vP-E6P-nAGpV)</b>",
      "authors": [
        "Damb\u00f6ck, Maximilian",
        " Vogl, Richard*",
        " Knees, Peter"
      ],
      "authors_and_affil": [
        "Maximilian Damb\u00f6ck (Faculty of Informatics, TU Wien, Austria)",
        " Richard Vogl (Faculty of Informatics, TU Wien, Austria)*",
        " Peter Knees (Faculty of Informatics, TU Wien, Austria, School of Music, Georgia Institute of Technology, USA)"
      ],
      "channel_url": "https://slack.com/app_redirect?channel=C04CMQWJ0J0",
      "day": "4",
      "keywords": [
        "MIR tasks -> automatic classification",
        "Domain knowledge -> machine learning/artificial intelligence for music",
        " Musical features and properties -> musical style and genre"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Bengaluru",
      "pdf_path": "https://archives.ismir.net/ismir2022/paper/000113.pdf",
      "poster_pdf": "https://drive.google.com/open?id=1Rve9CLZFElIha7IOBKh554H9iklMqpXQ",
      "session": [
        "7"
      ],
      "slack_channel": "p7-17-vogl",
      "title": "On the Impact and Interplay of Input Representations and Network Architectures for Automatic Music Tagging",
      "video": "https://drive.google.com/uc?export=preview&id=1d14BKzh77h8vwRgw7KP9vP-E6P-nAGpV"
    },
    "forum": "317",
    "id": "317",
    "pic_id": "https://drive.google.com/open?id=1usXebeP2k8G1jDAahpoTC1A82Dy9Q77o",
    "position": "17",
    "poster_pdf": "GLTR_poster.pdf",
    "session": "7"
  }
]
