uid,title,day,start_date,start_time,end_time,category,description,organiser,organiser_emails,organiser_affiliation,organiser_bio,image,web_link,slack_channel,channel_url
1,Registration,1,2023-11-05,8:00,9:00,Registration,Time to register at ISMIR2023! ,,,,,,,,
2,T1(M): Analysing Physiological Data Collected During Music Listening: An Introduction,1,2023-11-05,9:00,13:00,Tutorials,"Music has diverse effects on listeners, including inducing emotions, triggering movement or
dancing, and prompting changes in visual attention. These effects are often associated with
psychophysiological responses like changes in heart activity, respiratory rate, and pupil size,
which can themselves be influenced by the cognitive effort exerted during music listening, e.g.,
when engaging with unfamiliar tracks on a web radio for music discovery.

This tutorial aims to introduce psychophysiological data analysis for a broad MIR audience, with
a particular focus on the analysis of heart rate, electrodermal activity and pupillometry data. It
will be structured in three parts. The first part will provide a presentation of psychophysiological
data that we collected in the context of a preliminary study related to music discovery. The
second part will be a hands-on tutorial during which we will guide the participants to remake two
of our data analyses. In the third part, we will assist participants in undertaking their own data
analysis of our data. These analyses will be demonstrated using R and Python.

Our aim with this tutorial is twofold: to promote underrepresented topics in the MIR community,
especially the recognition of induced emotions from physiological data and discovery-oriented
music recommendation; and to encourage researchers from those domains to interact with the
MIR community. The audience we target is therefore relatively large. Participants should,
however, possess sufficient knowledge of R and/or Python and standard statistical analysis
methods to participate in the hands-on parts of the tutorial.","Laura Bishop (University of Oslo), Geoffray Bonnin (Université de Lorraine), Jérémy Frey (Ullo)",,,"<b>Laura Bishop</b> is a researcher at the RITMO Centre for Interdisciplinary Studies in Rhythm, Time and Motion and the Department of Musicology at the University of Oslo. She specialises in pupillometry, eye-tracking, and motion capture using approaches mainly grounded in psychology. She completed her PhD in music psychology at the MARCS Institute, Western Sydney University, Australia, in 2013. She currently co-leads the Austrian Science Fund project “Achieving togetherness in music ensembles” in collaboration with the University for Music and Performing Arts Vienna (mdw), which investigates physiological and body motion coordination in ensemble playing.

<b>Geoffray Bonnin</b> is an Associate Professor at the Lorraine Research Laboratory in Computer Science and its Applications (Loria), Université de Lorraine. He obtained his Ph.D. in 2010 and joined the Loria lab in 2014 as an Associate Professor. His research topics are related to artificial intelligence for music and for education. He is currently in charge of the Music-Mouv’ project, which is a collaboration with researchers in the domain of psychology that started in October 2021. The project aims at helping individuals with Parkinson’s disease to walk by triggering relevant emotions through physiology-based music recommendations.


<b>Jérémy Frey</b> is the CTO and co-founder of Ullo. After a master degree in cognitive sciences, he obtained his PhD degree in computer science in 2015 from the University of Bordeaux, France. During his work within the Inria research team Potioc, he had been studying how passive brain-computer interfaces could contribute to the evaluation of user experience, using for example EEG to infer a continuous index of cognitive load. His current research interests revolve around increasing introspection and social presence, by displaying inner states through tangible interfaces or wearables, with applications ranging from well-being to education.
",,,t1-m,
3,T2(M): Introduction to Differentiable Audio Synthesizer Programming,1,2023-11-05,9:00,13:00,Tutorials,"Differentiable digital signal processing is a technique in which signal processing algorithms are implemented as differentiable programs used in combination with deep neural networks. The advantages of this methodology include a reduction in model complexity, lower data requirements, and an inherently interpretible intermediate representation. In recent years, differentiable audio synthesizers have been applied to a variety of tasks, including voice and instrument modelling, synthesizer control, pitch estimation, source separation, and parameter estimation. Yet despite the growing popularity of such methods, the implementation of differentiable audio synthesizers remains poorly documented, and the simple formulation of many synthesizers belies their complex optimization behaviour. To address this gap, this tutorial offers an introduction to the fundamentals of differentiable synthesizer programming.","Ben Hayes (Queen Mary University of London), Jordie Shier (Centre for Digital Music, Queen Mary University of London), Chin-Yun Yu (Queen Mary University of London), David Südholt (Queen Mary University of London), Rodrigo Diaz (Queen Mary University of London)"," b.j.hayes@qmul.ac.uk, j.m.shier@qmul.ac.uk, chin-yun.yu@qmul.ac.uk, d.sudholt@qmul.ac.uk, r.diazfernandez@qmul.ac.uk",,"<b>Ben Hayes</b> is a third year PhD student at the Centre for Digital Music’s CDT in Artificial In- telligence and Music, based at Queen Mary University of London, under the supervision of Dr György Fazekas and Dr Charalampos Saitis. His research focuses on expanding the capabilities of differentiable digital signal processing by enabling control over non-convex operations. His work has been accepted to leading conferences in the field, including ISMIR, ICASSP, ICA, and the AES Convention, and published in the Journal of the Audio Engineering Society. He also holds an MSc with Distinction in Sound and Music Computing from QMUL and a first class BMus(Hons) in Electronic Music from the Guildhall School of Music and Drama, where he is now a member of teaching faculty. He is a founding member of the Special Interest Group on Neural Audio Synthesis at C4DM, and is the organizer of the international Neural Audio Synthesis Hackathon. Previously he was a Research intern at ByteDance, music lead at the award-winning generative music startup Jukedeck, and an internationally touring musician signed to R&S Records.

<b>Jordie Shier</b> is a first year PhD student in the Artificial Intelligence and Music (AIM) programme based at Queen Mary University of London (QMUL), studying under the supervision of Prof. Andrew McPherson and Dr. Charalampos Saitis. His research is focused on the development of novel methods for synthesizing audio and the creation of new interaction paradigms for music synthesizers. His current PhD project is on real-time timbral mapping for synthesized percussive performance and is being conducted in collaboration with Ableton. He was a co-organizer of the 2021 Holistic Evaluation of Audio Representations (HEAR) NeurIPS challenge and his work has been published in PMLR, DAFx, and the JAES. Previously, he completed an MSc in Computer Science and Music under the supervision of Prof. George Tzanetakis and Assoc. Prof. Kirck McNally.

<b>Chin-Yun Yu</b> is a first year PhD student in the Artificial Intelligence and Music (AIM) programme based at Queen Mary University of London (QMUL), under the supervision of Dr György Fazekas. His current research theme is on leveraging signal processing and deep generative models for controllable, expressive vocal synthesis. In addition, he is dedicated to open science and reproducible research by developing open-source packages and contributing to public research projects. He received a BSc in Computer Science from National Chiao Tung University in 2018 and was a research assistant at the Institute of Information Science, Academia Sinica, supervised by Prof. Li Su. His recent work has been published at ICASSP.

<b>David Südholt</b> is a first year PhD student in the Artificial Intelligence and Music (AIM) programme based at Queen Mary University of London (QMUL). Supervised by Prof. Joshua Reiss, he is researching parameter estimation for physical modelling synthesis, focussing on the synthesis and expressive transformation of the human voice. He received an MSc degree in Sound and Music Computing from Aalborg University Copenhagen in 2022, where he was supervised by Prof. Stefania Serafin and Assoc. Prof. Cumhur Erkut. His work has been published at the SMC conference and in the IEEE/ACM Transactions on Audio, Speech and Language Processing.

<b>Rodrigo Diaz</b> is a PhD candidate in Artificial Intelligence and Music at Queen Mary University in London, under the supervision of Prof. Mark Sandler and Dr. Charalampos Saitis. Rodrigo’s work has been published in leading computer vision and audio conferences, including CVPR, ICASSP, IC3D, and the AES Conference on Headphone Technology. Before starting his PhD studies, he worked as a researcher at the Immersive Communications group at the Fraunhofer HHI Institute in Berlin, where he investigated volumetric reconstruction from images using neural networks. His current research focuses on real-time audio synthesis using neural networks for 3D objects and drums. Rodrigo’s interdisciplinary background includes a Master’s degree in Media Arts and Design from Bauhaus University in Weimar and a Bachelor of Music from Texas Christian University.",,,t2-m,
4,"T3(M):  Transformer-based Symbolic Music Generation: Fundamentals to Advanced Concepts, Stylistic Considerations, Conditioning Mechanisms and Large Language Models",1,2023-11-05,9:00,13:00,Tutorials,"With the rise of the attention mechanism and the success of auto-regressive generative modelling and large language models, the Transformer architecture has arguably been the most promising technology for symbolic music generation. While audio-based methods have shown promise, symbolic music generation offers distinct advantages in terms of control, long-term coherence and computational efficiency. This tutorial explores the potential of the Transformer architecture in symbolic music generation and aims to provide (1) a thorough understanding of the vanilla Transformer architecture (emphasising the reasoning behind its design choices) and the utilisation of large language models for symbolic music generation. Additionally, it offers (2) a comprehensive overview of the field, including a taxonomy and a curated list of valuable datasets. The tutorial delves into (3) an in-depth analysis of Transformer variants and large language models specifically tailored for symbolic music generation. Also, it examines (4) examples and advanced considerations such as style, musical conditioning, and real-time performance. Furthermore, the tutorial offers (5) two hands-on exercises using Google Colab Notebooks, enabling participants to apply the concepts covered. Overall, this tutorial equips participants with the theoretical knowledge and practical skills necessary to explore the power of the Transformer architecture in symbolic music generation.","Berker Banar (Queen Mary University of London), Pedro Sarmento (Queen Mary University of London), Sara Adkins (INFINITE ALBUM)"," b.banar@qmul.ac.uk, p.p.sarmento@qmul.ac.uk, sara@infinitealbum.io",,"
<b>Berker Banar</b> is a PhD Researcher (Comp. Sci.) at the Centre for Doctoral Training in AI and Music (AIM CDT) and the Centre for Digital Music (C4DM) at Queen Mary University of London (QMUL), and also an Enrichment Student at the Alan Turing Institute. His PhD focuses on ‘Composing Contemporary Classical Music using Generative Deep Learning’ under supervision of Simon Colton to enhance human creativity and enable new aesthetics. Berker’s research interests include transformer-based generative modelling, optimisation, self-supervised representation learning for audio and music, explainable AI, quality-diversity analysis of generative model and out-of-distribution generation. He has worked at Sony and Bose as a research intern, and at Northwestern University Metamaterials and Nanophotonic Devices Lab as a nanophotonics researcher. Berker holds a BS in Electrical and Electronics Engineering from Bilkent University, Ankara, Turkey and a BM in Electronic Production and Design from Berklee College of Music, Boston, MA. His awards include Enrichment Community Award (The Alan Turing Institute), Exceptional Great Work Award (Bose), Outstanding Students of 2022 (EvoMUSART), Roland Award Endowed Scholarship (Berklee) and Outstanding Success Scholarship (Turkish Educational Foundation, upon ranking 17th in 1.5 million people in national university entrance exam). As a musician (drums and electronics), Berker has performed at venues such as the Museum of Fine Arts Boston, Harvard University Holden Chapel & Carpenter Center for Visual Arts (an original piece premiered as part of Berklee Interdisciplinary Arts Institute), Berklee Performance Center, Wally’s Jazz Club Boston, Nardis Jazz Club Istanbul and Istanbul Jazz Festival.

<b>Pedro Sarmento</b> is a PhD researcher at the Centre for Digital Music (C4DM), Queen Mary University of London (QMUL), working under the supervision of Mathieu Barthet within the UKRI Centre for Doctoral Training in Artificial Intelligence and Music (AIM). His research focuses on guitar-focused symbolic music generation with deep learning. This concerns the exploration of techniques for the creation of novel music that is represented in a digital tablature format, in which additional information about how to play specific music passages is provided. He holds an Integrated MSc degree in Electrical Engineering from Faculdade de Engenharia da Universidade do Porto (FEUP), a degree in Classical Guitar from the Conversatory of Music of Porto, and a second MSc degree in Multimedia and Interactive Sound from FEUP. He has an ongoing collaboration with Orquestra de Jazz de Matosinhos (OJM) where he leads sessions that foster an approach to STEM via musical concepts for young students. He volunteers for an online music magazine, writing album reviews and conducting interviews with artists from the Metal scene.

<b>Sara Adkins</b> is a music technologist, machine learning engineer, and performer who is enthusiastic about promoting the use of machine learning and AI in the creative arts. Currently, she works as a Generative Music and Audio Developer at Infinite Album, developing a real-time, interactive, and copyright-safe music engine for Twitch streamers. Sara holds a Master of Science in Sound and Music Computing from Queen Mary University of London where she was funded through a US-UK Fulbright grant. Her master's thesis focused on developing a Transformer model capable of generating loopable musical phrases for live coding and algorave performances, and received an Outstanding Student Mention at EvoMUSART 2023. Before moving to London, Sara spent three years in Boston where she worked as a machine learning engineer at Bose and played as a freelance classical guitarist. At Bose, she worked on deep learning models for speech enhancement that were optimized to run live on a hearing aid micro-controller. She also led a research project that developed generative audio algorithms that adapt to biofeedback signals to induce sleep using soothing music. Sara graduated from Carnegie Mellon University with an interdisciplinary bachelor's degree in music technology and computer science. Her senior capstone project, ""Creating with the Machine,"" combined algorithmic and traditional methods of composition into live performances to explore how interactive generative algorithms can influence creativity in musical improvisation. ""Creating with the Machine"" was premiered by the Carnegie Mellon Exploded Ensemble in the spring of 2018 and was awarded the Henry Armero Memorial Award for Inclusive Creativity.",,,t3-m,
9,Lunch,1,2023-11-05,13:00,14:30,Lunch,,,,,,,,,
10,"T4(A): Computer-Assisted Music-Making Systems: Taxonomy, Review, and Coding",1,2023-11-05,14:30,19:00,Tutorials,"Computer-Assisted Music-Making (CAMM) systems, are software-based tools designed to assist and augment the musical creativity of composers, performers, and music enthusiasts. CAMM systems encompass a wide range of systems that can be broadly categorized into two main types according to their design purposes: to assist music performance and to assist music composition. This tutorial offers a comprehensive review of the design principles, practical applications, taxonomy, and the state-of-the-art research of CAMM systems, with an emphasis on systems assisting music performance, which are also called “interactive music systems” or “musical agents” in the literature. Research on CAMMs is interdisciplinary in its nature, combining fields such as Music Information Retrieval (MIR), Artificial Intelligence (AI) and Human-Computer Interaction (HCI). Participants will gain an understanding of how these fields converge to create innovative and interactive musical experiences. This tutorial will also feature a coding session for participants to build a real-time musical agent, under the framework of Euterpe, a prototyping framework for creating music interactions on the Web. The tutorial will examine existing systems built using Euterpe, provide insights into the development process, and guide participants through the creation of their own musical agents. Participants in the coding part should bring a laptop with Chrome and Node.js [https://nodejs.org/en/download](https://nodejs.org/en/download)  installed, as well as have some coding experience. Familiarity with JavaScript will be helpful, but not necessary.","Christodoulos Benetatos (University of Rochester), Zhiyao Duan (University of Rochester), Philippe Pasquier (Simon Fraser University)",,,"**Christodoulos Benetatos** is a 5th year Ph.D student in the Department of Electrical and Computer Engineering at the University of Rochester. He received his B.S and M.Eng in Electrical Engineering from National Technical University of Athens in 2018. His research interests are focused primarily on automatic music generation as well as the design  and development of computer-assisted music-making systems. During his research internships at Kwai and TikTok, he worked on audio digital signal processing and music generation algorithms. As a classical guitarist, he has won several prizes in international guitar competitions and is a regular performer both as a soloist and as part of ensembles.

**Philippe Pasquier** is a professor at Simon Fraser University's School of Interactive Arts and Technology, where he directs the Metacreation Lab for Creative AI. He leads a research-creation program around generative systems for creative tasks. As such, he is a scientist specialized in artificial intelligence, a software designer, a multidisciplinary media artist, an educator, and a community builder. Pursuing a multidisciplinary research-creation program, his contributions bridge fundamental research on generative systems, machine learning, affective computing and computer-assisted creativity, applied research in the creative software industry, and artistic practice in interactive and generative art.

**Zhiyao Duan** is an associate professor in Electrical and Computer Engineering, Computer Science and Data Science at the University of Rochester. He received his B.S. in Automation and M.S. in Control Science and Engineering from Tsinghua University, China, in 2004 and 2008, respectively, and received his Ph.D. in Computer Science from Northwestern University in 2013. His research interest is in computer audition and its connections with computer vision, natural language processing, and augmented and virtual reality. He received a best paper award at the Sound and Music Computing (SMC) conference in 2017, a best paper nomination at the International Society for Music Information Retrieval (ISMIR) conference in 2017, and a CAREER award from the National Science Foundation (NSF). He served as a Scientific Program Co-Chair of ISMIR 2021, and is serving as an associate editor for IEEE Open Journal of Signal Processing, a guest editor for Transactions of the International Society for Music Information Retrieval, and a guest editor for Frontiers in Signal Processing. He is the President-Elect of ISMIR.",,,t4-a,
11,T5(A): Learning with Music Signals: Technology Meets Education,1,2023-11-05,14:30,19:00,Tutorials,"Music information retrieval (MIR) is an exciting and challenging research area that aims to develop techniques and tools for organizing, analyzing, retrieving, and presenting music-related data. Being at the intersection of engineering and humanities, MIR relates to different research disciplines, including signal processing, machine learning, information retrieval, musicology, and the digital humanities. In this tutorial, using music as a tangible and concrete application domain, we approach the concept of learning from different angles, addressing technological and educational aspects. In this way, the tutorial serves several purposes: we give a gentle introduction to MIR, highlight avenues for developing explainable machine-learning models, discuss how recent technology can be applied and communicated in interdisciplinary research and education, and introduce a new software package for teaching and learning music processing.
Our primary goal is to give an exciting tutorial that builds a bridge from basic to advanced techniques in MIR while highlighting technological and educational aspects. This tutorial should appeal to a broad audience, including students, educators, non-experts, and researchers new to the field, by covering concrete MIR tasks while providing many illustrative audio examples.
Links:
**Textbook: Fundamentals of Music Processing**
[www.music-processing.de](www.music-processing.de)
**FMP Notebooks**
[https://www.audiolabs-erlangen.de/FMP](https://www.audiolabs-erlangen.de/FMP)
**Python package: libfmp**
[https://github.com/meinardmueller/libfmp](https://github.com/meinardmueller/libfmp)
**PCP Notebooks**
[https://www.audiolabs-erlangen.de/PCP](https://www.audiolabs-erlangen.de/PCP)",Meinard Muller (International Audio Laboratories Erlangen),,,"**Meinard Müller** received the Diploma degree (1997) in mathematics and the Ph.D. degree (2001) in computer science from the University of Bonn, Germany. Since 2012, he has held a professorship for Semantic Audio Signal Processing at the International Audio Laboratories Erlangen, a joint institute of the Friedrich-Alexander-Universität and the Fraunhofer Institute for Integrated Circuits IIS. His recent research interests include music processing, music information retrieval, audio signal processing, and motion processing. He was a member of the IEEE Audio and Acoustic Signal Processing Technical Committee from 2010 to 2015, a member of the Senior Editorial Board of the IEEE Signal Processing Magazine (2018-2022), and a member of the Board of Directors of the International Society for Music Information Retrieval (2009-2021, being its president in 2020/2021). In 2020, he was elevated to IEEE Fellow for contributions to music signal processing.

Besides his scientific research, Meinard Müller has been very active in teaching music and audio processing. He gave numerous tutorials at major conferences, including ISMIR (2007, 2010, 2011, 2014, 2017, 2019), ICASSP (2009, 2011, 2019), Deep Learning IndabaX (2021), GI Jahrestagung (2017), Eurographics (2009, 2023), and ICME (2008). Furthermore, he wrote a monograph titled ""Information Retrieval for Music and Motion"" (Springer, 2007) as well as a textbook titled ""Fundamentals of Music Processing"" (Springer, 2015, www.music-processing.de). Recently, he released a comprehensive collection of educational Python notebooks designed for teaching and learning audio signal processing using music as an instructive application domain (https://www.audiolabs-erlangen.de/FMP).",,,t5-a,
12,T6(A): Kymatio: Deep Learning meets Wavelet Theory for Music Signal Processing,1,2023-11-05,14:30,19:00,Tutorials,"We present a tutorial on MIR with the open-source Kymatio (Andreux et al., 2020) toolkit for analysis and synthesis of music signals and timbre with differentiable computing. Kymatio is a Python package for applications at the intersection of deep learning and wavelet scattering. Its latest release (v0.4) provides an implementation of the joint time—frequency scattering transform (JTFS), which is an idealisation of a neurophysiological model that is commonly known in musical timbre perception research: the spectrotemporal receptive field (STRF) (Patil et al., 2012). In the MIR research, scattering transforms have demonstrated effectiveness in musical instrument classification (Vahidi et al., 2022), neural audio synthesis (Andreux et al., 2018), playing technique recognition and similarity (Lostanlen et al., 2021), acoustic modelling (Lostanlen et al., 2020), synthesizer parameter estimation and objective audio similarity (Vahidi et al., 2023, Lostanlen et al., 2023).
 
The Kymatio ecosystem will be introduced with examples in MIR:
 
Wavelet transform and scattering introduction (including constant-Q transform, scattering transforms, joint time–frequency scattering transforms, and visualizations)
MIR with scattering: music classification and segmentation
A perceptual distance objective for gradient descent
Generative evaluation of audio representations (GEAR) (Lostanlen et al., 2023)
 
A comprehensive overview of Kymatio’s frontend user interface will be given, with examples of extensibility of the core routines and filterbank construction.
 
We ask our participants to have some prior knowledge in:
 
Python and NumPy programming (familiarity with Pytorch is a bonus, but not essential)
Spectrogram visualization
Computer-generated sounds
 
No prior knowledge of wavelet or scattering transforms is expected.","Cyrus Vahidi (Queen Mary University of London), Christopher Mitcheltree (Queen Mary University of London), Vincent Lostanlen (Nantes Université)",c.vahidi@qmul.ac.uk,,"**Cyrus Vahidi** is a PhD researcher at the UKRI CDT in Artificial Intelligence and Music at the Centre for Digital Music, London and computer science graduate from Imperial College London. His research covers computational representations of auditory perception in machine listening and computer music. He is a core contributor to Kymatio, the open-source package for wavelet scattering. Previously, he was a visiting researcher at LS2N (CNRS, France) and worked on MIR/ML in ByteDance’s SAMI group. He is the founder of Sonophase AI and performs experimental electronic music with Max/MSP and modular synthesis.
 
**Christopher Mitcheltree** is a PhD researcher at the UKRI CDT in Artificial Intelligence and Music at the Centre for Digital Music, London. He researches time-varying modulations of synthesizers / audio effects and is a founding developer of Neutone, an open-source neural audio plugin and SDK. In the past, he has worked on machine learning and art projects at a variety of different companies and institutions including: Google, Airbnb, AI2, Keio University, and Qosmo.
 
**Dr. Vincent Lostanlen** obtained his PhD in 2017 from École normale supérieure, under the supervision of Stéphane Mallat. Since then, he is a scientist (chargé de recherche) at CNRS and a visiting scholar at New York University. He is a founding member of the Kymatio consortium.
",,,t6-a,
17,Welcome Reception and Concert,1,2023-11-05,20:00,22:00,Social,,,,,,,,,
18,Registration,2,2023-11-06,8:00,9:00,Registration,Time to register at ISMIR2023! ,,,,,,,,
19,Opening Session,2,2023-11-06,9:00,10:00,Opening,,,,,,,,ismir-social,
,Paper Session - 1,2,2023-11-06,10:00,12:30,Poster session,,Session Chair: Emilia Parada-Cabaleiro (Johannes Kepler University),,,,,,,
,Lunch,2,2023-11-06,12:30,13:30,Lunch,,,,,,,,,
,Paper Session - 2,2,2023-11-06,13:30,16:00,Poster session,,Session Chair: Chitralekha Gupta (National University of Singapore),,,,,,,
,WiMIR plenary session,2,2023-11-06,16:00,17:30,WiMIR Meetup,,"Moderators: Xiao Hu (Hong Kong University)(remote), Ranjani H G (Ericsson R&D) (in-person)",,,,,,wimir,https://ismir2022program.slack.com/archives/C04CJ7FKYNR
,Performance by Dhaatu Puppet Show,2,2023-11-06,17:30,19:00,Social,,Team Dhaatu,,,,,https://www.dhaatupuppets.org/,ismir-social,https://ismir2022program.slack.com/archives/C04CM67BZUJ
,Welcome Reception,2,2023-11-06,19:00,21:00,Social,,ISMIR 2022 committee,,,,,,ismir-social,https://ismir2022program.slack.com/archives/C04CM67BZUJ
,Special Session A (Online): Ethics/Code of Conduct for ISMIR,2,2023-11-06,22:00,23:15,VMeetup,,"Moderators: Andre Holzapfel (KTH Royal Institute of Technology, Sweden), Fabio Morreale (University of Auckland), Bob Sturm (KTH Royal Institute of Technology, Sweden)",,,,,,special-sessions,https://ismir2022program.slack.com/archives/C04CPLHT8MA
,Registration,3,2023-11-07,8:00,9:00,Registration,,,,,,,,,
,"Keynote-1: TM Krishna on ""Evolution of Performance and Aesthetics in Indian Art Music""",3,2023-11-07,9:00,10:00,All Meeting,,TM Krishna,,"Karnatik Musician, Author & Activist","TM Krishna, is one of the pre-eminent vocalists in the rigorous Karnatik tradition of India's classical music. As a public intellectual, Krishna speaks and writes about issues affecting the human condition and about matters cultural. As a vocalist, he has made path-breaking innovations in both the style and substance of his concerts. His award-winning book, A Southern Music – The Karnatik Story, published by Harper Collins in 2013 was a first-of-its-kind philosophical, aesthetic and socio-political exploration of Karnatik music. TM Krishna has partnered with individuals and collectives working at the intersections of social change, a new politics for contemporary India, a fresh new imagining of the wider universe of the Arts. In 2016, TM Krishna received the prestigious Ramon Magsaysay Award in recognition of ""his forceful commitment as artist and advocate to art’s power to heal India’s deep social divisions"".",TMKrishna.jpeg,https://www.tmkrishna.com/,keynote-tmk,https://ismir2022program.slack.com/archives/C04CM3T0YQ3
,Paper Session - 3 (Special Call),3,2023-11-07,10:00,12:30,Poster session,,Session Chair: Rafael Caro Repetto (University of Music and Performing Arts Graz),,,,,,,
,Lunch,3,2023-11-07,12:30,13:30,Lunch,,,,,,,,,
,Paper Session - 4,3,2023-11-07,13:30,16:00,Poster session,,Session Chair: Vinoo Alluri (IIIT Hyderabad),,,,,,,
,Special Session - 1: Enhancing music listening with MIR,3,2023-11-07,16:00,17:00,Meetup,,Moderator: Xavier Serra (Universitat Pompeu Fabra),,,,,,special-sessions,https://ismir2022program.slack.com/archives/C04CPLHT8MA
,ISMIR Music Program,3,2023-11-07,17:00,18:30,Music,,Session Chair: Carlos Guedes (NYU Abu Dhabi),,,,,,,
,ISMIR Music Concert,3,2023-11-07,18:30,20:00,Social,"ISMIR 2022 Music Concert will feature a Jugalbandi vocal Indian art music concert by Kaustuv Kanti Ganguli and Vignesh Ishwar. The jugalbandi concert will aim to showcase the commonalities, differences and nuances of Hindustani and Carnatic music, the two predominant art music traditions of India. Kaustuv Kanti Ganguli will be accompanied by Ravindra Katoti on the harmonium and Tejovrush Joshi on the tabla. Vignesh Ishwar will be accompanied by Sayee Rakshith on the violin and Sumesh Narayanan on the mridangam. Kaustuv and Vignesh are seasoned professional musicians and MIR researchers who can bring their expertise and understanding to put together an enthralling performance interesting to the conference participants. A brief biography of the artists and additional details of the concert will be available soon.","Hindustani vocals: Kaustuv Kanti Ganguli 

Carnatic vocals: Vignesh Ishwar 

Harmonium: Ravindra Katoti 

Carnatic violin: Sayee Rakshith 

Tabla: Tejovrush Joshi 

Mridangam: Sumesh Narayanan",,,,,,ismir-social,https://ismir2022program.slack.com/archives/C04CM67BZUJ
,Special Session B (Online): PhD in MIR - Challenges and Opportunities,3,2023-11-07,22:00,23:15,VMeetup,"Music information retrieval (MIR) is an exciting research field related to different disciplines, including signal processing, machine learning, information retrieval, psychology, musicology, and the digital humanities. This diversity opens up many opportunities for challenging, interdisciplinary, and fascinating research projects at the intersection of engineering and humanities. However, younger researchers can also feel overwhelmed by the variety and complexity of MIR research questions. In this session, we will have an informal exchange of ideas and experiences, inviting doctoral candidates and more experienced MIR researchers. Responding to questions from the audience, we hope this interactive session will be helpful for current PhD students and students considering a PhD in MIR.",Moderator: Meinard Müller (International Audio Laboratories Erlangen),,,,,,special-sessions,https://ismir2022program.slack.com/archives/C04CPLHT8MA
,Registration,4,2023-11-08,8:00,9:00,Registration,Time to register yourself for ISMIR'22 (if you haven't dont it already on the previous days).,,,,,,,,
,"Keynote - 2: Richa Singh on ""Adventures of AI: Deepfake and Bias in Audio Processing""",4,2023-11-08,9:00,10:00,All Meeting,"The increasing capabilities for machine learning algorithms is enabling the usage of ML models for a variety of tasks including for creativity such as generating new music and modifying existing music. Similar applications are present in different kinds of audio signals such as voice biometrics, speaker and speech recognition. However, these technologies that support creativity can also be used for malicious purposes. Deepfake audios are one such technology which enable flawlessly altering existing audio signals or creating new signals from any given text. Audio can also be integrated with videos to provide a complete multimodal experience, which can be purely synthetic and fake. While there is significant research ongoing in image and video, the space of detecting these anomalies in audio processing is relatively unaddressed. We will discuss some of these possible adventures of machine learning in audio processing and the research efforts that we are undertaking to detect them. In addition, we will also discuss the bias and fairness issues in audio processing where we will highlight ""out of distribution"" behavior of popular approaches and some strategies to address them.",Richa Singh,,"Professor and Head, Dept. of Computer Science and Engineering, Indian Institute of Technology Jodhpur","Richa Singh received her Ph.D. degree in computer science from West Virginia University, Morgantown, USA, in 2008. She is currently a Professor and Head at Department of CSE, IIT Jodhpur. She has co-edited the book Deep Learning in Biometrics and has delivered keynote talks/tutorials on deep learning, trusted AI, and domain adaptation in NVIDIA GTC 2021, BIOSIG2021, ICCV 2017, AFGR 2017, and IJCNN 2017. Her areas of interest are pattern recognition, machine learning, and biometrics. She is a Fellow of IEEE, IAPR and AAIA, and a Senior Member of ACM. She was a recipient of the Kusum and Mohandas Pai Faculty Research Fellowship at the IIIT-Delhi, the FAST Award by the Department of Science and Technology, India, and several best paper and best poster awards in international conferences. She is/was served as the Program Co-Chair of CVPR2022, ICMI2022, IJCB2020, AFGR2019 and BTAS 2016, and a General Co-Chair of FG 2021 and ISBA 2017. She is also the Vice President (Publications) of the IEEE Biometrics Council and an Associate Editor-in-Chief of Pattern Recognition.",richa_singh.jpeg,http://home.iitj.ac.in/~richa/,keynote-richa,https://ismir2022program.slack.com/archives/C04CM6W0KMG
,Paper Session - 5,4,2023-11-08,10:00,12:30,Poster session,"Browse the active poster session's channels, joining calls to ask questions and discuss research with presenters, and leave comments in the channel for asynchronous chatting later.


* <b>P5-01*: Sonus Texere! Automated Dense Soundtrack Construction for Books using Movie Adaptations</b><br>Jaidev Shriram, Makarand Tapaswi, Vinoo Alluri
* <b>P5-02: Musika! Fast Infinite Waveform Music Generation</b><br>Marco Pasini, Jan Schlüter
* <b>P5-03: Symphony Generation with Permutation Invariant Language Model</b><br>Jiafeng Liu, Yuanliang Dong, Zehua Cheng, Xinran Zhang, Xiaobing Li, Feng Yu, Maosong Sun
* <b>P5-04: MuLan: A Joint Embedding of Music Audio and Natural Language</b><br>Qingqing Huang, Aren Jansen, Joonseok Lee, Ravi Ganti, Judith Yue Li, Daniel P W Ellis
* <b>P5-05: MeloForm: Generating Melody with Musical Form based on Expert Systems and Neural Networks</b><br>Peiling Lu, Xu Tan, Botao Yu, Tao Qin, Sheng Zhao, Tie-Yan Liu
* <b>P5-06: Towards robust music source separation on loud commercial music</b><br>Chang-Bin Jeon, Kyogu Lee
* <b>P5-07: Towards Quantifying the Strength of Music Scenes Using Live Event Data</b><br>Michael Zhou, Andrew Mcgraw, Douglas R Turnbull
* <b>P5-08: Learning Multi-Level Representations for Hierarchical Music Structure Analysis</b><br>Morgan Buisson, Brian Mcfee, Slim Essid, Hélène C. Crayencour Crayencour
* <b>P5-09: Multi-instrument Music Synthesis with Spectrogram Diffusion</b><br>Curtis Hawthorne, Ian Simon, Adam Roberts, Neil Zeghidour, Joshua Gardner, Ethan Manilow, Jesse Engel
* <b>P5-10: DDX7: Differentiable FM Synthesis of Musical Instrument Sounds</b><br>Franco Caspe, Andrew Mcpherson, Mark Sandler
* <b>P5-11: Singing beat tracking with Self-supervised front-end and linear transformers</b><br>Mojtaba Heydari, Zhiyao Duan
* <b>P5-12: EnsembleSet: a new high quality synthesised dataset for chamber ensemble separation</b><br>Saurjya Sarkar, Emmanouil Benetos, Mark Sandler
* <b>P5-13: End-to-End Lyrics Transcription Informed by Pitch and Onset Estimation</b><br>Tengyu Deng, Eita Nakamura, Kazuyoshi Yoshii
* <b>P5-14: Contrastive Audio-Language Learning for Music</b><br>Ilaria Manco, Emmanouil Benetos, Elio Quinton, George Fazekas
* <b>P5-15: MusAV: A dataset of relative arousal-valence annotations for validation of audio models</b><br>Dmitry Bogdanov, Xavier Lizarraga-Seijas, Pablo Alonso-Jiménez, Xavier Serra
* <b>P5-16: What is missing in deep music generation? A study of repetition and structure in popular music</b><br>Shuqi Dai, Huiran Yu, Roger B Dannenberg
* <b>P5-17: Heterogeneous Graph Neural Network for Music Emotion Recognition</b><br>Angelo Cesar Mendes Da Silva, Diego F Silva, Ricardo Marcondes Marcacini


An asterisk (*) indicates long presentations (paper award candidates)
",Session Chair: Rachel Bittner (Spotify),,,,,,,
,Lunch,4,2023-11-08,12:30,13:30,Lunch,Lunch time!,,,,,,,,
,Paper Session - 6,4,2023-11-08,13:30,16:00,Poster session,"Browse the active poster session's channels, joining calls to ask questions and discuss research with presenters, and leave comments in the channel for asynchronous chatting later.



* <b>P6-01*: And what if two musical versions don't share melody, harmony, rhythm, or lyrics?</b><br>Mathilde Abrassart, Guillaume Doras
* <b>P6-02: A diffusion-inspired training strategy for singing voice extraction in the waveform domain</b><br>Genís Plaja-Roglans, Marius Miron, Xavier Serra
* <b>P6-03: A Model You Can Hear: Audio Identification with Playable Prototypes</b><br>Romain Loiseau, Baptiste Bouvier, Yann Teytaut, Elliot Vincent, Mathieu Aubry, Loic Landrieu
* <b>P6-04: An Exploration of Generating Sheet Music Images</b><br>Marcos Acosta, Irmak Bukey, T J Tsai
* <b>P6-05: HPPNet: Modeling the Harmonic Structure and Pitch Invariance in Piano Transcription</b><br>Weixing Wei, Peilin Li, Yi Yu, Wei Li
* <b>P6-06: Generating music with sentiment using Transformer-GANs</b><br>Pedro L T Neves, José Fornari, João B Florindo
* <b>P6-07: Improving Choral Music Separation through Expressive Synthesized Data from Sampled Instruments</b><br>Ke Chen, Hao-Wen Dong, Yi Luo, Julian Mcauley, Taylor Berg-Kirkpatrick, Miller Puckette, Shlomo Dubnov
* <b>P6-08: Ethics of Singing Voice Synthesis: Perceptions of Users and Developers</b><br>Kyungyun Lee, Gladys Hitt, Emily Terada, Jin Ha Lee
* <b>P6-09: Emotion-driven Harmonisation And Tempo Arrangement of Melodies Using Transfer Learning</b><br>Takuya Takahashi, Mathieu Barthet
* <b>P6-10: Using Activation Functions for Improving Measure-Level Audio Synchronization</b><br>Yigitcan Özer, Matej Ištvánek, Vlora Arifi-Müller, Meinard Müller
* <b>P6-11: A deep learning method for melody extraction from a polyphonic symbolic music representation</b><br>Katerina Kosta, Wei Tsung Lu, Gabriele Medeot, Pierre Chanquion
* <b>P6-12: A Reproducibility Study on User-centric MIR Research and Why it is Important</b><br>Peter Knees, Bruce Ferwerda, Andreas Rauber, Sebastian Strumbelj, Annabel Resch, Laurenz Tomandl, Valentin Bauer, Fung Yee Tang, Josip Bobinac, Amila Ceranic, Riad Dizdar
* <b>P6-13: Music Separation Enhancement with Generative Modeling</b><br>Noah Schaffer, Boaz Cogan, Ethan Manilow, Max Morrison, Prem Seetharaman, Bryan Pardo
* <b>P6-14: SampleMatch: Drum Sample Retrieval by Musical Context</b><br>Stefan Lattner
* <b>P6-15: A Transformer-Based """"Spellchecker"""" for Detecting Errors in OMR Output</b><br>Timothy De Reuse, Ichiro Fujinaga
* <b>P6-16: """"More than words"""": Linking Music Preferences and Moral Values through Lyrics</b><br>Vjosa Preniqi, Kyriaki Kalimeri, Charalampos Saitis


An asterisk (*) indicates long presentations (paper award candidates)
",Session Chair: Juhan Nam (Korea Advanced Institute of Science and Technology),,,,,,,
,Special Session 2: Enhancing music creativity with MIR,4,2023-11-08,16:00,17:00,Meetup,"While audio technology has always had an important role in music production, it is now recognised that MIR tools can provide for workflows that enhance music creativity at every stage of the journey. The panel will discuss the possibilities and challenges of this exciting partnership between music computing and creativity.

Panelists: Georgi Dzhambazov (Smule), Dorien Herremans (SUTD), Oriol Nieto (Adobe), Akira Maezawa (Yamaha), Igor Pereira (Moises.ai)",Moderator: Jan Van Balen (Spotify),,,,,,special-sessions,https://ismir2022program.slack.com/archives/C04CPLHT8MA
,ISMIR 2022 Banquet,4,2023-11-08,17:00,21:00,Social,,,,,,,,,
,Special Session - C (Online): TISMIR: the open journal of the ISMIR society,4,2023-11-08,22:00,23:15,VMeetup,"Transactions of the International Society for Music Information Retrieval(TISMIR) was established in 2018 to complement the ISMIR conference proceedings and provide a vehicle for the dissemination of the highest quality and most substantial scientific research in MIR. TISMIR retains the Open Access model of the ISMIR Conference proceedings, encourages reproducibility of the published research papers, and maintains a low publication cost. Almost 5 years later, this ISMIR 2022 is devoted to discuss and brainstorm on the current status and future perspectives of the journal with a series of TISMIR recent and potential authors, reviewers and editors. We will address the following questions, and others proposed by participants: What do you appreciate more about TISMIR? What is the link and complementarity to the ISMIR conference? Which are the main challenges/limitations that need to be addressed? How to make TISMIR competitive as a journal in the current publication landscape? How to engage with more community members in order to make TISMIR a success? Which are future avenues for conference vs journal outlets in the ISMIR field?","Moderator: Emilia Gómez (Joint Research Centre, European Commission and Universitat Pompeu Fabra)",,,,,,special-sessions,https://ismir2022program.slack.com/archives/C04CPLHT8MA
,Registration,5,2023-11-09,8:00,9:00,Registration,Time to register yourself for ISMIR'22 (if you haven't dont it already on the previous days).,,,,,,,,
,Paper Session - 7,5,2023-11-09,9:00,11:30,Poster session,"Browse the active poster session's channels, joining calls to ask questions and discuss research with presenters, and leave comments in the channel for asynchronous chatting later.




* <b>P7-01: A unified model for zero-shot singing voice conversion and synthesis</b><br>Jui-Te Wu, Jun-You Wang, Jyh-Shing Roger Jang, Li Su
* <b>P7-02: Semantic Control of Generative Musical Attributes</b><br>Stewart Greenhill, Majid Abdolshah, Vuong Le, Sunil Gupta, Svetha Venkatesh
* <b>P7-03: Music Representation Learning Based on Editorial Metadata from Discogs</b><br>Pablo Alonso-Jiménez, Xavier Serra, Dmitry Bogdanov
* <b>P7-04: Melody Infilling with User-Provided Structural Context</b><br>Chih-Pin Tan, Alvin W Y Su, Yi-Hsuan Yang
* <b>P7-05: Robust Melody Track Identification in Symbolic Music</b><br>Xichu Ma, Xiao Liu, Bowen Zhang, Ye Wang
* <b>P7-06: Tracking the Evolution of a Band's Live Performances over Decades</b><br>Florian Thalmann, Eita Nakamura, Kazuyoshi Yoshii
* <b>P7-07: Evaluating Generative Audio Systems and Their Metrics</b><br>Ashvala Vinay, Alexander Lerch
* <b>P7-08: Representation Learning for the Automatic Indexing of Sound Effects Libraries</b><br>Alison B Ma, Alexander Lerch
* <b>P7-09: Concept-Based Techniques for """"Musicologist-Friendly"""" Explanations in Deep Music Classifiers</b><br>Francesco Foscarin, Katharina Hoedt, Verena Praher, Arthur Flexer, Gerhard Widmer
* <b>P7-10: Verse versus Chorus: Structure-aware Feature Extraction for Lyrics-based Genre Recognition</b><br>Maximilian Mayerl, Stefan Brandl, Günther Specht, Markus Schedl, Eva Zangerle
* <b>P7-11: Transfer Learning of wav2vec 2.0 for Automatic Lyric Transcription</b><br>Longshen Ou, Xiangming Gu, Ye Wang
* <b>P7-12: A Novel Dataset and Deep Learning Benchmark for Classical Music Form Recognition and Analysis</b><br>Daniel Szelogowski, Lopamudra Mukherjee, Benjamin Whitcomb
* <b>P7-13: BAF: An audio fingerprinting dataset for broadcast monitoring</b><br>Guillem Cortès, Alex Ciurana, Emilio Molina, Marius Miron, Owen Meyers, Joren Six, Xavier Serra
* <b>P7-14: Cadence Detection in Symbolic Classical Music using Graph Neural Networks</b><br>Emmanouil Karystinaios, Gerhard Widmer
* <b>P7-15: Domain Adversarial Training on Conditional Variational Auto-Encoder for Controllable Music Generation</b><br>Jingwei Zhao, Gus Xia, Ye Wang
* <b>P7-16: Modeling perceptual loudness of piano tone: theory and applications</b><br>Yang Qu, Yutian Qin, Lecheng Chao, Hangkai Qian, Ziyu Wang, Gus Xia
* <b>P7-17: On the Impact and Interplay of Input Representations and Network Architectures for Automatic Music Tagging</b><br>Maximilian Damböck, Richard Vogl, Peter Knees

An asterisk (*) indicates long presentations (paper award candidates)

",Session Chair: ‪Gaël Richard (Télécom Paris),,,,,,,‬
,Industry Session,5,2023-11-09,11:30,12:30,Industry,"Industry presentation session will include short presentations from our industry sponsors. The session will have a 12 min talks by our Platinum sponsors Spotify and Moises, and 8 min talks by our Gold sponsors Adobe, Deezer, Utopia music, Pandora, Smule, Yamaha and Chordify.",Session Chair: Siddharth Bhardwaj (beatoven.ai),,,,,,,
,Lunch,5,2023-11-09,12:30,13:30,Lunch,Lunch time!,,,,,,,,
,"Society meeting, awards and closing",5,2023-11-09,13:30,15:30,Awards,"Society meeting, awards and closing","Session Chair: Geoffroy Peeters (IRCAM, Télécom Paris)",,,,,,,
,Late-breaking/Demo (Physical),5,2023-11-09,15:30,17:30,LBD,"The Late-breaking/Demo (LBD) session is a forum for sharing prototype systems, initial concepts, and early results which may have not yet fully matured but are of interest to the Music-IR community. It is also a great entry point for people who are new to ISMIR to showcase their preliminary work and receive early feedback from fellow researchers. Attendees of the LBD can interact with demos or discuss their thoughts on the latest developments in the field.","Session Chairs: Sanjeel Parekh (Télécom Paris), Siddharth Gururani (NVIDIA)",,,,,,,
,Late-breaking/Demo (Virtual),5,2023-11-09,17:30,19:00,LBD,"The Late-breaking/Demo (LBD) session is a forum for sharing prototype systems, initial concepts, and early results which may have not yet fully matured but are of interest to the Music-IR community. It is also a great entry point for people who are new to ISMIR to showcase their preliminary work and receive early feedback from fellow researchers. Attendees of the LBD can interact with demos or discuss their thoughts on the latest developments in the field.","Session Chairs: Sanjeel Parekh (Télécom Paris), Siddharth Gururani (NVIDIA)",,,,,,,
,Indian Music Experience Satellite Workshop,6,2023-11-09,10:00,18:00,Satellite,,,,,,,https://ismir2022.ismir.net/satellites/ime,,
